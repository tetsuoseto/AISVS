# 부록 A: 용어집

이 포괄적인 용어집은 명확성과 공통된 이해를 보장하기 위해 AISVS 전반에 걸쳐 사용되는 주요 AI, ML 및 보안 용어의 정의를 제공합니다.

* 적대적 예제: 인간이 인지할 수 없는 미묘한 변형을 추가하여 AI 모델이 실수를 하도록 의도적으로 만든 입력.
  ​
* 적대적 견고성 – AI에서 적대적 견고성은 의도적으로 설계된 악성 입력에 의해 오류를 일으키거나 속지 않고 모델이 성능을 유지하고 저항하는 능력을 의미합니다.
  ​
* 에이전트 – AI 에이전트는 사용자를 대신하여 목표를 추구하고 작업을 완료하기 위해 AI를 사용하는 소프트웨어 시스템입니다. 이들은 추론, 계획 및 기억 능력을 보여주며, 의사결정, 학습 및 적응을 할 수 있는 일정 수준의 자율성을 가지고 있습니다.
  ​
* 에이전틱 AI: 일정 수준의 자율성을 가지고 목표를 달성하기 위해 작동할 수 있으며, 종종 직접적인 인간 개입 없이 의사결정과 행동을 수행하는 AI 시스템.
  ​
* 속성 기반 접근 제어(ABAC): 사용자, 자원, 행위 및 환경의 속성을 기반으로 권한 부여 결정을 내리며, 쿼리 시점에 평가되는 접근 제어 패러다임입니다.
  ​
* 백도어 공격: 모델이 특정 트리거에 대해 특정 방식으로 응답하도록 학습되면서 그 외에는 정상적으로 동작하는 일종의 데이터 포이즈닝 공격입니다.
  ​
* 편향: 특정 그룹이나 특정 상황에서 불공정하거나 차별적인 결과를 초래할 수 있는 AI 모델 출력의 체계적인 오류.
  ​
* 편향 악용: AI 모델의 알려진 편향을 이용하여 출력이나 결과를 조작하는 공격 기법.
  ​
* Cedar: AI 시스템을 위한 ABAC 구현에 사용되는 세밀한 권한을 위한 아마존의 정책 언어 및 엔진.
  ​
* Chain of Thought: 최종 답변을 생성하기 전에 중간 추론 단계를 생성하여 언어 모델의 추론 능력을 향상시키는 기법입니다.
  ​
* 서킷 브레이커: 특정 위험 임계값을 초과할 때 AI 시스템 작동을 자동으로 중단하는 메커니즘.
  ​
* 기밀 추론 서비스: 신뢰할 수 있는 실행 환경(TEE) 또는 이에 상응하는 기밀 컴퓨팅 메커니즘 내에서 AI 모델을 실행하는 추론 서비스로, 모델 가중치와 추론 데이터가 암호화되고 봉인되며 무단 접근 또는 변조로부터 보호되도록 보장합니다.
  ​
* 기밀 작업 부하: 하드웨어 기반 격리, 메모리 암호화 및 원격 증명을 통해 코드, 데이터 및 모델을 호스트 또는 동일 테넌트의 접근으로부터 보호하는 신뢰 실행 환경(TEE) 내에서 실행되는 AI 작업 부하(예: 학습, 추론, 전처리).
  ​
* 데이터 누출: AI 모델 출력이나 동작을 통해 민감한 정보가 의도치 않게 노출되는 것.
  ​
* 데이터 중독: 모델 무결성을 훼손하기 위해 학습 데이터를 고의로 변조하는 행위로, 종종 백도어를 설치하거나 성능 저하를 초래하기 위해 수행됨.
  ​
* 차등 개인정보 보호 – 차등 개인정보 보호는 개별 데이터 주체의 프라이버시를 보호하면서 데이터셋에 대한 통계 정보를 공개하기 위한 수학적으로 엄밀한 프레임워크입니다. 이는 데이터 보유자가 특정 개인에 관한 정보 누출을 제한하면서 그룹의 집계 패턴을 공유할 수 있게 합니다.
  ​
* 임베딩: 의미적 의미를 고차원 공간에서 포착하는 데이터(텍스트, 이미지 등)의 밀집 벡터 표현.
  ​
* 설명 가능성 – AI에서 설명 가능성은 AI 시스템이 그 결정과 예측에 대해 사람이 이해할 수 있는 이유를 제공하여 내부 작동 방식에 대한 통찰을 제공하는 능력입니다.
  ​
* 설명 가능한 인공지능(XAI): 다양한 기법과 프레임워크를 통해 결정 및 행동에 대해 사람이 이해할 수 있는 설명을 제공하도록 설계된 인공지능 시스템.
  ​
* 연합 학습: 데이터 자체를 교환하지 않고, 로컬 데이터 샘플을 보유한 여러 분산 장치에서 모델을 학습하는 기계 학습 방법입니다.
  ​
* 정식화: 하이퍼파라미터, 학습 구성, 전처리 단계 또는 빌드 스크립트와 같이 아티팩트나 데이터셋을 생성하기 위해 사용되는 레시피 또는 방법.
  ​
* 가드레일: AI 시스템이 해롭거나 편향된, 또는 바람직하지 않은 출력을 생성하는 것을 방지하기 위해 구현된 제약 조건.
  ​
* 환각 – AI 환각은 AI 모델이 학습 데이터나 사실적 현실에 기반하지 않은 잘못되거나 오해의 소지가 있는 정보를 생성하는 현상을 의미합니다.
  ​
* 휴먼 인 더 루프(HITL): 중요한 의사 결정 지점에서 인간의 감독, 확인 또는 개입을 필요로 하도록 설계된 시스템.
  ​
* 코드형 인프라스트럭처(IaC): 수동 프로세스 대신 코드를 통해 인프라를 관리하고 프로비저닝하며, 이를 통해 보안 스캐닝과 일관된 배포가 가능하도록 하는 것.
  ​
* Jailbreak: 특히 대형 언어 모델에서 금지된 콘텐츠를 생성하기 위해 AI 시스템의 안전 장치를 우회하는 데 사용되는 기술.
  ​
* 최소 권한 원칙: 사용자 및 프로세스에 대해 필요한 최소한의 접근 권한만 부여하는 보안 원칙.
  ​
* LIME (국소 해석 가능 모델-불가지론적 설명): 해석 가능한 모델로 국소적으로 근사하여 어떤 머신러닝 분류기의 예측도 설명하는 기법.
  ​
* MCP(모델 컨텍스트 프로토콜): AI 모델과 에이전트가 정의된 전송 방식을 통해 구조화되고 유형화된 요청 및 응답을 교환하여 외부 도구, 데이터 소스 및 자원에 접근할 수 있게 하는 프로토콜입니다.
  ​
* 멤버십 추론 공격: 특정 데이터 포인트가 머신러닝 모델 학습에 사용되었는지 여부를 판단하는 것을 목표로 하는 공격.
  ​
* MITRE ATLAS: 인공 지능 시스템에 대한 적대적 위협 지형; AI 시스템에 대한 적대적 전술 및 기법의 지식 기반입니다.
  ​
* 모델 카드 – 모델 카드는 AI 모델의 성능, 한계, 의도된 사용 및 윤리적 고려사항에 대한 표준화된 정보를 제공하여 투명성과 책임있는 AI 개발을 촉진하는 문서입니다.
  ​
* 모델 추출: 공격자가 대상 모델에 반복적으로 쿼리를 보내 허가 없이 기능적으로 유사한 복사본을 만드는 공격.
  ​
* 모델 반전: 모델 출력을 분석하여 학습 데이터를 재구성하려는 공격.
  ​
* 모델 수명 주기 관리 – AI 모델 수명 주기 관리는 설계, 개발, 배포, 모니터링, 유지보수 및 최종 폐기를 포함한 AI 모델의 모든 단계들을 감독하여 모델이 효과적이고 목표에 부합하도록 보장하는 프로세스입니다.
  ​
* 모델 중독: 학습 과정 중에 모델에 취약점이나 백도어를 직접 도입하는 것.
  ​
* 모델 도용/절도: 반복적인 쿼리를 통해 독점 모델의 복사본 또는 근사치를 추출하는 행위.
  ​
* 다중 에이전트 시스템: 서로 상호작용하는 여러 AI 에이전트로 구성된 시스템으로, 각 에이전트는 잠재적으로 다른 능력과 목표를 가질 수 있습니다.
  ​
* OPA (오픈 정책 에이전트): 스택 전반에 걸쳐 통합된 정책 적용을 가능하게 하는 오픈 소스 정책 엔진입니다.
  ​
* 개인정보 보호 기계 학습(PPML): 훈련 데이터의 개인정보를 보호하면서 머신러닝 모델을 훈련하고 배포하는 기술 및 방법.
  ​
* 프롬프트 인젝션: 모델의 의도된 동작을 무시하도록 악의적인 명령어가 입력에 삽입되는 공격.
  ​
* RAG(검색 기반 생성): 외부 지식 소스에서 관련 정보를 검색한 후 응답을 생성하여 대규모 언어 모델을 향상시키는 기술입니다.
  ​
* 레드-티밍: 취약점을 식별하기 위해 적대적 공격을 시뮬레이션하여 AI 시스템을 적극적으로 테스트하는 행위.
  ​
* SBOM (소프트웨어 자재 명세서): 소프트웨어 또는 AI 모델을 구축하는 데 사용된 다양한 구성 요소의 세부 사항과 공급망 관계를 포함하는 공식 기록.
  ​
* SHAP(SHapley Additive exPlanations): 각 특성이 예측에 기여하는 정도를 계산하여 어떤 머신러닝 모델의 출력도 설명할 수 있는 게임 이론적 접근법입니다.
  ​
* 강력한 인증: 적어도 두 가지 요소(지식, 소지, 고유성)를 요구하고 FIDO2/WebAuthn, 인증서 기반 서비스 인증 또는 단기 토큰과 같은 피싱 저항 메커니즘을 사용하여 자격 증명 도용 및 재사용 공격에 대응하는 인증 방식입니다.
  ​
* 공급망 공격: 타사 라이브러리, 데이터세트 또는 사전 학습된 모델과 같은 공급망 내 보안이 덜한 요소를 표적으로 삼아 시스템을 침해하는 것.
  ​
* 전이 학습: 한 작업을 위해 개발된 모델을 두 번째 작업의 모델 시작점으로 재사용하는 기술.
  ​
* 벡터 데이터베이스: 고차원 벡터(임베딩)를 저장하고 효율적인 유사도 검색을 수행하도록 설계된 전문 데이터베이스입니다.
  ​
* 취약점 스캐닝: AI 프레임워크 및 종속성을 포함한 소프트웨어 구성 요소에서 알려진 보안 취약점을 식별하는 자동화 도구.
  ​
* 워터마킹: AI 생성 콘텐츠의 출처를 추적하거나 AI 생성 여부를 감지하기 위해 인지할 수 없는 마커를 삽입하는 기술.
  ​
* 제로데이 취약점: 개발자가 수정 패치를 만들고 배포하기 전에 공격자가 악용할 수 있는 이전에 알려지지 않은 취약점.

