# Vedlegg A: Ordlista

Denne omfattende ordlisten gir definisjoner av nøkkelbegreper innen AI, ML og sikkerhet som brukes gjennom hele AISVS for å sikre klarhet og felles forståelse.

* Adversarialt eksempel: En input bevisst utformet for å få en AI-modell til å gjøre en feil, ofte ved å legge til subtile forstyrrelser som er uoppdagelige for mennesker.
  ​
* Adversarial Robustness – Adversarial robusthet i AI refererer til en modells evne til å opprettholde ytelsen og motstå å bli lurt eller manipulert av med vilje utformede, ondsinnede input som er designet for å forårsake feil.
  ​
* Agent – AI-agenter er programvaresystemer som bruker AI for å forfølge mål og utføre oppgaver på vegne av brukere. De viser resonnement, planlegging og hukommelse, og har et nivå av autonomi for å ta beslutninger, lære og tilpasse seg.
  ​
* Agentisk AI: AI-systemer som kan operere med en viss grad av autonomi for å oppnå mål, ofte ved å ta beslutninger og iverksette tiltak uten direkte menneskelig inngrep.
  ​
* Attributt-basert tilgangskontroll (ABAC): En tilgangskontrollparadigme hvor autorisasjonsbeslutninger baseres på attributter til bruker, ressurs, handling og miljø, evaluert ved spørringstidspunktet.
  ​
* Bakdørsangrep: En type dataforgiftningsangrep der modellen trenes til å reagere på en bestemt måte på visse triggere, samtidig som den oppfører seg normalt ellers.
  ​
* Skjevhet: Systematiske feil i AI-modellutdata som kan føre til urettferdige eller diskriminerende resultater for visse grupper eller i spesifikke kontekster.
  ​
* Biasutnyttelse: En angrepsteknikk som utnytter kjente skjevheter i AI-modeller for å manipulere resultater eller utfall.
  ​
* Cedar: Amazons policiespråk og motor for detaljert tillatelsesstyring brukt i implementeringen av ABAC for AI-systemer.
  ​
* Tankerekke: En teknikk for å forbedre resonnering i språkmodeller ved å generere mellomliggende resonnementstrinn før man produserer et endelig svar.
  ​
* Kretsbrytere: Mekanismer som automatisk stanser AI-systemoperasjoner når spesifikke risikogrenseverdier overskrides.
  ​
* Konfidensiell inferansetjeneste: En inferansetjeneste som kjører AI-modeller inne i et betrodd utførelsesmiljø (TEE) eller tilsvarende konfidensielt databehandlingsmekanisme, som sikrer at modellvekter og inferansedata forblir krypterte, forseglet og beskyttet mot uautorisert tilgang eller manipulering.
  ​
* Konfidensiell arbeidsbelastning: En AI-arbeidsbelastning (f.eks. trening, inferens, forhåndsbehandling) som kjører inne i et betrodd utførelsesmiljø (TEE) med maskinvarehåndhevet isolasjon, minnekryptering og fjernattestering for å beskytte kode, data og modeller mot tilgang fra vert eller sameier.
  ​
* Datautslipp: Utilsiktet eksponering av sensitiv informasjon gjennom AI-modellens resultater eller oppførsel.
  ​
* Datainjeksjon: Den bevisste forringelsen av treningsdata for å kompromittere modellens integritet, ofte for å installere bakdører eller redusere ytelsen.
  ​
* Differensiell personvern – Differensiell personvern er et matematisk strengt rammeverk for å offentliggjøre statistisk informasjon om datasett samtidig som det beskytter personvernet til individuelle dataobjekter. Det gjør det mulig for en dataeier å dele aggregerte mønstre i gruppen samtidig som det begrenser informasjon som lekkes om spesifikke individer.
  ​
* Innbetydninger: Tette vektorrepresentasjoner av data (tekst, bilder, etc.) som fanger semantisk mening i et høy-dimensjonalt rom.
  ​
* Forklarbarhet – Forklarbarhet i AI er evnen til et AI-system til å gi menneskeforståelige grunner for sine beslutninger og prediksjoner, og tilbyr innsikt i dets interne funksjoner.
  ​
* Forklarbar AI (XAI): AI-systemer designet for å gi menneskeforståelige forklaringer på deres beslutninger og atferd gjennom ulike teknikker og rammeverk.
  ​
* Federert læring: En maskinlæringstilnærming der modeller trenes på tvers av flere desentraliserte enheter som holder lokale datasett, uten å utveksle selve dataene.
  ​
* Formulering: Oppskriften eller metoden som brukes for å produsere en artefakt eller et datasett, slik som hyperparametere, treningskonfigurasjon, forhåndsbehandlingssteg eller byggeskript.
  ​
* Vernelinjer: Begrensninger implementert for å forhindre at AI-systemer produserer skadelige, partiske eller på annen måte uønskede resultater.
  ​
* Hallusinasjon – En AI-hallusinasjon refererer til et fenomen der en AI-modell genererer feilaktig eller misvisende informasjon som ikke er basert på treningsdataene eller den faktiske virkeligheten.
  ​
* Human-in-the-Loop (HITL): Systemer designet for å kreve menneskelig overvåking, verifisering eller inngripen på kritiske beslutningspunkter.
  ​
* Infrastruktur som kode (IaC): Håndtering og provisjonering av infrastruktur gjennom kode i stedet for manuelle prosesser, noe som muliggjør sikkerhetsskanning og konsistente distribusjoner.
  ​
* Jailbreak: Teknikker brukt for å omgå sikkerhetsbegrensninger i AI-systemer, spesielt i store språkmodeller, for å generere forbudt innhold.
  ​
* Minste privilegium: Sikkerhetsprinsippet om å gi kun de minste nødvendige tilgangsrettigheter til brukere og prosesser.
  ​
* LIME (Lokale tolkbare modell-uavhengige forklaringer): En teknikk for å forklare prediksjonene til hvilken som helst maskinlæringsklassifikator ved å tilnærme den lokalt med en tolkbar modell.
  ​
* MCP (Model Context Protocol): En protokoll som gjør det mulig for AI-modeller og agenter å få tilgang til eksterne verktøy, datakilder og ressurser ved å utveksle strukturerte, typede forespørsler og svar over et definert transportlag.
  ​
* Medlemskapsinformasjonsangrep: Et angrep som har som mål å avgjøre om et spesifikt datapunkt ble brukt til å trene en maskinlæringsmodell.
  ​
* MITRE ATLAS: Adversarial Threat Landscape for Artificial-Intelligence Systems; en kunnskapsbase over adversarielle taktikker og teknikker mot AI-systemer.
  ​
* Modellkort – Et modellkort er et dokument som gir standardisert informasjon om en AI-modells ytelse, begrensninger, tiltenkte bruksområder og etiske vurderinger for å fremme åpenhet og ansvarlig AI-utvikling.
  ​
* Modelluttrekk: Et angrep der en angriper gjentatte ganger sender forespørsler til en målmodell for å lage en funksjonelt lik kopi uten autorisasjon.
  ​
* Modellinversjon: Et angrep som forsøker å rekonstruere treningsdata ved å analysere modellutdata.
  ​
* Modell livssyklusstyring – AI-modell livssyklusstyring er prosessen med å overvåke alle faser av en AI-modells eksistens, inkludert design, utvikling, distribusjon, overvåking, vedlikehold og til slutt avvikling, for å sikre at den forblir effektiv og i samsvar med målsetningene.
  ​
* Modellforgiftning: Innføring av sårbarheter eller bakdører direkte i en modell under treningsprosessen.
  ​
* Modelltyveri: Å hente ut en kopi eller en tilnærming av en proprietær modell gjennom gjentatte forespørsler.
  ​
* Multi-agent system: Et system sammensatt av flere samhandlende AI-agenter, hver med potensielt forskjellige kapasiteter og mål.
  ​
* OPA (Open Policy Agent): En åpen kildekode policy-motor som muliggjør enhetlig håndheving av policy på tvers av hele teknologistakken.
  ​
* Personvernbevarende maskinlæring (PPML): Teknikker og metoder for å trene og distribuere ML-modeller samtidig som personvernet til treningsdataene beskyttes.
  ​
* Prompt Injection: Et angrep der ondsinnede instruksjoner er innebygd i input for å overstyre modellens tiltenkte oppførsel.
  ​
* RAG (Retrieval-Augmented Generation): En teknikk som forbedrer store språkmodeller ved å hente relevant informasjon fra eksterne kunnskapskilder før generering av et svar.
  ​
* Red-Teaming: Praksisen med aktivt å teste AI-systemer ved å simulere fiendtlige angrep for å identifisere sårbarheter.
  ​
* SBOM (Programvarematerialliste): En formell oversikt som inneholder detaljer og leverandørkjede-forhold for forskjellige komponenter brukt i bygging av programvare eller AI-modeller.
  ​
* SHAP (SHapley Additive exPlanations): En spillteoretisk tilnærming for å forklare utfallet av enhver maskinlæringsmodell ved å beregne bidraget til hver funksjon i prediksjonen.
  ​
* Sterk autentisering: Autentisering som motstår tyveri av legitimasjon og replay ved å kreve minst to faktorer (kunnskap, eierskap, iboende) og phishing-resistente mekanismer som FIDO2/WebAuthn, sertifikatbasert tjenesteautentisering eller kortvarige tokens.
  ​
* Forsyningskjedeangrep: Å kompromittere et system ved å rette mål mot mindre sikre elementer i forsyningskjeden, slik som tredjepartsbiblioteker, datasett eller forhåndstrente modeller.
  ​
* Overføring av læring: En teknikk hvor en modell utviklet for en oppgave gjenbrukes som utgangspunkt for en modell på en annen oppgave.
  ​
* Vektordatabase: En spesialisert database designet for å lagre høy-dimensjonale vektorer (innebygginger) og utføre effektive likhetssøk.
  ​
* Sårbarhetsskanning: Automatiserte verktøy som identifiserer kjente sikkerhetssårbarheter i programvarekomponenter, inkludert AI-rammeverk og avhengigheter.
  ​
* Vannmerking: Teknikkene for å legge inn umerkelige markører i AI-generert innhold for å spore opprinnelsen eller oppdage AI-generering.
  ​
* Zero-Day-sårbarhet: En tidligere ukjent sårbarhet som angripere kan utnytte før utviklere lager og distribuerer en oppdatering.

