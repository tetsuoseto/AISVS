# Appendix A: Glossary

This comprehensive glossary provides definitions of key AI, ML, and security terms used throughout the AISVS to ensure clarity and a common understanding.

* Adversarial Example: An input intentionally designed to cause an AI model to make an error, often by adding subtle perturbations that are imperceptible to humans.
  ​
* Adversarial Robustness – In AI, adversarial robustness refers to a model’s ability to maintain its performance and resist being deceived or manipulated by intentionally crafted malicious inputs designed to cause errors.
  ​
* Agent – AI agents are software systems that use artificial intelligence to pursue goals and complete tasks on behalf of users. They demonstrate reasoning, planning, and memory abilities and possess a level of autonomy to make decisions, learn, and adapt.
  ​
* Agentic AI: AI systems that can operate with a certain degree of autonomy to achieve goals, often making decisions and taking actions without direct human intervention.
  ​
* Attribute-Based Access Control (ABAC): An access control model where authorization decisions are made based on attributes of the user, resource, action, and environment, evaluated at the time of the request.
  ​
* Backdoor Attack: A form of data poisoning attack where the model is trained to respond in a specific way to certain triggers while behaving normally in all other situations.
  ​
* Bias: Systematic errors in AI model outputs that can result in unfair or discriminatory outcomes for certain groups or specific contexts.
  ​
* Bias Exploitation: An attack technique that leverages known biases in AI models to manipulate outputs or outcomes.
  ​
* Cedar: Amazon's policy language and engine for fine-grained permissions used in implementing Attribute-Based Access Control (ABAC) for AI systems.
  ​
* Chain of Thought: A technique for enhancing reasoning in language models by generating intermediate reasoning steps before providing a final answer.
  ​
* Circuit Breakers: Mechanisms that automatically stop AI system operations when specific risk thresholds are exceeded.
  ​
* Data Leakage: Unintentional exposure of sensitive information through AI model outputs or behavior.
  ​
* Data Poisoning: The intentional corruption of training data to compromise model integrity, often to introduce backdoors or reduce performance.
  ​
* Differential Privacy – Differential privacy is a mathematically rigorous framework for releasing statistical information about datasets while protecting the privacy of individual data subjects. It allows a data holder to share aggregate group patterns while limiting the information leaked about specific individuals.
  ​
* Embeddings: Dense vector representations of data (such as text, images, etc.) that capture semantic meaning within a high-dimensional space.
  ​
* Explainability in AI refers to an AI system's ability to provide reasons for its decisions and predictions that humans can easily understand, offering insights into how it operates internally.
  ​
* Explainable AI (XAI): AI systems designed to offer human-understandable explanations for their decisions and behaviors using various techniques and frameworks.
  ​
* Federated Learning: A machine learning approach in which models are trained across multiple decentralized devices that hold local data samples, without exchanging the data itself.
  ​
* Guardrails: Constraints implemented to prevent AI systems from generating harmful, biased, or otherwise undesirable outputs.
  ​
* Hallucination – An AI hallucination refers to a phenomenon in which an AI model produces incorrect or misleading information that is not derived from its training data or factual reality.
  ​
* Human-in-the-Loop (HITL): Systems designed to require human oversight, verification, or intervention at critical decision points.
  ​
* Infrastructure as Code (IaC): Managing and provisioning infrastructure using code instead of manual processes, enabling security scanning and consistent deployments.
  ​
* Jailbreak: Techniques used to bypass safety guardrails in AI systems, especially in large language models, to generate prohibited content.
  ​
* Least Privilege: The security principle of granting users and processes only the minimum access rights necessary.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): A technique for explaining the predictions of any machine learning classifier by locally approximating it with an interpretable model.
  ​
* Membership Inference Attack: An attack that aims to determine whether a specific data point was included in the training of a machine learning model.
  ​
* MITRE ATLAS: Adversarial Threat Landscape for Artificial Intelligence Systems; a knowledge base of adversarial tactics and techniques targeting AI systems.
  ​
* Model Card – A model card is a document that provides standardized information about an AI model’s performance, limitations, intended uses, and ethical considerations to promote transparency and responsible AI development.
  ​
* Model Extraction: An attack in which an adversary repeatedly queries a target model to create an unauthorized, functionally similar copy.
  ​
* Model Inversion: An attack that attempts to reconstruct training data by analyzing a model's outputs.
  ​
* Model Lifecycle Management – AI Model Lifecycle Management is the process of overseeing all stages of an AI model’s existence, including its design, development, deployment, monitoring, maintenance, and eventual retirement, to ensure it remains effective and aligned with its objectives.
  ​
* Model Poisoning: Introducing vulnerabilities or backdoors directly into a model during the training process.
  ​
* Model Stealing/Theft: Obtaining a copy or an approximation of a proprietary model by making repeated queries.
  ​
* Multi-agent System: A system consisting of multiple interacting AI agents, each potentially having different capabilities and goals.
  ​
* OPA (Open Policy Agent): An open-source policy engine that enables unified policy enforcement throughout the stack.
  ​
* Privacy-Preserving Machine Learning (PPML): Techniques and methods for training and deploying ML models while safeguarding the privacy of the training data.
  ​
* Prompt Injection: An attack in which malicious instructions are embedded in inputs to override a model's intended behavior.
  ​
* RAG (Retrieval-Augmented Generation): A technique that improves large language models by retrieving relevant information from external knowledge sources before generating a response.
  ​
* Red-Teaming: The practice of actively testing AI systems by simulating adversarial attacks to identify vulnerabilities.
  ​
* SBOM (Software Bill of Materials): A formal record that includes the details and supply chain relationships of the various components used in building software or AI models.
  ​
* SHAP (SHapley Additive exPlanations): A game-theoretic approach to explain the output of any machine learning model by calculating the contribution of each feature to the prediction.
  ​
* Supply Chain Attack: Compromising a system by targeting less secure components in its supply chain, such as third-party libraries, datasets, or pre-trained models.
  ​
* Transfer Learning: A technique in which a model developed for one task is reused as the starting point for a model on a different task.
  ​
* Vector Database: A specialized database designed to store high-dimensional vectors (embeddings) and efficiently perform similarity searches.
  ​
* Vulnerability Scanning: Automated tools that detect known security vulnerabilities in software components, including AI frameworks and dependencies.
  ​
* Watermarking: Techniques for embedding imperceptible markers in AI-generated content to track its origin or detect AI generation.
  ​
* Zero-day vulnerability: A previously unknown security flaw that attackers can exploit before developers create and release a patch.

