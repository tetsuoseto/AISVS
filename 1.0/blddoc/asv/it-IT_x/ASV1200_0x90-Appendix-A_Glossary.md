# Appendice A: Glossario

>Questo glossario completo fornisce definizioni dei principali termini di AI, ML e sicurezza utilizzati in tutto l'AISVS per garantire chiarezza e comprensione comune.

* Esempio avversario: un input appositamente creato per far commettere un errore a un modello di IA, spesso aggiungendo lievi perturbazioni impercettibili agli esseri umani.
  ​
* Robustezza avversaria – La robustezza avversaria nell'IA si riferisce alla capacità di un modello di mantenere le proprie prestazioni e di resistere a essere ingannato o manipolato da input intenzionalmente progettati e malevoli, creati per causare errori.
  ​
* Agente – Gli agenti AI sono sistemi software che utilizzano l'intelligenza artificiale per perseguire obiettivi e completare compiti per conto degli utenti. Mostrano capacità di ragionamento, pianificazione e memoria e possiedono un livello di autonomia per prendere decisioni, apprendere e adattarsi.
  ​
* Agentic AI: sistemi di intelligenza artificiale che possono operare con un certo grado di autonomia per raggiungere obiettivi, spesso prendendo decisioni e compiendo azioni senza intervento umano diretto.
  ​
* Controllo degli Accessi Basato su Attributi (ABAC): Un paradigma di controllo degli accessi in cui le decisioni di autorizzazione si basano sugli attributi dell'utente, della risorsa, dell'azione e dell'ambiente, valutati al momento della richiesta.
  ​
* Attacco Backdoor: Un tipo di attacco di avvelenamento dei dati in cui il modello viene addestrato a rispondere in un modo specifico a determinati trigger mentre si comporta normalmente altrimenti.
  ​
* Bias: Errori sistematici nei risultati dei modelli di IA che possono portare a risultati ingiusti o discriminatori per determinati gruppi o in specifici contesti.
  ​
* Sfruttamento dei bias: una tecnica di attacco che sfrutta i bias noti nei modelli di intelligenza artificiale per manipolare risultati o output.
  ​
* Cedar: il linguaggio di policy e motore di Amazon per permessi granulati utilizzato nell'implementazione di ABAC per sistemi di intelligenza artificiale.
  ​
* Catena di pensiero: una tecnica per migliorare il ragionamento nei modelli linguistici generando passaggi intermedi di ragionamento prima di produrre una risposta finale.
  ​
* Interruttori automatici: meccanismi che interrompono automaticamente le operazioni del sistema di intelligenza artificiale quando vengono superate soglie di rischio specifiche.
  ​
* Perdita di dati: esposizione non intenzionale di informazioni sensibili tramite output o comportamento del modello AI.
  ​
* Avvelenamento dei dati: la corruzione deliberata dei dati di addestramento per compromettere l'integrità del modello, spesso per installare backdoor o degradare le prestazioni.
  ​
* Privacy Differenziale – La privacy differenziale è un quadro matematicamente rigoroso per la divulgazione di informazioni statistiche su insiemi di dati proteggendo la privacy dei singoli soggetti coinvolti. Consente a un detentore di dati di condividere modelli aggregati del gruppo limitando le informazioni che possono essere divulgate su individui specifici.
  ​
* Incorporamenti: rappresentazioni vettoriali dense di dati (testo, immagini, ecc.) che catturano il significato semantico in uno spazio ad alta dimensione.
  ​
* Spiegabilità – La spiegabilità nell'IA è la capacità di un sistema di intelligenza artificiale di fornire motivazioni comprensibili dall'uomo per le sue decisioni e previsioni, offrendo approfondimenti sul suo funzionamento interno.
  ​
* Intelligenza Artificiale Spiegabile (XAI): sistemi di IA progettati per fornire spiegazioni comprensibili all'essere umano riguardo alle loro decisioni e comportamenti attraverso varie tecniche e framework.
  ​
* Apprendimento Federato: un approccio di machine learning in cui i modelli vengono addestrati su più dispositivi decentralizzati che possiedono campioni di dati locali, senza scambiare i dati stessi.
  ​
* Guardrails: vincoli implementati per impedire ai sistemi di intelligenza artificiale di produrre output dannosi, faziosi o altrimenti indesiderati.
  ​
* Allucinazione – Un’allucinazione dell’IA si riferisce a un fenomeno in cui un modello di intelligenza artificiale genera informazioni errate o fuorvianti che non sono basate sui suoi dati di addestramento o sulla realtà fattuale.
  ​
* Human-in-the-Loop (HITL): Sistemi progettati per richiedere supervisione, verifica o intervento umano nei punti critici decisionali.
  ​
* Infrastructure as Code (IaC): Gestione e provisioning dell'infrastruttura tramite codice invece di processi manuali, consentendo la scansione della sicurezza e distribuzioni coerenti.
  ​
* Jailbreak: Tecniche utilizzate per eludere le barriere di sicurezza nei sistemi di intelligenza artificiale, in particolare nei modelli linguistici di grandi dimensioni, per produrre contenuti proibiti.
  ​
* Privilegio minimo: il principio di sicurezza che prevede di concedere solo i diritti di accesso minimi necessari agli utenti e ai processi.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): una tecnica per spiegare le predizioni di qualsiasi classificatore di apprendimento automatico approssimandolo localmente con un modello interpretabile.
  ​
* Attacco di Inferenza sull'Appartenenza: un attacco che mira a determinare se un punto dati specifico è stato utilizzato per addestrare un modello di machine learning.
  ​
* MITRE ATLAS: Panorama delle minacce avversarie per i sistemi di intelligenza artificiale; una base di conoscenza delle tattiche e tecniche avversarie contro i sistemi di IA.
  ​
* Scheda del Modello – Una scheda del modello è un documento che fornisce informazioni standardizzate sulle prestazioni di un modello di intelligenza artificiale, le sue limitazioni, gli usi previsti e le considerazioni etiche per promuovere la trasparenza e uno sviluppo responsabile dell'IA.
  ​
* Estrazione del Modello: Un attacco in cui un avversario interroga ripetutamente un modello target per creare una copia funzionalmente simile senza autorizzazione.
  ​
* Inversione del modello: un attacco che tenta di ricostruire i dati di addestramento analizzando gli output del modello.
  ​
* Gestione del Ciclo di Vita del Modello – La Gestione del Ciclo di Vita del Modello AI è il processo di supervisione di tutte le fasi dell'esistenza di un modello AI, inclusi progettazione, sviluppo, implementazione, monitoraggio, manutenzione e eventuale ritiro, per garantire che rimanga efficace e allineato agli obiettivi.
  ​
* Avvelenamento del Modello: Introduzione di vulnerabilità o backdoor direttamente in un modello durante il processo di addestramento.
  ​
* Furto/Rubare il Modello: Estrarre una copia o un'approssimazione di un modello proprietario tramite richieste ripetute.
  ​
* Sistema multi-agente: un sistema composto da più agenti IA interagenti, ciascuno con potenzialmente capacità e obiettivi diversi.
  ​
* OPA (Open Policy Agent): Un motore di policy open-source che consente l'applicazione unificata delle policy attraverso l'intero stack.
  ​
* Apprendimento Automatico che Preserva la Privacy (PPML): Tecniche e metodi per addestrare e distribuire modelli di ML proteggendo la privacy dei dati di addestramento.
  ​
* Iniezione di prompt: un attacco in cui istruzioni dannose sono incorporate negli input per sovrascrivere il comportamento previsto di un modello.
  ​
* RAG (Generazione Integrata con Recupero): Una tecnica che migliora i modelli di linguaggio di grandi dimensioni recuperando informazioni rilevanti da fonti di conoscenza esterne prima di generare una risposta.
  ​
* Red-Teaming: La pratica di testare attivamente i sistemi di IA simulando attacchi avversari per identificare le vulnerabilità.
  ​
* SBOM (Software Bill of Materials): Un registro formale contenente i dettagli e le relazioni della catena di fornitura di vari componenti utilizzati nella creazione di software o modelli di Intelligenza Artificiale.
  ​
* SHAP (SHapley Additive exPlanations): Un approccio teorico dei giochi per spiegare l'output di qualsiasi modello di machine learning calcolando il contributo di ciascuna caratteristica alla previsione.
  ​
* Attacco alla catena di approvvigionamento: compromettere un sistema prendendo di mira elementi meno sicuri della sua catena di approvvigionamento, come librerie di terze parti, set di dati o modelli pre-addestrati.
  ​
* Apprendimento per trasferimento: Una tecnica in cui un modello sviluppato per un compito viene riutilizzato come punto di partenza per un modello su un secondo compito.
  ​
* Database vettoriale: un database specializzato progettato per memorizzare vettori ad alta dimensionalità (embedding) ed eseguire ricerche di similarità efficienti.
  ​
* Scansione delle vulnerabilità: Strumenti automatizzati che identificano vulnerabilità di sicurezza note nei componenti software, inclusi framework di IA e dipendenze.
  ​
* Watermarking: Tecniche per inserire marker impercettibili nei contenuti generati dall'IA per tracciare la loro origine o rilevare la generazione da parte dell'IA.
  ​
* Vulnerabilità Zero-Day: Una vulnerabilità precedentemente sconosciuta che gli aggressori possono sfruttare prima che gli sviluppatori creino e distribuiscano una patch.

