# Frontespizio

## Informazioni sullo standard

Lo Standard di Verifica della Sicurezza dell'Intelligenza Artificiale (AISVS) è un catalogo guidato dalla comunità di requisiti di sicurezza che scienziati dei dati, ingegneri MLOps, architetti del software, sviluppatori, tester, professionisti della sicurezza, fornitori di strumenti, regolatori e consumatori possono utilizzare per progettare, costruire, testare e verificare sistemi e applicazioni abilitati all'IA affidabili. Fornisce un linguaggio comune per specificare i controlli di sicurezza lungo l'intero ciclo di vita dell'IA, dalla raccolta dei dati e dallo sviluppo del modello fino al dispiegamento e al monitoraggio continuo, in modo che le organizzazioni possano misurare e migliorare la resilienza, la privacy e la sicurezza delle loro soluzioni di IA.

## Diritti d'autore e licenza

Versione 0.1 (Prima bozza pubblica - in sviluppo), 2025  

![license](../images/license.png)

Diritti d'autore © 2025 The AISVS Project.  

Rilasciato sotto la[Creative Commons Attribution‑ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).  
Per qualsiasi riutilizzo o distribuzione, devi comunicare chiaramente i termini di licenza di questa opera agli altri.

## Responsabili di progetto

|            |                         |
| ---------- | ----------------------- |
| Jim Manico | Aras “Russ” Memisyazici |

## Contributori e Revisori

|                                    |                             |
| ---------------------------------- | --------------------------- |
| https://github.com/ottosulin       | https://github.com/mbhatt1  |
| https://github.com/vineethsai      | https://github.com/cciprofm |
| https://github.com/deepakrpandey12 |                             |

---

AISVS è uno standard nuovissimo creato appositamente per affrontare le sfide di sicurezza uniche dei sistemi di intelligenza artificiale. Pur prendendo ispirazione dalle migliori pratiche di sicurezza più ampie, ogni requisito di AISVS è stato sviluppato ex novo per riflettere il panorama delle minacce legate all'IA e per aiutare le organizzazioni a costruire soluzioni di IA più sicure e robuste.

