# Приложение A: Глоссарий

>This полный глоссарий предоставляет определения основных терминов в области искусственного интеллекта (ИИ), машинного обучения (МО) и кибербезопасности, используемых на протяжении AISVS, для обеспечения ясности и общего понимания.

* Адверсариальный пример: вход, специально созданный, чтобы привести модель искусственного интеллекта (ИИ) к ошибке, часто путём добавления тонких возмущений, незаметных человеку.
  ​
* Устойчивость к атакующим примерам – в области искусственного интеллекта относится к способности модели сохранять свою производительность и сопротивляться тому, чтобы её вводили в заблуждение или манипуливали ею преднамеренно созданными вредоносными входными данными, призванными вызвать ошибки.
  ​
* Агент – ИИ-агенты являются программными системами, которые используют ИИ для достижения целей и выполнения задач от имени пользователей. Они демонстрируют рассуждения, планирование и память и обладают уровнем автономии для принятия решений, обучения и адаптации.
  ​
* ИИ-агент: ИИ-системы, которые могут действовать с определённой степенью автономии для достижения целей, часто принимая решения и предпринимая действия без прямого вмешательства человека.
  ​
* Контроль доступа на основе атрибутов (ABAC): парадигма контроля доступа, при которой решения об авторизации основаны на атрибутах пользователя, ресурса, действия и окружения, которые оцениваются во время запроса.
  ​
* Атака через бэкдор: тип атаки на данные, при которой модель обучается реагировать определённым образом на определённые триггеры, а в остальное время ведёт себя нормально.
  ​
* Смещение: систематические ошибки в выводах моделей ИИ, которые могут приводить к несправедливым или дискриминационным результатам для определённых групп или в конкретных контекстах.
  ​
* Эксплуатация предвзятости: техника атаки, которая использует известные предвзятости в моделях ИИ для манипулирования их выходами или результатами.
  ​
* Cedar: язык политик Amazon и движок для детализированных разрешений, используемых в реализации ABAC в системах искусственного интеллекта.
  ​
* Цепочка рассуждений: метод улучшения рассуждений языковых моделей путём генерации промежуточных шагов рассуждений перед выдачей окончательного ответа.
  ​
* Аварийные выключатели: механизмы, которые автоматически останавливают работу систем искусственного интеллекта (ИИ), когда превышены конкретные пороги риска.
  ​
* Утечка данных: непреднамеренное раскрытие конфиденциальной информации через выходы модели ИИ или поведение.
  ​
* Отравление данных: Преднамеренное искажение обучающих данных с целью подорвать целостность модели, часто для внедрения бэкдоров или снижения производительности.
  ​
* Дифференциальная приватность – математически строгий рамочный подход к публикации статистической информации о наборах данных при обеспечении конфиденциальности отдельных субъектов данных. Он позволяет владельцу данных делиться агрегированными закономерностями группы, одновременно ограничивая информацию, которая может быть раскрыта о конкретных лицах.
  ​
* Эмбеддинги: плотные векторные представления данных (текста, изображений и т. п.), которые отражают семантику в пространстве высокой размерности.
  ​
* Объяснимость – Объяснимость в ИИ является способностью системы ИИ предоставлять понятные человеку обоснования своих решений и предсказаний, давая представление о внутренних механизмах системы.
  ​
* Объяснимый ИИ (XAI): системы искусственного интеллекта, разработанные для предоставления человеку понятных объяснений своих решений и поведения с помощью различных техник и рамок.
  ​
* Федеративное обучение: подход машинного обучения, при котором модели обучаются на нескольких децентрализованных устройствах, на которых хранятся локальные образцы данных, без обмена самими данными.
  ​
* Защитные рамки: ограничения, внедрённые для предотвращения того, чтобы системы ИИ генерировали вредные, предвзятые или иным образом нежелательные выходы.
  ​
* Галлюцинация – это феномен, при котором модель ИИ генерирует неверную или вводящую в заблуждение информацию, не основанную на её обучающих данных или фактической реальности.
  ​
* Человек в петле (HITL): Системы, разработанные так, чтобы требовать человеческий надзор, проверку или вмешательство на критических этапах принятия решений.
  ​
* Инфраструктура как код (IaC): управление и развёртывание инфраструктуры через код вместо ручных процессов, обеспечивая сканирование безопасности и последовательные развёртывания.
  ​
* Джейлбрейк: методы обхода защитных барьеров безопасности в системах ИИ, особенно в больших языковых моделях, для создания запрещённого контента.
  ​
* Принцип наименьших привилегий: принцип безопасности, заключающийся в предоставлении пользователям и процессам только минимально необходимых прав доступа.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): метод объяснения предсказаний любого классификатора машинного обучения путём локальной аппроксимации его интерпретируемой моделью.
  ​
* Атака на принадлежность к обучающему набору данных: цель — определить, была ли конкретная точка данных использована при обучении модели машинного обучения.
  ​
* MITRE ATLAS: Ландшафт угроз со стороны противников для систем искусственного интеллекта; база знаний по тактикам и техникам противников, направленным против систем ИИ.
  ​
* Карточка модели – это документ, который предоставляет стандартизированную информацию о показателях эффективности модели ИИ, её ограничениях, предполагаемом использовании и этических соображениях, с целью повышения прозрачности и ответственного развития ИИ.
  ​
* Экстракция модели: атака, при которой злоумышленник повторно делает запросы к целевой модели, чтобы создать функционально аналогичную копию без авторизации.
  ​
* Инверсия модели: атака, направленная на восстановление обучающих данных путём анализа выходов модели.
  ​
* Управление жизненным циклом модели – Управление жизненным циклом модели ИИ – это процесс надзора за всеми стадиями существования модели ИИ, включая её проектирование, разработку, развёртывание, мониторинг, обслуживание и последующее снятие с эксплуатации, чтобы обеспечить её эффективность и соответствие целям.
  ​
* Отравление модели: внедрение уязвимостей или задних дверей непосредственно в модель во время обучения.
  ​
* Кража модели: извлечение копии или приближённой версии проприетарной модели путём повторных запросов.
  ​
* Мульти-агентная система: система, состоящая из нескольких взаимодействующих агентов ИИ, каждый из которых может обладать различными возможностями и целями.
  ​
* OPA (Open Policy Agent): движок политик с открытым исходным кодом, который обеспечивает единообразное применение политик по всему стеку.
  ​
* Машинное обучение с сохранением конфиденциальности (PPML): техники и методы обучения и развертывания ML-моделей при защите конфиденциальности обучающих данных.
  ​
* Инъекция подсказки: атака, при которой вредоносные инструкции внедряются во входные данные с целью обойти запрограммированное поведение модели.
  ​
* RAG (Retrieval-Augmented Generation): Техника, которая улучшает крупные языковые модели за счет извлечения релевантной информации из внешних источников знаний до формирования ответа.
  ​
* Red-Teaming: Практика активного тестирования систем ИИ путём моделирования адверсариальных атак для выявления уязвимостей.
  ​
* SBOM (Software Bill of Materials): Формальный реестр, содержащий сведения и связи цепочки поставок различных компонентов, используемых при создании программного обеспечения или моделей ИИ.
  ​
* SHAP (SHapley Additive exPlanations): игрово-теоретический подход к объяснению вывода любой модели машинного обучения путём вычисления вклада каждого признака в предсказание.
  ​
* Атака цепочки поставок: компрометация системы путём целевой атаки на менее защищённые элементы её цепочки поставок, таких как сторонние библиотеки, наборы данных или предобученные модели.
  ​
* Перенос обучения: техника, при которой модель, разработанная для одной задачи, повторно используется в качестве отправной точки для модели, решающей вторую задачу.
  ​
* Векторная база данных: специализированная база данных, предназначенная для хранения векторов высокой размерности (эмбеддингов) и эффективного поиска по близости.
  ​
* Сканирование уязвимостей: автоматизированные инструменты, выявляющие известные уязвимости в программных компонентах, включая ИИ-фреймворки и зависимости.
  ​
* Водяной знак: методы внедрения незаметных маркеров в контент, созданный ИИ, для отслеживания его происхождения или обнаружения генерации ИИ.
  ​
* Уязвимость нулевого дня: ранее неизвестная уязвимость, которую злоумышленники могут использовать до того, как разработчики создадут и развернут патч.

