# Приложение A: Глоссарий

>Этот всеобъемлющий глоссарий содержит определения ключевых терминов в области ИИ, МО и безопасности, используемых в AISVS для обеспечения ясности и общего понимания.

* Противоправный пример: входные данные, намеренно созданные для того, чтобы вызвать ошибку в работе модели искусственного интеллекта, часто путем добавления тонких возмущений, незаметных для человека.
  ​
* Противостояние атакам – Противостояние атакам в ИИ относится к способности модели сохранять свою производительность и противостоять обману или манипуляциям со стороны специально созданных, вредоносных входных данных, предназначенных для вызова ошибок.
  ​
* Агент — AI-агенты — это программные системы, которые используют ИИ для достижения целей и выполнения задач от имени пользователей. Они проявляют умозаключения, планирование и память, а также обладают уровнем автономности для принятия решений, обучения и адаптации.
  ​
* Агентный ИИ: системы искусственного интеллекта, которые могут функционировать с определенной степенью автономии для достижения целей, зачастую принимая решения и предпринимая действия без прямого вмешательства человека.
  ​
* Управление доступом на основе атрибутов (ABAC): парадигма контроля доступа, при которой решения об авторизации принимаются на основе атрибутов пользователя, ресурса, действия и окружения, оцениваемых во время запроса.
  ​
* Атака с закладкой: тип атаки отравления данных, при которой модель обучается реагировать определённым образом на определённые триггеры, при этом ведя себя нормально в остальных случаях.
  ​
* Смещение: систематические ошибки в выводах моделей ИИ, которые могут привести к несправедливым или дискриминационным результатам для определённых групп или в конкретных контекстах.
  ​
* Эксплуатация смещений: техника атаки, которая использует известные смещения в моделях ИИ для манипулирования результатами или выводами.
  ​
* Cedar: язык политики и движок Amazon для детализированных разрешений, используемые при реализации ABAC для ИИ-систем.
  ​
* Цепочка рассуждений: метод улучшения логического мышления в языковых моделях путем генерации промежуточных шагов рассуждения перед выведением окончательного ответа.
  ​
* Прерыватель цепи: Механизмы, которые автоматически останавливают работу системы ИИ при превышении определённых порогов риска.
  ​
* Утечка данных: непреднамеренное раскрытие конфиденциальной информации через выходные данные или поведение ИИ-модели.
  ​
* Отравление данных: намеренное искажение обучающих данных с целью подрыва целостности модели, часто для установки "черных ходов" или ухудшения производительности.
  ​
* Дифференциальная приватность – дифференциальная приватность представляет собой математически строгую основу для предоставления статистической информации о наборах данных с сохранением конфиденциальности отдельных субъектов данных. Она позволяет владельцу данных делиться агрегированными паттернами группы, ограничивая при этом утечку информации о конкретных лицах.
  ​
* Встраивания: плотные векторные представления данных (текста, изображений и т.д.), которые отражают семантическое значение в высокоразмерном пространстве.
  ​
* Объяснимость – объяснимость в ИИ – это способность системы ИИ предоставлять человеку понятные причины своих решений и прогнозов, предлагая понимание её внутреннего механизма.
  ​
* Объяснимая ИИ (XAI): системы искусственного интеллекта, разработанные для предоставления человеком понятных объяснений своих решений и поведения с помощью различных техник и рамок.
  ​
* Федеративное обучение: метод машинного обучения, при котором модели обучаются на нескольких децентрализованных устройствах, хранящих локальные данные, без обмена самими данными.
  ​
* Ограничения: Ограничения, внедренные для предотвращения генерации вредоносных, предвзятых или иным образом нежелательных результатов искусственными интеллектуальными системами.
  ​
* Галлюцинация – Галлюцинация ИИ относится к явлению, при котором модель ИИ генерирует неправильную или вводящую в заблуждение информацию, не основанную на её учебных данных или фактической реальности.
  ​
* Человек в цикле (HITL): Системы, разработанные с требованием человеческого надзора, проверки или вмешательства в ключевых точках принятия решений.
  ​
* Инфраструктура как код (IaC): управление и предоставление инфраструктуры посредством кода вместо ручных процессов, что позволяет проводить проверку безопасности и обеспечивать последовательные развертывания.
  ​
* Jailbreak: Техники, используемые для обхода защитных ограничений в системах ИИ, особенно в больших языковых моделях, с целью создания запрещенного контента.
  ​
* Минимальные привилегии: принцип безопасности, заключающийся в предоставлении пользователям и процессам только минимально необходимых прав доступа.
  ​
* LIME (Локальные интерпретируемые модели-независимые объяснения): метод объяснения предсказаний любого классификатора машинного обучения путем локального приближения его интерпретируемой моделью.
  ​
* Атака на определение членства: атака, целью которой является установление того, использовалась ли конкретная точка данных для обучения модели машинного обучения.
  ​
* MITRE ATLAS: Ландшафт враждебных угроз для систем искусственного интеллекта; база знаний о враждебных тактиках и техниках против систем ИИ.
  ​
* Карта модели – это документ, который предоставляет стандартизированную информацию о производительности модели ИИ, ограничениях, предполагаемых областях применения и этических аспектах для содействия прозрачности и ответственному развитию ИИ.
  ​
* Извлечение модели: атака, при которой злоумышленник многократно отправляет запросы целевой модели с целью создать функционально похожую копию без разрешения.
  ​
* Инверсия модели: атака, направленная на восстановление обучающих данных путем анализа выходных данных модели.
  ​
* Управление жизненным циклом модели – это процесс контроля всех этапов существования ИИ-модели, включая её проектирование, разработку, внедрение, мониторинг, обслуживание и последующий вывод из эксплуатации, с целью обеспечения её эффективности и соответствия поставленным задачам.
  ​
* Отравление модели: введение уязвимостей или скрытых каналов непосредственно в модель в процессе обучения.
  ​
* Кража/Похищение модели: Извлечение копии или приближения проприетарной модели посредством повторных запросов.
  ​
* Мультиагентная система: система, состоящая из нескольких взаимодействующих ИИ-агентов, каждый из которых может иметь разные способности и цели.
  ​
* OPA (Open Policy Agent): Открытый движок политик с открытым исходным кодом, обеспечивающий единообразное применение политик во всем стеке.
  ​
* Защищённое конфиденциальностью машинное обучение (PPML): методы и подходы для обучения и развертывания моделей машинного обучения с сохранением конфиденциальности обучающих данных.
  ​
* Инъекция запросов: атака, при которой в вводимые данные внедряются вредоносные инструкции с целью переопределения преднамеренного поведения модели.
  ​
* RAG (Retrieval-Augmented Generation): Техника, которая улучшает большие языковые модели за счет извлечения релевантной информации из внешних источников знаний перед генерацией ответа.
  ​
* Red-Teaming: практика активного тестирования ИИ-систем путем моделирования враждебных атак для выявления уязвимостей.
  ​
* SBOM (спецификация программных компонентов): Формальная запись, содержащая детали и цепочки поставок различных компонентов, используемых при создании программного обеспечения или моделей ИИ.
  ​
* SHAP (SHapley Additive exPlanations): Теоретико-игровой подход для объяснения результата любой модели машинного обучения путем вычисления вклада каждого признака в предсказание.
  ​
* Атака на цепочку поставок: Компрометация системы путем нацеливания на менее защищенные элементы в её цепочке поставок, такие как сторонние библиотеки, наборы данных или предварительно обученные модели.
  ​
* Трансферное обучение: метод, при котором модель, разработанная для одной задачи, используется в качестве отправной точки для модели, решающей вторую задачу.
  ​
* Векторная база данных: специализированная база данных, предназначенная для хранения высокоразмерных векторов (эмбеддингов) и выполнения эффективного поиска по сходству.
  ​
* Сканирование на уязвимости: Автоматизированные инструменты, которые выявляют известные уязвимости безопасности в программных компонентах, включая AI-фреймворки и зависимости.
  ​
* Водяные знаки: методы встраивания незаметных маркеров в контент, сгенерированный ИИ, для отслеживания его происхождения или обнаружения генерации ИИ.
  ​
* Уязвимость нулевого дня: ранее неизвестная уязвимость, которую злоумышленники могут использовать до того, как разработчики создадут и выпустят исправление.

