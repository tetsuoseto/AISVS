# Приложение A: Глоссарий

>Этот всеобъемлющий глоссарий содержит определения ключевых терминов ИИ, МО и безопасности, используемых в течение всего AISVS для обеспечения ясности и общего понимания.

* Пример противодействия: входные данные, специально созданные для того, чтобы вызвать ошибку модели ИИ, часто путём добавления тонких возмущений, незаметных для человека.
  ​
* Противостояние атакам – Противостояние атакам в ИИ относится к способности модели сохранять свою производительность и сопротивляться ошибкам или манипуляциям, вызванным намеренно созданными вредоносными входными данными, предназначенными для вызова ошибок.
  ​
* Агент – AI-агенты — это программные системы, использующие искусственный интеллект для достижения целей и выполнения задач от имени пользователей. Они демонстрируют рассуждения, планирование и память, а также обладают уровнем автономии для принятия решений, обучения и адаптации.
  ​
* Агентный ИИ: ИИ-системы, которые могут функционировать с определенной степенью автономии для достижения целей, часто принимая решения и выполняя действия без непосредственного вмешательства человека.
  ​
* Контроль доступа на основе атрибутов (ABAC): парадигма контроля доступа, при которой решения об авторизации принимаются на основе атрибутов пользователя, ресурса, действия и среды, оцениваемых в момент запроса.
  ​
* Атака с использованием "закладки": тип атаки с отравлением данных, при которой модель обучается реагировать определённым образом на определённые триггеры, при этом в остальном ведёт себя нормально.
  ​
* Смещение: Систематические ошибки в выводах модели ИИ, которые могут привести к несправедливым или дискриминационным результатам для определённых групп или в конкретных контекстах.
  ​
* Эксплуатация смещений: техника атаки, которая использует известные смещения в моделях ИИ для манипулирования результатами или выводами.
  ​
* Cedar: Языковая политика и движок Amazon для детальных разрешений, используемые при реализации ABAC для AI-систем.
  ​
* Цепочка рассуждений: техника улучшения логики в языковых моделях путем генерации промежуточных шагов рассуждения перед формированием окончательного ответа.
  ​
* Автоматические прерыватели: механизмы, которые автоматически останавливают работу системы ИИ при превышении определённых порогов риска.
  ​
* Утечка данных: непреднамеренное раскрытие конфиденциальной информации через результаты работы ИИ-модели или ее поведение.
  ​
* Отравление данных: преднамеренное искажение обучающих данных с целью нарушить целостность модели, часто для установки задних дверей или снижения производительности.
  ​
* Дифференциальная приватность – Дифференциальная приватность представляет собой математически строгую основу для публикации статистической информации о наборах данных при защите конфиденциальности отдельных субъектов данных. Она позволяет владельцу данных делиться агрегированными характеристиками группы, ограничивая при этом утечку информации о конкретных лицах.
  ​
* Встраивания: плотные векторные представления данных (текст, изображения и т. д.), которые захватывают семантическое значение в пространстве высокой размерности.
  ​
* Объяснимость – Объяснимость в ИИ — это способность системы искусственного интеллекта предоставлять человеку понятные причины своих решений и прогнозов, предоставляя понимание её внутренней работы.
  ​
* Объяснимая ИИ (XAI): системы искусственного интеллекта, разработанные для предоставления человекопонятных объяснений своих решений и поведения с использованием различных методов и рамок.
  ​
* Федеративное обучение: подход к машинному обучению, при котором модели обучаются на нескольких децентрализованных устройствах, содержащих локальные образцы данных, без обмена самими данными.
  ​
* Ограничения: Ограничения, внедренные для предотвращения генерации вредных, предвзятых или иным образом нежелательных результатов системами ИИ.
  ​
* Галлюцинация – Галлюцинация искусственного интеллекта означает явление, при котором модель ИИ генерирует неправильную или вводящую в заблуждение информацию, которая не основана на её тренировочных данных или фактической реальности.
  ​
* Человек в цикле (HITL): Системы, разработанные для обеспечения участия человека в контроле, проверке или вмешательстве на ключевых этапах принятия решений.
  ​
* Инфраструктура как код (IaC): управление и предоставление инфраструктуры с помощью кода вместо ручных процессов, что обеспечивает возможность сканирования безопасности и последовательных развертываний.
  ​
* Jailbreak: Техники, используемые для обхода защитных ограничений в системах искусственного интеллекта, особенно в больших языковых моделях, с целью создания запрещённого контента.
  ​
* Наименьшие привилегии: принцип безопасности, предусматривающий предоставление только минимально необходимых прав доступа для пользователей и процессов.
  ​
* LIME (локально интерпретируемые объяснения, не зависящие от модели): техника объяснения предсказаний любого классификатора машинного обучения путем локального приближения его интерпретируемой моделью.
  ​
* Атака на вывод членства: атака, целью которой является определить, был ли конкретный элемент данных использован для обучения модели машинного обучения.
  ​
* MITRE ATLAS: Ландшафт враждебных угроз для систем искусственного интеллекта; база знаний о враждебных тактиках и методах против систем ИИ.
  ​
* Карточка модели – это документ, который предоставляет стандартизированную информацию о производительности модели ИИ, её ограничениях, предполагаемых сферах применения и этических аспектах для содействия прозрачности и ответственному развитию ИИ.
  ​
* Извлечение модели: атака, при которой злоумышленник многократно отправляет запросы целевой модели для создания функционально аналогичной копии без разрешения.
  ​
* Инверсия модели: атака, которая пытается восстановить тренировочные данные путем анализа выходных данных модели.
  ​
* Управление жизненным циклом модели – управление жизненным циклом AI модели представляет собой процесс контроля всех этапов существования AI модели, включая её проектирование, разработку, развертывание, мониторинг, обслуживание и окончательный вывод из эксплуатации, с целью обеспечения её эффективности и соответствия поставленным задачам.
  ​
* Отравление модели: введение уязвимостей или «черных ходов» непосредственно в модель в процессе обучения.
  ​
* Кража/Похищение модели: извлечение копии или приближенного варианта проприетарной модели через многократные запросы.
  ​
* Мультиагентная система: система, состоящая из нескольких взаимодействующих ИИ-агентов, каждый из которых может обладать разными возможностями и целями.
  ​
* OPA (Open Policy Agent): Открытый движок политик, который обеспечивает единообразное применение политик на всех уровнях стека.
  ​
* Сохранение конфиденциальности в машинном обучении (PPML): методы и подходы для обучения и развертывания моделей машинного обучения с защитой конфиденциальности данных обучения.
  ​
* Внедрение подсказок: Атака, при которой злонамеренные инструкции внедряются во входные данные для переопределения предназначенного поведения модели.
  ​
* RAG (генерация с помощью дополненного поиска): техника, улучшающая большие языковые модели за счет извлечения релевантной информации из внешних источников знаний перед генерацией ответа.
  ​
* Red-Teaming: практика активного тестирования ИИ-систем путем моделирования враждебных атак для выявления уязвимостей.
  ​
* SBOM (перечень компонентов программного обеспечения): официальный документ, содержащий детали и цепочки поставок различных компонентов, используемых при создании программного обеспечения или моделей искусственного интеллекта.
  ​
* SHAP (SHapley Additive exPlanations): Теоретический подход из теории игр для объяснения результата любой модели машинного обучения путем вычисления вклада каждого признака в предсказание.
  ​
* Атака на цепочку поставок: компрометация системы путем нацеливания на менее защищенные элементы в ее цепочке поставок, такие как сторонние библиотеки, наборы данных или предварительно обученные модели.
  ​
* Трансферное обучение: техника, при которой модель, разработанная для одной задачи, используется в качестве отправной точки для модели по второй задаче.
  ​
* Векторная база данных: специализированная база данных, предназначенная для хранения высокоразмерных векторов (встраиваний) и выполнения эффективного поиска по сходству.
  ​
* Сканирование на уязвимости: Автоматизированные инструменты, которые выявляют известные уязвимости в компонентах программного обеспечения, включая AI-фреймворки и зависимости.
  ​
* Водяные знаки: методы внедрения незаметных меток в созданный ИИ контент для отслеживания его происхождения или обнаружения генерации ИИ.
  ​
* Уязвимость нулевого дня: ранее неизвестная уязвимость, которую злоумышленники могут использовать до того, как разработчики создадут и выпустят патч.

