# Appendix A: Glossary

This comprehensive glossary provides definitions of key AI, ML, and security terms used throughout the AISVS to ensure clarity and a common understanding.

* Adversarial Example: An input intentionally designed to cause an AI model to make an error, often by adding subtle perturbations that are imperceptible to humans.
  ​
* Adversarial Robustness – In AI, adversarial robustness refers to a model's ability to maintain its performance and resist being deceived or manipulated by intentionally crafted malicious inputs designed to cause errors.
  ​
* Agent – AI agents are software systems that use artificial intelligence to pursue goals and complete tasks on behalf of users. They demonstrate reasoning, planning, and memory and possess a level of autonomy to make decisions, learn, and adapt.
  ​
* Agentic AI: AI systems capable of operating with a certain level of autonomy to achieve goals, often making decisions and taking actions without direct human intervention.
  ​
* Attribute-Based Access Control (ABAC): An access control model where authorization decisions are made based on the attributes of the user, resource, action, and environment, evaluated at the time of the query.
  ​
* Backdoor Attack: A type of data poisoning attack in which the model is trained to respond in a specific way to certain triggers while otherwise behaving normally.
  ​
* Bias: Systematic errors in AI model outputs that may result in unfair or discriminatory outcomes for certain groups or in specific contexts.
  ​
* Bias Exploitation: An attack technique that leverages known biases in AI models to manipulate outputs or outcomes.
  ​
* Cedar: Amazon's policy language and engine for fine-grained permissions used to implement ABAC in AI systems.
  ​
* Chain of Thought: A technique for enhancing reasoning in language models by generating intermediate reasoning steps before providing a final answer.
  ​
* Circuit Breakers: Mechanisms that automatically stop AI system operations when specific risk thresholds are exceeded.
  ​
* Data Leakage: The unintended exposure of sensitive information through AI model outputs or behavior.
  ​
* Data Poisoning: The intentional corruption of training data to compromise model integrity, often to install backdoors or degrade performance.
  ​
* Differential Privacy – Differential privacy is a mathematically rigorous framework for releasing statistical information about datasets while protecting the privacy of individual data subjects. It enables a data holder to share aggregate group patterns while limiting the information leaked about specific individuals.
  ​
* Embeddings: Dense vector representations of data (such as text, images, etc.) that capture semantic meaning within a high-dimensional space.
  ​
* Explainability in AI refers to an AI system’s ability to provide reasons for its decisions and predictions that humans can understand, offering insights into how the system operates internally.
  ​
* Explainable AI (XAI): AI systems designed to offer human-understandable explanations for their decisions and behaviors using various techniques and frameworks.
  ​
* Federated Learning: A machine learning method where models are trained across multiple decentralized devices that hold local data samples, without sharing the data itself.
  ​
* Guardrails: Constraints implemented to prevent AI systems from generating harmful, biased, or otherwise undesirable outputs.
  ​
* Hallucination – An AI hallucination refers to a phenomenon in which an AI model produces incorrect or misleading information that is not based on its training data or factual reality.
  ​
* Human-in-the-Loop (HITL): Systems designed to require human oversight, verification, or intervention at critical decision points.
  ​
* Infrastructure as Code (IaC): Managing and provisioning infrastructure through code rather than manual processes, enabling security scanning and consistent deployments.
  ​
* Jailbreak: Techniques used to bypass safety measures in AI systems, especially in large language models, to generate prohibited content.
  ​
* Least Privilege: The security principle of granting users and processes only the minimum necessary access rights.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): A technique to explain the predictions of any machine learning classifier by locally approximating it with an interpretable model.
  ​
* Membership Inference Attack: An attack that seeks to determine whether a particular data point was included in the training of a machine learning model.
  ​
* MITRE ATLAS: Adversarial Threat Landscape for Artificial Intelligence Systems; a knowledge base of adversarial tactics and techniques targeting AI systems.
  ​
* Model Card – A model card is a document that provides standardized information about an AI model's performance, limitations, intended uses, and ethical considerations to promote transparency and responsible AI development.
  ​
* Model Extraction: An attack in which an adversary repeatedly queries a target model to create an unauthorized, functionally similar copy.
  ​
* Model Inversion: An attack that tries to reconstruct training data by analyzing the outputs of a model.
  ​
* Model Lifecycle Management – AI Model Lifecycle Management is the process of overseeing all phases of an AI model's existence, including its design, development, deployment, monitoring, maintenance, and eventual retirement, to ensure it remains effective and aligned with its objectives.
  ​
* Model Poisoning: Introducing vulnerabilities or backdoors directly into a model during the training process.
  ​
* Model Stealing/Theft: Obtaining a copy or approximation of a proprietary model by making repeated queries.
  ​
* Multi-agent System: A system composed of multiple interacting AI agents, each potentially having different capabilities and goals.
  ​
* OPA (Open Policy Agent): An open-source policy engine that enables unified policy enforcement across the stack.
  ​
* Privacy-Preserving Machine Learning (PPML): Techniques and methods for training and deploying ML models while protecting the privacy of the training data.
  ​
* Prompt Injection: An attack in which harmful instructions are embedded in inputs to override a model's intended behavior.
  ​
* RAG (Retrieval-Augmented Generation): A technique that improves large language models by retrieving relevant information from external knowledge sources prior to generating a response.
  ​
* Red-Teaming: The practice of actively testing AI systems by simulating adversarial attacks to identify vulnerabilities.
  ​
* SBOM (Software Bill of Materials): A formal record that includes the details and supply chain relationships of the various components used in developing software or AI models.
  ​
* SHAP (SHapley Additive exPlanations): A game-theoretic approach to explain the output of any machine learning model by calculating the contribution of each feature to the prediction.
  ​
* Supply Chain Attack: Compromising a system by targeting less secure elements in its supply chain, such as third-party libraries, datasets, or pre-trained models.
  ​
* Transfer Learning: A technique in which a model developed for one task is reused as the foundation for a model on a second task.
  ​
* Vector Database: A specialized database designed to store high-dimensional vectors (embeddings) and perform efficient similarity searches.
  ​
* Vulnerability Scanning: Automated tools that detect known security vulnerabilities in software components, including AI frameworks and dependencies.
  ​
* Watermarking: Techniques for embedding imperceptible markers in AI-generated content to trace its origin or detect AI generation.
  ​
* Zero-Day Vulnerability: A previously unknown vulnerability that attackers can exploit before developers create and release a patch.

