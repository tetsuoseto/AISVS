# Appendix A: Glossary

>This comprehensive glossary provides definitions of key AI, ML, and security terms used throughout the AISVS to ensure clarity and common understanding.

* Adversarial example: An input deliberately crafted to cause an AI model to make a mistake, often by introducing subtle perturbations that are imperceptible to humans.
  ​
* Adversarial Robustness – In AI, adversarial robustness refers to a model's ability to maintain its performance and resist being fooled or manipulated by intentionally crafted, malicious inputs designed to cause errors.
  ​
* Agent – AI agents are software systems that use AI to pursue goals and complete tasks on behalf of users. They exhibit reasoning, planning, and memory, and they possess a degree of autonomy to make decisions, learn, and adapt.
  ​
* Agentic AI: AI systems that operate with some autonomy to achieve goals, often making decisions and taking actions without direct human intervention.
  ​
* Attribute-Based Access Control (ABAC): an access-control paradigm in which authorization decisions are based on the attributes of the user, the resource, the action, and the environment, evaluated at query time.
  ​
* Backdoor attack: a type of data-poisoning attack in which the model is trained to respond in a specific way to certain triggers, while behaving normally otherwise.
  ​
* Bias: Systematic errors in AI model outputs that can lead to unfair or discriminatory outcomes for certain groups or in specific contexts.
  ​
* Bias exploitation: An attack technique that exploits known biases in AI models to manipulate outputs or outcomes.
  ​
* Cedar is Amazon's policy language and engine for fine-grained permissions used to implement ABAC for AI systems.
  ​
* Chain of Thought: a technique for improving reasoning in language models by generating intermediate reasoning steps before producing a final answer.
  ​
* Circuit Breakers: Mechanisms that automatically halt AI system operations when specific risk thresholds are exceeded.
  ​
* Data leakage: Unintended exposure of sensitive information through AI model outputs or behaviors.
  ​
* Data poisoning is the deliberate corruption of training data intended to compromise a model's integrity, often to install backdoors or degrade performance.
  ​
* Differential privacy is a mathematically rigorous framework for releasing statistical information about datasets while protecting the privacy of individual data subjects. It enables data holders to share aggregate patterns of the group while limiting information leaked about specific individuals.
  ​
* Embeddings: dense vector representations of data (text, images, etc.) that capture semantic meaning in a high-dimensional space.
  ​
* Explainability in AI is the ability of an AI system to provide human-understandable reasons for its decisions and predictions, offering insights into its internal workings.
  ​
* Explainable AI (XAI): AI systems designed to provide human-understandable explanations for their decisions and behaviors through various techniques and frameworks.
  ​
* Federated learning is a machine learning approach in which models are trained across multiple decentralized devices that hold local data samples, without exchanging the data itself.
  ​
* Guardrails: Constraints designed to prevent AI systems from producing outputs that are harmful, biased, or otherwise undesirable.
  ​
* Hallucination – An AI hallucination refers to a phenomenon in which an AI model generates information that is incorrect or misleading and that is not based on its training data or factual reality.
  ​
* Human-in-the-Loop (HITL): Systems that require human oversight, verification, or intervention at crucial decision points.
  ​
* Infrastructure as Code (IaC): The practice of managing and provisioning infrastructure through code rather than manual processes, enabling security scanning and consistent deployments.
  ​
* Jailbreak: Techniques for circumventing safety guardrails in AI systems, especially in large language models, to produce prohibited content.
  ​
* Least Privilege: The security principle of granting only the minimum necessary access rights to users and processes.
  ​
* LIME (Local Interpretable Model-agnostic Explanations) is a technique for explaining the predictions of any machine learning classifier by approximating it locally with an interpretable model.
  ​
* Membership Inference Attack: A type of attack that aims to determine whether a specific data point was used to train a machine learning model.
  ​
* MITRE ATLAS: Adversarial Threat Landscape for Artificial-Intelligence Systems, a knowledge base of adversarial tactics and techniques against AI systems.
  ​
* Model Card – A model card is a document that provides standardized information about an AI model's performance, limitations, intended uses, and ethical considerations to promote transparency and responsible AI development.
  ​
* Model extraction: An attack in which an adversary repeatedly queries a target model to create a functionally similar copy without authorization.
  ​
* Model inversion: an attack that attempts to reconstruct training data by analyzing the model's outputs.
  ​
* Model lifecycle management – AI model lifecycle management is the process of overseeing all stages of an AI model's existence, including its design, development, deployment, monitoring, maintenance, and eventual retirement, to ensure it remains effective and aligned with objectives.
  ​
* Model Poisoning: Introducing vulnerabilities or backdoors directly into a model during training.
  ​
* Model stealing/theft: Extracting a copy or an approximation of a proprietary model by repeatedly querying it.
  ​
* A multi-agent system is a system composed of multiple interacting AI agents, each with potentially different capabilities and goals.
  ​
* OPA (Open Policy Agent): an open-source policy engine that enables unified policy enforcement across the stack.
  ​
* Privacy-Preserving Machine Learning (PPML): Techniques and methods for training and deploying ML models while protecting the privacy of the training data.
  ​
* Prompt injection: An attack in which malicious instructions are embedded in inputs to override a model's intended behavior.
  ​
* RAG (Retrieval-Augmented Generation) is a technique that enhances large language models by retrieving relevant information from external knowledge sources before generating a response.
  ​
* Red-Teaming: the practice of actively testing AI systems by simulating adversarial attacks to identify vulnerabilities.
  ​
* SBOM (Software Bill of Materials): A formal record detailing the components and their supply chain relationships used to build software or AI models.
  ​
* SHAP (SHapley Additive exPlanations): A game-theoretic approach for explaining the output of any machine learning model by computing each feature's contribution to the prediction.
  ​
* Supply-chain attack: compromising a system by targeting less-secure elements in its supply chain, such as third-party libraries, datasets, or pre-trained models.
  ​
* Transfer learning: A technique in which a model developed for one task is reused as the starting point for a model intended for a second task.
  ​
* Vector Database: A specialized database designed to store high-dimensional vectors (embeddings) and to perform efficient similarity searches.
  ​
* Vulnerability scanning: Automated tools that identify known security vulnerabilities in software components, including AI frameworks and their dependencies.
  ​
* Watermarking: Techniques for embedding imperceptible markers in AI-generated content to track its origin or detect AI generation.
  ​
* Zero-day vulnerability: a previously unknown vulnerability that attackers can exploit before developers create and deploy a patch.

