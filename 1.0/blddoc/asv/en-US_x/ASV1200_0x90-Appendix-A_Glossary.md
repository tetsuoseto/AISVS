# Appendix A: Glossary

This comprehensive glossary provides definitions of key AI, ML, and security terms used throughout the AISVS to ensure clarity and a common understanding.

* Adversarial Example: An input intentionally designed to cause an AI model to make an error, often by adding subtle perturbations that are imperceptible to humans.
  ​
* Adversarial Robustness – In AI, adversarial robustness refers to a model's ability to maintain performance and resist being deceived or manipulated by intentionally crafted malicious inputs designed to cause errors.
  ​
* Agent – AI agents are software systems that use artificial intelligence to pursue goals and complete tasks on behalf of users. They demonstrate reasoning, planning, and memory and possess a level of autonomy to make decisions, learn, and adapt.
  ​
* Agentic AI: AI systems capable of operating with a certain degree of autonomy to achieve goals, often making decisions and taking actions without direct human intervention.
  ​
* Attribute-Based Access Control (ABAC): An access control model where authorization decisions are made based on attributes of the user, resource, action, and environment, evaluated at the time of the query.
  ​
* Backdoor Attack: A type of data poisoning attack in which the model is trained to respond in a specific way to certain triggers while behaving normally otherwise.
  ​
* Bias: Systematic errors in AI model outputs that can result in unfair or discriminatory outcomes for certain groups or in specific contexts.
  ​
* Bias Exploitation: An attack technique that leverages known biases in AI models to manipulate outputs or results.
  ​
* Cedar: Amazon's policy language and engine for fine-grained permissions used to implement ABAC in AI systems.
  ​
* Chain of Thought: A technique for enhancing reasoning in language models by generating intermediate reasoning steps prior to producing a final answer.
  ​
* Circuit Breakers: Mechanisms that automatically stop AI system operations when specific risk thresholds are exceeded.
  ​
* Data Leakage: The unintended exposure of sensitive information through AI model outputs or behavior.
  ​
* Data Poisoning: The intentional corruption of training data to compromise model integrity, often to insert backdoors or reduce performance.
  ​
* Differential Privacy – Differential privacy is a mathematically rigorous framework for releasing statistical information about datasets while protecting the privacy of individual data subjects. It allows data holders to share aggregate group patterns while limiting the information that can be inferred about specific individuals.
  ​
* Embeddings are dense vector representations of data (such as text, images, etc.) that capture semantic meaning in a high-dimensional space.
  ​
* Explainability in AI refers to the capability of an AI system to provide reasons for its decisions and predictions that are understandable to humans, offering insights into its internal processes.
  ​
* Explainable AI (XAI): AI systems designed to offer human-understandable explanations for their decisions and behaviors using various techniques and frameworks.
  ​
* Federated Learning: A machine learning approach in which models are trained across multiple decentralized devices holding local data samples, without exchanging the data itself.
  ​
* Guardrails: Constraints implemented to prevent AI systems from generating harmful, biased, or otherwise undesirable outputs.
  ​
* Hallucination – An AI hallucination refers to a phenomenon where an AI model produces incorrect or misleading information that is not based on its training data or factual reality.
  ​
* Human-in-the-Loop (HITL): Systems designed to require human oversight, verification, or intervention at critical decision points.
  ​
* Infrastructure as Code (IaC): Managing and provisioning infrastructure using code instead of manual processes, enabling security scanning and consistent deployments.
  ​
* Jailbreak: Techniques used to bypass safety guardrails in AI systems, especially in large language models, to generate prohibited content.
  ​
* Least Privilege: The security principle of granting users and processes only the minimum access rights necessary.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): A technique for explaining the predictions of any machine learning classifier by locally approximating it with an interpretable model.
  ​
* Membership Inference Attack: An attack that seeks to determine whether a specific data point was used to train a machine learning model.
  ​
* MITRE ATLAS: Adversarial Threat Landscape for Artificial Intelligence Systems; a knowledge base of adversarial tactics and techniques targeting AI systems.
  ​
* Model Card – A model card is a document that provides standardized information about an AI model’s performance, limitations, intended uses, and ethical considerations to promote transparency and responsible AI development.
  ​
* Model Extraction: An attack in which an adversary repeatedly queries a target model to create an unauthorized, functionally similar copy.
  ​
* Model Inversion: An attack that attempts to reconstruct training data by analyzing the outputs of a model.
  ​
* Model Lifecycle Management – AI Model Lifecycle Management refers to the process of overseeing all stages of an AI model's existence, including its design, development, deployment, monitoring, maintenance, and eventual retirement, to ensure it remains effective and aligned with its objectives.
  ​
* Model Poisoning: Introducing vulnerabilities or backdoors directly into a model during the training process.
  ​
* Model Stealing/Theft: Obtaining a copy or an approximation of a proprietary model by making repeated queries.
  ​
* Multi-agent System: A system consisting of multiple interacting AI agents, each potentially having different capabilities and goals.
  ​
* OPA (Open Policy Agent): An open-source policy engine that enables unified policy enforcement throughout the stack.
  ​
* Privacy-Preserving Machine Learning (PPML): Techniques and methods for training and deploying ML models while protecting the privacy of the training data.
  ​
* Prompt Injection: An attack in which malicious instructions are embedded in inputs to override a model's intended behavior.
  ​
* RAG (Retrieval-Augmented Generation): A technique that improves large language models by retrieving relevant information from external knowledge sources before generating a response.
  ​
* Red Teaming: The practice of actively testing AI systems by simulating adversarial attacks to identify vulnerabilities.
  ​
* SBOM (Software Bill of Materials): A formal record that includes the details and supply chain relationships of various components used in building software or AI models.
  ​
* SHAP (SHapley Additive exPlanations): A game-theoretic approach that explains the output of any machine learning model by calculating each feature's contribution to the prediction.
  ​
* Supply Chain Attack: Compromising a system by targeting less secure elements in its supply chain, such as third-party libraries, datasets, or pre-trained models.
  ​
* Transfer Learning: A technique in which a model developed for one task is reused as the starting point for a model on a different task.
  ​
* Vector Database: A specialized database designed to store high-dimensional vectors (embeddings) and to perform efficient similarity searches.
  ​
* Vulnerability Scanning: Automated tools that detect known security vulnerabilities in software components, including AI frameworks and dependencies.
  ​
* Watermarking: Techniques for embedding imperceptible markers in AI-generated content to track its origin or detect AI generation.
  ​
* Zero-Day Vulnerability: A previously unknown security flaw that attackers can exploit before developers create and release a patch.

