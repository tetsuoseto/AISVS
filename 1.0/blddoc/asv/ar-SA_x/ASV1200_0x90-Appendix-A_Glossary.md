# الملحق أ: قائمة المصطلحات

>This معجم شامل يقدم تعريفات للمصطلحات الرئيسية في الذكاء الاصطناعي والتعلم الآلي والأمن المستخدم عبر AISVS لضمان الوضوح والفهم المشترك.

* مثال عدائي: مدخل مصمم عمدًا لإحداث خطأ في نموذج الذكاء الاصطناعي، غالبًا من خلال إضافة تشويهات دقيقة لا يمكن ملاحظتها للبشر.
  ​
* المتانة ضد الهجمات العدائية – المتانة ضد الهجمات العدائية في الذكاء الاصطناعي تشير إلى قدرة النموذج على الحفاظ على أدائه ومقاومة أن يُخدع أو يُتلاعب به من قِبل مدخلات مُصمَّمة عمداً وخبيثة بهدف التسبب في أخطاء.
  ​
* الوكيل – وكلاء الذكاء الاصطناعي هم أنظمة برمجية تستخدم الذكاء الاصطناعي لتحقيق الأهداف وإنجاز المهام نيابة عن المستخدمين. يظهرون الاستدلال والتخطيط والذاكرة ولهم مستوى من الاستقلالية لاتخاذ القرارات والتعلم والتكيف.
  ​
* ذكاء اصطناعي ذو وكالة: أنظمة ذكاء اصطناعي يمكنها العمل بقدر من الاستقلال لتحقيق الأهداف، غالبًا ما تتخذ قرارات وتتصرف دون تدخل بشري مباشر.
  ​
* التحكّم بالوصول القائم على السمات (ABAC): نمط تحكّم بالوصول يتم فيه اعتماد قرارات التفويض بناءً على سمات المستخدم والموارد والإجراء والبيئة، وتُقيَّم في وقت الاستعلام.
  ​
* هجوم باب خلفي: نوع من هجمات تسميم البيانات حيث يتم تدريب النموذج على الاستجابة بطريقة محددة لمحفزات معينة، بينما يتصرف بشكل طبيعي في الحالات الأخرى.
  ​
* التحيز: أخطاء منهجية في مخرجات نموذج الذكاء الاصطناعي يمكن أن تؤدي إلى نتائج غير عادلة أو تمييزية لفئات معينة أو في سياقات محددة.
  ​
* استغلال الانحياز: تقنية هجوم تستغل الانحيازات المعروفة في نماذج الذكاء الاصطناعي للتلاعب بالمخرجات أو النتائج.
  ​
* Cedar: لغة سياسات أمازون ومحركها للأذونات الدقيقة المفصّلة المستخدمة في تنفيذ ABAC لأنظمة الذكاء الاصطناعي.
  ​
* سلسلة التفكير: تقنية لتحسين الاستدلال في نماذج اللغة من خلال توليد خطوات استدلال وسيطة قبل إنتاج الإجابة النهائية.
  ​
* قواطع الدائرة: آليات توقف تشغيل أنظمة الذكاء الاصطناعي تلقائياً عندما تتجاوز عتبات المخاطر المحددة.
  ​
* تسرب البيانات: الكشف غير المقصود عن معلومات حساسة من خلال مخرجات نموذج الذكاء الاصطناعي أو سلوكه.
  ​
* تلويث البيانات: التلاعب المتعمد ببيانات التدريب من أجل المساس بسلامة النموذج، وغالبًا ما يكون ذلك بغرض تثبيت باب خلفي أو تدهور الأداء.
  ​
* الخصوصية التفاضلية – الخصوصية التفاضلية إطار رياضي صارم لنشر معلومات إحصائية عن مجموعات البيانات مع حماية خصوصية الأفراد. إنها تتيح لمالك البيانات مشاركة أنماط مجمَّعة للمجموعة مع الحد من المعلومات التي يتم تسريبها عن أفراد بعينهم.
  ​
* التضمينات: تمثيلات متجهية كثيفة للبيانات (النصوص، الصور، وغيرها) تلتقط المعنى الدلالي في فضاء عالي الأبعاد.
  ​
* قابلية التفسير – قابلية التفسير في الذكاء الاصطناعي هي قدرة نظام ذكاء اصطناعي على تقديم أسباب يمكن فهمها للبشر لقراراته وتوقعاته، مع توفير رؤى حول آليات عمله الداخلية.
  ​
* الذكاء الاصطناعي القابل للتفسير (XAI): أنظمة ذكاء اصطناعي مصممة لتوفير تفسيرات قابلة للفهم من قبل البشر لقراراتها وسلوكها من خلال تقنيات وأطر مختلفة.
  ​
* التعلم الفيدرالي: نهج تعلم آلي يتم فيه تدريب النماذج عبر أجهزة لا مركزية متعددة تحمل عينات بيانات محلية، دون تبادل البيانات نفسها.
  ​
* ضوابط: قيود منفذة لمنع أنظمة الذكاء الاصطناعي من إنتاج مخرجات ضارة أو متحيزة أو غير مرغوب فيها بأي شكل.
  ​
* الهلوسة – تشير الهلوسة في الذكاء الاصطناعي إلى ظاهرة حيث يولّد نموذج الذكاء الاصطناعي معلومات غير صحيحة أو مضللة لا تستند إلى بيانات تدريبه أو إلى الواقع الفعلي.
  ​
* الإنسان في الحلقة (HITL): أنظمة مصممة لتتطلب إشرافًا بشريًا، والتحقق، أو التدخل عند نقاط اتخاذ القرار الحاسمة.
  ​
* البنية التحتية كرمز (IaC): إدارة وتوفير البنية التحتية من خلال الشفرة بدلاً من العمليات اليدوية، مما يمكّن من فحص الأمان وعمليات النشر المتسقة.
  ​
* كسر الحماية: تقنيات تُستخدم لتجاوز حواجز السلامة في أنظمة الذكاء الاصطناعي، خاصة في نماذج اللغة الكبيرة، لإنتاج محتوى محظور.
  ​
* مبدأ الأقل امتيازًا: المبدأ الأمني الذي يمنح فقط الحد الأدنى من حقوق الوصول اللازمة للمستخدمين والعمليات.
  ​
* LIME (تفسيرات محلية قابلة للتفسير ومستقلة عن النموذج): تقنية لشرح توقعات أي مصنف تعلم آلي عن طريق تقريبه محلياً بنموذج قابل للتفسير.
  ​
* هجوم استنتاج الانتماء: هجوم يهدف إلى تحديد ما إذا كانت نقطة بيانات محددة قد استخدمت في تدريب نموذج تعلم آلي.
  ​
* MITRE ATLAS: مشهد التهديدات العدائية لأنظمة الذكاء الاصطناعي؛ قاعدة معرفة للأساليب والتقنيات العدائية ضد أنظمة الذكاء الاصطناعي.
  ​
* بطاقة النموذج – هي وثيقة توفر معلومات موحدة عن أداء نموذج الذكاء الاصطناعي وحدوده، واستخداماته المقصودة، والاعتبارات الأخلاقية، من أجل تعزيز الشفافية وتطوير الذكاء الاصطناعي بشكل مسؤول.
  ​
* استخراج النموذج: هجوم يقوم فيه خصم بطرح استفسارات متكررة على نموذج مستهدف بهدف إنشاء نسخة وظيفياً مشابهة دون إذن.
  ​
* هجوم انعكاس النموذج: يحاول إعادة بناء بيانات التدريب من خلال تحليل مخرجات النموذج.
  ​
* إدارة دورة حياة النموذج – إدارة دورة حياة نموذج الذكاء الاصطناعي هي العملية التي تتولى الإشراف على جميع مراحل وجود نموذج الذكاء الاصطناعي، بما في ذلك تصميمه وتطويره ونشره ومراقبته وصيانته والتقاعد النهائي، لضمان بقائه فعالاً ومتوافقاً مع الأهداف.
  ​
* تلويث النموذج: إدخال ثغرات أو باب خلفي مباشرةً في النموذج أثناء عملية التدريب.
  ​
* سرقة/انتهاك النموذج: استخراج نسخة أو تقريب من نموذج مملوك من خلال استفسارات متكررة.
  ​
* نظام متعدد الوكلاء: نظام مكوّن من عدة وكلاء ذكاء اصطناعي يتفاعلون مع بعضهم البعض، كل واحد منهم قد يمتلك قدرات وأهداف مختلفة محتملة.
  ​
* OPA (Open Policy Agent): محرك سياسات مفتوح المصدر يتيح تنفيذ سياسات موحدة عبر كامل المكدس.
  ​
* التعلم الآلي المحافظ على الخصوصية (PPML): تقنيات وطرق لتدريب ونشر نماذج التعلم الآلي مع حماية خصوصية بيانات التدريب.
  ​
* حقن الموجه: هجوم يتم فيه تضمين تعليمات خبيثة في المدخلات لتجاوز السلوك المقصود للنموذج.
  ​
* RAG (التوليد المعزز بالاسترجاع): تقنية تعمل على تعزيز نماذج اللغة الكبيرة من خلال استرجاع معلومات ذات صلة من مصادر معرفة خارجية قبل توليد الاستجابة.
  ​
* الاختبار-الأحمر: ممارسة اختبار أنظمة الذكاء الاصطناعي بنشاط من خلال محاكاة هجمات عدائية لتحديد الثغرات.
  ​
* SBOM (قائمة المواد البرمجية): سجل رسمي يحتوي على التفاصيل وعلاقات سلسلة التوريد للمكونات المختلفة المستخدمة في بناء البرمجيات أو نماذج الذكاء الاصطناعي.
  ​
* SHAP (SHapley Additive exPlanations): نهج مستند إلى نظرية الألعاب لشرح مخرجات أي نموذج تعلم آلي من خلال حساب مساهمة كل ميزة في التنبؤ.
  ​
* هجوم سلسلة التوريد: اختراق النظام من خلال استهداف عناصر أقل أماناً في سلسلة التوريد الخاصة به، مثل مكتبات الطرف الثالث، ومجموعات البيانات، أو نماذج مدربة مسبقاً.
  ​
* التعلم بالنقل: تقنية يتم فيها استخدام نموذج تم تطويره لمهمة واحدة كنقطة انطلاق لنموذج لمهمة ثانية.
  ​
* قاعدة بيانات المتجهات: قاعدة بيانات متخصصة مصممة لتخزين المتجهات عالية الأبعاد (تمثيلات التضمين) وتنفيذ عمليات بحث تشابه فعالة.
  ​
* مسح الثغرات الأمنية: أدوات آلية تحدد الثغرات الأمنية المعروفة في مكوّنات البرمجيات، بما في ذلك أطر العمل الخاصة بالذكاء الاصطناعي والتبعيات.
  ​
* إضافة علامة مائية: تقنيات لإدراج علامات غير ملحوظة في المحتوى الناتج عن الذكاء الاصطناعي لتتبع مصدره أو الكشف عن توليده بواسطة الذكاء الاصطناعي.
  ​
* ثغرة يوم الصفر: ثغرة غير معروفة سابقاً يمكن للمهاجمين استغلالها قبل أن يقوم المطورون بإنشاء ونشر التصحيح.

