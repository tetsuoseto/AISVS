# الملحق أ: مسرد المصطلحات

>يقدم هذا المسرد الشامل تعريفات للمصطلحات الرئيسية في الذكاء الاصطناعي، وتعلم الآلة، والأمان المستخدمة في جميع أنحاء AISVS لضمان الوضوح والفهم المشترك.

* مثال عدائي: إدخال مصمم عمدًا لجعل نموذج الذكاء الاصطناعي يرتكب خطأً، غالبًا من خلال إضافة اضطرابات دقيقة غير ملحوظة للبشر.
  ​
* الصلابة ضد الهجمات العدائية – تشير الصلابة ضد الهجمات العدائية في الذكاء الاصطناعي إلى قدرة النموذج على الحفاظ على أدائه ومقاومة أن يُخدع أو يُتلاعب به من خلال مدخلات خبيثة مصممة عمدًا لإحداث أخطاء.
  ​
* الوكيل – الوكلاء الذكيون هم أنظمة برمجية تستخدم الذكاء الاصطناعي لتحقيق الأهداف وإتمام المهام نيابةً عن المستخدمين. يظهرون القدرة على التفكير، والتخطيط، والذاكرة، ولديهم درجة من الاستقلالية لاتخاذ القرارات، والتعلم، والتكيف.
  ​
* الذكاء الاصطناعي الوكلي: أنظمة الذكاء الاصطناعي التي يمكنها العمل بدرجة من الاستقلالية لتحقيق الأهداف، وغالبًا ما تتخذ قرارات وتتخذ إجراءات دون تدخل بشري مباشر.
  ​
* التحكم في الوصول بناءً على السمات (ABAC): هو نموذج تحكم في الوصول حيث تعتمد قرارات التفويض على سمات المستخدم، والموارد، والإجراء، والبيئة، ويتم تقييمها في وقت الاستعلام.
  ​
* هجوم الباب الخلفي: نوع من هجمات تسميم البيانات يتم فيه تدريب النموذج على الاستجابة بطريقة محددة لمؤثرات معينة مع التصرف بشكل طبيعي في الحالات الأخرى.
  ​
* التحيز: أخطاء منهجية في مخرجات نموذج الذكاء الاصطناعي يمكن أن تؤدي إلى نتائج غير عادلة أو تمييزية لفئات معينة أو في سياقات محددة.
  ​
* استغلال التحيز: تقنية هجوم تستغل التحيزات المعروفة في نماذج الذكاء الاصطناعي للتلاعب بالمخرجات أو النتائج.
  ​
* Cedar: لغة السياسات ومحركها من أمازون للأذونات الدقيقة المستخدمة في تنفيذ التحكم بالوصول القائم على السمات (ABAC) لأنظمة الذكاء الاصطناعي.
  ​
* سلسلة التفكير: تقنية لتحسين الاستدلال في نماذج اللغة من خلال توليد خطوات استدلالية وسيطة قبل إنتاج الإجابة النهائية.
  ​
* قواطع الدوائر: آليات تقوم بإيقاف عمليات نظام الذكاء الاصطناعي تلقائيًا عند تجاوز حدود معينة للمخاطر.
  ​
* تسرب البيانات: التعرض غير المقصود للمعلومات الحساسة من خلال مخرجات أو سلوك نموذج الذكاء الاصطناعي.
  ​
* تسميم البيانات: الفساد المتعمد لبيانات التدريب لتقويض سلامة النموذج، غالبًا لتثبيت أبواب خلفية أو تقليل الأداء.
  ​
* الخصوصية التفاضلية – الخصوصية التفاضلية هي إطار صارم رياضيًا لإصدار معلومات إحصائية حول مجموعات البيانات مع حماية خصوصية الأفراد. تمكن حامل البيانات من مشاركة الأنماط الإجمالية للمجموعة مع الحد من تسرب المعلومات المتعلقة بأفراد معينين.
  ​
* التضمينات: تمثيلات متجهة كثيفة للبيانات (النصوص، الصور، إلخ) تلتقط المعنى الدلالي في فضاء عالي الأبعاد.
  ​
* القابلية للتفسير – القابلية للتفسير في الذكاء الاصطناعي هي قدرة نظام الذكاء الاصطناعي على تقديم أسباب مفهومة للبشر لقراراته وتنبؤاته، مما يوفر رؤى حول آلياته الداخلية.
  ​
* الذكاء الاصطناعي القابل للتفسير (XAI): أنظمة الذكاء الاصطناعي المصممة لتقديم تفسيرات يمكن للبشر فهمها لقراراتها وسلوكياتها من خلال تقنيات وأُطُر مختلفة.
  ​
* التعلم الموزع: هو نهج في التعلم الآلي حيث يتم تدريب النماذج عبر عدة أجهزة لا مركزية تحتفظ بعينات بيانات محلية، دون تبادل البيانات نفسها.
  ​
* خطوط الحماية: قيود تُطبق لمنع أنظمة الذكاء الاصطناعي من إنتاج مخرجات ضارة أو متحيزة أو غير مرغوب فيها بأي شكل آخر.
  ​
* الهلاوس – تشير هلاوس الذكاء الاصطناعي إلى ظاهرة يقوم فيها نموذج الذكاء الاصطناعي بتوليد معلومات غير صحيحة أو مضللة لا تستند إلى بيانات التدريب الخاصة به أو الواقع الفعلي.
  ​
* الإنسان في الحلقة (HITL): أنظمة مصممة لتتطلب إشراف الإنسان أو التحقق منه أو تدخله في نقاط اتخاذ القرار الحاسمة.
  ​
* البنية التحتية كرمز (IaC): إدارة وتوفير البنية التحتية من خلال الكود بدلاً من العمليات اليدوية، مما يتيح فحص الأمان ونشرًا متسقًا.
  ​
* الهروب من القيود: تقنيات تُستخدم لتجاوز الحواجز الأمنية في أنظمة الذكاء الاصطناعي، خصوصاً في نماذج اللغة الكبيرة، لإنتاج محتوى محظور.
  ​
* الحد الأدنى من الامتيازات: مبدأ الأمان الذي ينص على منح حد الوصول الأدنى الضروري فقط للمستخدمين والعمليات.
  ​
* LIME (التفسيرات القابلة للتفسير محليًا والخالية من الاعتماد على النموذج): تقنية لشرح توقعات أي مصنف تعلم آلي من خلال تقريبها محليًا باستخدام نموذج يمكن تفسيره.
  ​
* هجوم استنتاج العضوية: هو هجوم يهدف إلى تحديد ما إذا تم استخدام نقطة بيانات معينة في تدريب نموذج تعلُّم الآلة.
  ​
* MITRE ATLAS: مشهد التهديدات العدائية لأنظمة الذكاء الاصطناعي؛ قاعدة معرفة بالتكتيكات والتقنيات العدائية ضد أنظمة الذكاء الاصطناعي.
  ​
* بطاقة النموذج – بطاقة النموذج هي وثيقة توفر معلومات موحدة حول أداء نموذج الذكاء الاصطناعي، وقيوده، والاستخدامات المقصودة، والاعتبارات الأخلاقية لتعزيز الشفافية والتنمية المسؤولة للذكاء الاصطناعي.
  ​
* استخلاص النموذج: هجوم يقوم فيه الخصم باستمرار باستجواب نموذج مستهدف لإنشاء نسخة مشابهة وظيفيًا دون إذن.
  ​
* انعكاس النموذج: هجوم يحاول إعادة بناء بيانات التدريب من خلال تحليل مخرجات النموذج.
  ​
* إدارة دورة حياة النموذج – إدارة دورة حياة نموذج الذكاء الاصطناعي هي عملية الإشراف على جميع مراحل وجود نموذج الذكاء الاصطناعي، بما في ذلك تصميمه، تطويره، نشره، مراقبته، صيانته، والتقاعد النهائي له، لضمان استمراره في الفعالية والتوافق مع الأهداف.
  ​
* تسميم النموذج: إدخال ثغرات أو أبواب خلفية مباشرة في النموذج أثناء عملية التدريب.
  ​
* سرقة/استنساخ النموذج: استخراج نسخة أو تقريبي من نموذج ملكية من خلال الاستفسارات المتكررة.
  ​
* نظام متعدد الوكلاء: نظام يتكون من عدة وكلاء ذكاء اصطناعي متفاعلين، كل منهم يمتلك قدرات وأهداف قد تكون مختلفة.
  ​
* OPA (وكيل السياسة المفتوحة): هو محرك سياسات مفتوح المصدر يتيح تنفيذ السياسات الموحدة عبر النظام.
  ​
* تعلم الآلة مع الحفاظ على الخصوصية (PPML): تقنيات وأساليب لتدريب ونشر نماذج تعلم الآلة مع حماية خصوصية بيانات التدريب.
  ​
* حقن الإرشادات: هجوم يتم فيه تضمين تعليمات خبيثة في المدخلات لتجاوز سلوك النموذج المقصود.
  ​
* RAG (التوليد المعزز بالاسترجاع): تقنية تعزز نماذج اللغة الكبيرة من خلال استرجاع المعلومات ذات الصلة من مصادر المعرفة الخارجية قبل توليد الاستجابة.
  ​
* الهجوم الأحمر: ممارسة اختبار أنظمة الذكاء الاصطناعي بنشاط من خلال محاكاة الهجمات العدائية لتحديد نقاط الضعف.
  ​
* SBOM (قائمة مكونات البرمجيات): سجل رسمي يحتوي على تفاصيل وعلاقات سلسلة التوريد لمكونات مختلفة تُستخدم في بناء البرمجيات أو نماذج الذكاء الاصطناعي.
  ​
* SHAP (تفسيرات شابلي الإضافية): نهج يعتمد على نظرية الألعاب لشرح مخرجات أي نموذج تعلم آلي من خلال حساب مساهمة كل ميزة في التنبؤ.
  ​
* هجوم سلسلة التوريد: اختراق نظام عن طريق استهداف العناصر الأقل أمانًا في سلسلة التوريد الخاصة به، مثل المكتبات الخارجية، مجموعات البيانات، أو النماذج المدربة مسبقًا.
  ​
* التعلم بالنقل: تقنية يُعاد فيها استخدام نموذج تم تطويره لمهمة واحدة كنقطة انطلاق لنموذج في مهمة ثانية.
  ​
* قاعدة بيانات المتجهات: قاعدة بيانات متخصصة مصممة لتخزين المتجهات عالية الأبعاد (التضمينات) وأداء عمليات بحث تشابه فعالة.
  ​
* فحص الثغرات الأمنية: أدوات آلية تحدد الثغرات الأمنية المعروفة في مكونات البرمجيات، بما في ذلك أُطُر الذكاء الاصطناعي والاعتمادات التابعة لها.
  ​
* وضع العلامات المائية: تقنيات لدمج علامات غير مرئية في المحتوى المُنشأ بواسطة الذكاء الاصطناعي لتتبع أصله أو الكشف عن التوليد بواسطة الذكاء الاصطناعي.
  ​
* ثغرة اليوم الصفري: هي ثغرة غير معروفة سابقًا يمكن للمهاجمين استغلالها قبل أن يقوم المطورون بإنشاء ونشر تصحيح.

