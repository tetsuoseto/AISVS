# الملحق أ: المعجم

هذا المسرد الشامل يقدم تعريفات للمصطلحات الرئيسية في الذكاء الاصطناعي وتعلم الآلة والأمان المستخدمة في جميع أنحاء AISVS لضمان الوضوح والفهم المشترك.

* مثال عدائي: إدخال مصمم عمدًا للتسبب في خطأ نموذج الذكاء الاصطناعي، غالبًا عن طريق إضافة تغييرات طفيفة لا يلاحظها البشر.
  ​
* الصلابة ضد الهجمات العدائية – تشير الصلابة ضد الهجمات العدائية في الذكاء الاصطناعي إلى قدرة النموذج على الحفاظ على أدائه ومقاومة الخداع أو التلاعب من خلال مدخلات خبيثة ومصممة عمدًا بهدف التسبب في أخطاء.
  ​
* الوكيل – الوكلاء الذكيون هم أنظمة برمجية تستخدم الذكاء الاصطناعي لتحقيق الأهداف وإكمال المهام نيابة عن المستخدمين. يظهرون مهارات في الاستدلال والتخطيط والذاكرة، ولديهم مستوى من الاستقلالية لاتخاذ القرارات والتعلم والتكيف.
  ​
* الذكاء الاصطناعي الوكِلي: أنظمة الذكاء الاصطناعي التي يمكنها العمل بدرجة معينة من الاستقلالية لتحقيق الأهداف، وغالبًا ما تتخذ قرارات وتتخذ إجراءات بدون تدخل بشري مباشر.
  ​
* التحكم في الوصول المعتمد على السمات (ABAC): نموذج تحكم في الوصول حيث تُتخذ قرارات التفويض بناءً على سمات المستخدم، المورد، الإجراء، والبيئة، ويتم تقييمها في وقت الاستعلام.
  ​
* هجوم الباب الخلفي: نوع من هجمات تسميم البيانات حيث يتم تدريب النموذج على الاستجابة بطريقة معينة لمحفزات محددة مع التصرف بشكل طبيعي في الحالات الأخرى.
  ​
* التحيّز: أخطاء منهجية في مخرجات نموذج الذكاء الاصطناعي يمكن أن تؤدي إلى نتائج غير عادلة أو تمييزية تجاه مجموعات معينة أو في سياقات محددة.
  ​
* استغلال التحيز: تقنية هجوم تستغل التحيزات المعروفة في نماذج الذكاء الاصطناعي للتلاعب في المخرجات أو النتائج.
  ​
* سيدار: لغة السياسة ومحرك أمازون للأذونات الدقيقة المستخدمة في تنفيذ التحكم في الوصول المعتمد على السمات (ABAC) لأنظمة الذكاء الاصطناعي.
  ​
* سلسلة التفكير: تقنية لتحسين عملية الاستدلال في نماذج اللغة من خلال توليد خطوات استدلال وسيطة قبل إنتاج الإجابة النهائية.
  ​
* قواطع الدائرة: آليات توقف عمليات نظام الذكاء الاصطناعي تلقائيًا عند تجاوز حدود مخاطر معينة.
  ​
* تسريب البيانات: الكشف غير المقصود عن معلومات حساسة من خلال مخرجات أو سلوك نموذج الذكاء الاصطناعي.
  ​
* تسميم البيانات: الفساد المتعمد لبيانات التدريب للإضرار بسلامة النموذج، غالبًا لزرع أبواب خلفية أو تقليل الأداء.
  ​
* الخصوصية التفاضلية – الخصوصية التفاضلية هي إطار رياضي دقيق لإصدار معلومات إحصائية حول مجموعات البيانات مع حماية خصوصية الأفراد المشاركين في البيانات. تتيح لحامل البيانات مشاركة الأنماط المجمعّة للمجموعة مع الحد من المعلومات التي يمكن تسريبها عن أفراد معينين.
  ​
* التضمينات: تمثيلات متجهية كثيفة للبيانات (النصوص، الصور، إلخ) تلتقط المعنى الدلالي في فضاء عالي الأبعاد.
  ​
* القابلية للتفسير – القابلية للتفسير في الذكاء الاصطناعي هي قدرة نظام الذكاء الاصطناعي على تقديم أسباب يمكن للبشر فهمها لقراراته وتنبؤاته، مما يوفر رؤى حول كيفية عمله الداخلي.
  ​
* الذكاء الاصطناعي القابل للتفسير (XAI): أنظمة الذكاء الاصطناعي المصممة لتقديم تفسيرات يمكن للبشر فهمها لقراراتها وسلوكياتها من خلال تقنيات وأطر عمل مختلفة.
  ​
* التعلم الموزع: هو نهج في تعلم الآلة يتم فيه تدريب النماذج عبر عدة أجهزة لا مركزية تحتفظ بعينات بيانات محلية، دون تبادل البيانات نفسها.
  ​
* الضوابط: قيود تُنفذ لمنع أنظمة الذكاء الاصطناعي من إنتاج مخرجات ضارة أو منحازة أو غير مرغوب فيها بأي شكل من الأشكال.
  ​
* الهلاوس – تشير هلاوس الذكاء الاصطناعي إلى ظاهرة يقوم فيها نموذج الذكاء الاصطناعي بتوليد معلومات خاطئة أو مضللة لا تستند إلى بيانات تدريبه أو الواقع الفعلي.
  ​
* الإنسان في الحلقة (HITL): أنظمة مصممة لتتطلب إشرافًا بشريًا، أو تحققًا، أو تدخلًا في نقاط اتخاذ القرار الحرجة.
  ​
* البنية التحتية ككود (IaC): إدارة وتوفير البنية التحتية من خلال الكود بدلاً من العمليات اليدوية، مما يتيح فحص الأمان والنشر المتسق.
  ​
* الاختراق: تقنيات تُستخدم لتجاوز ضوابط الأمان في أنظمة الذكاء الاصطناعي، وخاصة في نماذج اللغة الكبيرة، لإنتاج محتوى محظور.
  ​
* أقل الامتيازات: مبدأ الأمان الذي يمنح فقط حقوق الوصول اللازمة الأدنى للمستخدمين والعمليات.
  ​
* LIME (تفسيرات محلية مستقلة عن النموذج): تقنية لشرح تنبؤات أي مصنف تعلم آلي عن طريق تقريبها محليًا بنموذج قابل للتفسير.
  ​
* هجوم استنتاج العضوية: هو هجوم يهدف إلى تحديد ما إذا كانت نقطة بيانات محددة قد استخدمت في تدريب نموذج تعلم الآلة.
  ​
* MITRE ATLAS: مشهد التهديدات العدائية لأنظمة الذكاء الاصطناعي؛ قاعدة معرفية للتكتيكات والتقنيات العدائية ضد أنظمة الذكاء الاصطناعي.
  ​
* بطاقة النموذج – بطاقة النموذج هي وثيقة توفر معلومات موحدة حول أداء نموذج الذكاء الاصطناعي، وقيوده، والاستخدامات المقصودة، والاعتبارات الأخلاقية لتعزيز الشفافية وتطوير الذكاء الاصطناعي بمسؤولية.
  ​
* استخراج النموذج: هجوم يقوم فيه الخصم باستجواب نموذج مستهدف بشكل متكرر لإنشاء نسخة مشابهة وظيفياً بدون تفويض.
  ​
* عكس النموذج: هجوم يحاول إعادة بناء بيانات التدريب من خلال تحليل مخرجات النموذج.
  ​
* إدارة دورة حياة النموذج – إدارة دورة حياة نموذج الذكاء الاصطناعي هي عملية الإشراف على جميع مراحل وجود نموذج الذكاء الاصطناعي، بما في ذلك تصميمه وتطويره ونشره ورصده وصيانته وتقاعده في النهاية، لضمان بقائه فعالًا ومتوافقًا مع الأهداف.
  ​
* تسمم النموذج: إدخال ثغرات أمنية أو أبواب خلفية مباشرة في النموذج خلال عملية التدريب.
  ​
* سرقة/سرقة النموذج: استخراج نسخة أو تقريبيّة لنموذج مملوك من خلال استعلامات متكررة.
  ​
* نظام متعدد الوكلاء: نظام مكون من عدة وكلاء ذكاء اصطناعي متفاعلين، كل منهم قد يمتلك قدرات وأهداف مختلفة.
  ​
* OPA (وكيل السياسات المفتوح): محرك سياسات مفتوح المصدر يتيح تطبيق السياسات بشكل موحد عبر كامل النظام.
  ​
* التعلم الآلي مع الحفاظ على الخصوصية (PPML): تقنيات وأساليب لتدريب ونشر نماذج التعلم الآلي مع حماية خصوصية بيانات التدريب.
  ​
* حقن المطالبات: هجوم يتم فيه تضمين تعليمات خبيثة في المدخلات لتجاوز السلوك المقصود للنموذج.
  ​
* RAG (التوليد المدعوم بالاستخراج): تقنية تُحسّن نماذج اللغة الكبيرة عبر استرجاع المعلومات ذات الصلة من مصادر المعرفة الخارجية قبل توليد الرد.
  ​
* الهجوم الأحمر: وهو ممارسة اختبار أنظمة الذكاء الاصطناعي بنشاط من خلال محاكاة هجمات معادية لاكتشاف الثغرات.
  ​
* قائمة مكونات البرمجيات (SBOM): سجل رسمي يحتوي على تفاصيل وعلاقات سلسلة التوريد لمكونات مختلفة تُستخدم في بناء البرمجيات أو نماذج الذكاء الاصطناعي.
  ​
* SHAP (تفسيرات شابلي الإضافية): نهج نظرية الألعاب لشرح مخرجات أي نموذج تعلّم آلي عن طريق حساب مساهمة كل ميزة في التنبؤ.
  ​
* هجوم سلسلة التوريد: اختراق نظام عن طريق استهداف العناصر الأقل أمانًا في سلسلة التوريد الخاصة به، مثل المكتبات التابعة لأطراف ثالثة، مجموعات البيانات، أو النماذج المدربة مسبقًا.
  ​
* التعلم النقال: تقنية يتم فيها إعادة استخدام نموذج تم تطويره لمهمة معينة كنقطة انطلاق لنموذج في مهمة ثانية.
  ​
* قاعدة بيانات المتجهات: قاعدة بيانات متخصصة مصممة لتخزين المتجهات عالية الأبعاد (التضمينات) وتنفيذ عمليات بحث تشابه فعالة.
  ​
* فحص الثغرات الأمنية: أدوات آلية تقوم بتحديد الثغرات الأمنية المعروفة في مكونات البرمجيات، بما في ذلك أطر عمل الذكاء الاصطناعي والتبعيات.
  ​
* العلامات المائية: تقنيات تضمين علامات غير ملحوظة في المحتوى المولد بواسطة الذكاء الاصطناعي لتعقب أصله أو للكشف عن توليده بواسطة الذكاء الاصطناعي.
  ​
* ثغرة اليوم الصفري: ثغرة غير معروفة سابقًا يمكن للمهاجمين استغلالها قبل أن يقوم المطورون بإنشاء ونشر تصحيح.

