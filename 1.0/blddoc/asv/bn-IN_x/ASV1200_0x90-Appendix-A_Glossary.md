# পরিশিষ্ট ক: শব্দকোষ

>এই বিস্তৃত শব্দকোষটি AISVS জুড়ে ব্যবহৃত মূল AI, ML, এবং সিকিউরিটি শর্তাবলীর সংজ্ঞা প্রদান করে স্পষ্টতা এবং সাধারণ বোঝাপড়া নিশ্চিত করার জন্য।

* প্রতিযোগিতামূলক উদাহরণ: এমন একটি ইনপুট যা ইচ্ছাকৃতভাবে তৈরি করা হয়েছে একটি AI মডেলকে ভুল করাতে, প্রায়শই মানুষের জন্য অস্বচ্ছ সূক্ষ্ম পরিবর্তনগুলি যোগ করে।
  ​
* বিরোধী স্থিতিশীলতা – AI-তে বিরোধী স্থিতিশীলতা বলতে একটি মডেলের সক্ষমতা বোঝায় যেটি তার কার্যকারিতা বজায় রাখতে পারে এবং ইচ্ছাকৃতভাবে তৈরি করা ক্ষতিকর ইনপুট দ্বারা ভুলে যাওয়া বা প্রভাবিত হওয়া প্রতিরোধ করতে পারে, যা ত্রুটি সৃষ্টি করার উদ্দেশ্যে তৈরি।
  ​
* এজেন্ট – AI এজেন্ট হল সফটওয়্যার সিস্টেম যা ব্যবহারকারীদের পক্ষে লক্ষ্য অর্জন এবং কাজ সম্পন্ন করতে AI ব্যবহার করে। তারা যুক্তি, পরিকল্পনা, এবং স্মৃতি প্রদর্শন করে এবং সিদ্ধান্ত গ্রহণ, শেখার, এবং অভিযোজন করার জন্য একটি নির্দিষ্ট মাত্রার স্বায়ত্তশাসন রাখে।
  ​
* এজেন্টিক AI: এমন AI সিস্টেম যা কিছু পরিমাণ স্বায়ত্তশাসন নিয়ে কাজ করতে পারে লক্ষ্য অর্জনের জন্য, প্রায়শই সরাসরি মানুষের হস্তক্ষেপ ছাড়াই সিদ্ধান্ত গ্রহণ এবং কর্ম গ্রহণ করে।
  ​
* অ্যাট্রিবিউট-ভিত্তিক অ্যাক্সেস কন্ট্রোল (ABAC): একটি অ্যাক্সেস কন্ট্রোল প্যারাডাইম যেখানে অনুমোদনের সিদ্ধান্তগুলি ব্যবহারকারী, সম্পদ, ক্রিয়া, এবং পরিবেশের অ্যাট্রিবিউটের ভিত্তিতে নেওয়া হয়, যা কুয়েরি সময়ে মূল্যায়ন করা হয়।
  ​
* ব্যাকডোর আক্রমণ: ডেটা পয়জনিং আক্রমণের একটি ধরন যেখানে মডেলকে নির্দিষ্ট ট্রিগারগুলির প্রতি নির্দিষ্টভাবে সাড়া দিতে প্রশিক্ষণ দেওয়া হয়, আর বাকি সময় স্বাভাবিকভাবে আচরণ করে।
  ​
* প্রতিকৃতি: AI মডেল আউটপুটে সিস্টেম্যাটিক ত্রুটিসমূহ যা নির্দিষ্ট গোষ্ঠী বা নির্দিষ্ট প্রেক্ষাপটে অসাম্য বা বৈষম্যমূলক ফলাফল ডেকে আনতে পারে।
  ​
* বায়াস শোষণ: একটি আক্রমণ কৌশল যা AI মডেলগুলিতে পরিচিত বায়াসগুলি ব্যবহার করে আউটপুট বা ফলাফল নিয়ন্ত্রিত করে।
  ​
* Cedar: অ্যামাজনের নীতি ভাষা এবং ইঞ্জিন যা AI সিস্টেমগুলির জন্য ABAC বাস্তবায়নে ব্যবহৃত সূক্ষ্ম-গ্রেনুলার অনুমতির জন্য ব্যবহৃত হয়।
  ​
* চেইন অফ থট: একটি কৌশল যা ভাষা মডেলগুলিতে যুক্তি উন্নত করার জন্য ব্যবহার করা হয়, যেখানে চূড়ান্ত উত্তর প্রদানের আগে মধ্যবর্তী যুক্তির ধাপ তৈরি করা হয়।
  ​
* সার্কিট ব্রেকার্স: এমন যন্ত্রপাতি যা স্বয়ংক্রিয়ভাবে AI সিস্টেমের কার্যক্রম বন্ধ করে দেয় যখন নির্দিষ্ট ঝুঁকি সীমা অতিক্রম করা হয়।
  ​
* ডেটা লিকেজ: এআই মডেল আউটপুট বা আচরণের মাধ্যমে সংবেদনশীল তথ্যের অপ্রত্যাশিত এক্সপোজার।
  ​
* ডাটা পয়জনিং: মডেলের অখণ্ডতা ক্ষুণ্ন করার জন্য প্রশিক্ষণ ডেটাকে উদ্দেশ্যমূলকভাবে ক্ষতিগ্রস্ত করা, প্রায়ই ব্যাকডোর ইনস্টল করার বা পারফরম্যান্স হ্রাস করার জন্য।
  ​
* ডিফারেনশিয়াল প্রাইভেসি – ডিফারেনশিয়াল প্রাইভেসি হল একটি গাণিতিকভাবে কঠোর কাঠামো যা ডেটাসেট সম্পর্কে পরিসংখ্যানগত তথ্য প্রকাশের সময় ব্যক্তিগত তথ্যবর্গের গোপনীয়তা রক্ষা করে। এটি ডেটা ধারককে একটি গোষ্ঠীর সামগ্রিক ধাচ শেয়ার করতে সক্ষম করে এবং নির্দিষ্ট ব্যক্তিদের সম্পর্কে ফাঁস হওয়া তথ্য সীমিত করে।
  ​
* এম্বেডিংস: ঘন ভেক্টর উপস্থাপনাগুলি যা ডেটা (টেক্সট, ছবি, ইত্যাদি) এর অর্থবোধক অর্থকে উচ্চ-মাত্রার স্থানে ধারণ করে।
  ​
* ব্যাখ্যাযোগ্যতা – AI-তে ব্যাখ্যাযোগ্যতা হলো একটি AI সিস্টেমের ক্ষমতা যাতে এটি তার সিদ্ধান্ত এবং পূর্বাভাসের জন্য মানব-বোধগম্য কারণ প্রদান করতে পারে, এবং তার অভ্যন্তরীণ কার্যপ্রণালীর বিষয়ে অন্তর্দৃষ্টি প্রদান করে।
  ​
* ব্যাখ্যাযোগ্য এআই (XAI): এআই সিস্টেমসমূহ যা তাদের সিদ্ধান্ত এবং আচরণের জন্য মানব-অনুসন্ধানযোগ্য ব্যাখ্যা প্রদান করার জন্য বিভিন্ন কৌশল ও কাঠামো ব্যবহার করে ডিজাইন করা হয়।
  ​
* ফেডারেটেড লার্নিং: একটি মেশিন লার্নিং পদ্ধতি যেখানে মডেলগুলি বহু বিকেন্দ্রীভূত ডিভাইসে প্রশিক্ষিত হয়, যারা স্থানীয় ডেটা নমুনাগুলো ধারণ করে, ডেটা নিজেই বিনিময় না করে।
  ​
* গার্ডরেইলস: এমন সীমাবদ্ধতা যা AI সিস্টেমকে ক্ষতিকর, পক্ষপাতমূলক বা অন্যথায় অপছন্দনীয় আউটপুট তৈরি করা থেকে বিরত রাখতে প্রয়োগ করা হয়।
  ​
* হ্যালুসিনেশন – একটি AI হ্যালুসিনেশন বলতে বোঝায় এমন একটি ঘটনা যেখানে একটি AI মডেল তার প্রশিক্ষণ ডেটা বা বাস্তব তথ্যের উপর ভিত্তি না করে ভুল বা বিভ্রান্তিকর তথ্য তৈরি করে।
  ​
* মানব-ইন-দ্য-লুপ (HITL): এমন সিস্টেম যা গুরুত্বপূর্ণ সিদ্ধান্ত নেওয়ার পর্যায়ে মানুষের নজরদারি, যাচাই-বাছাই বা হস্তক্ষেপের প্রয়োজনীয়তার জন্য ডিজাইন করা হয়েছে।
  ​
* ইনফ্রাস্ট্রাকচার অ্যাজ কোড (IaC): ম্যানুয়াল প্রক্রিয়ার পরিবর্তে কোডের মাধ্যমে ইনফ্রাস্ট্রাকচার ম্যানেজ এবং প্রোভিশন করা, যা সিকিউরিটি স্ক্যানিং এবং নিয়মিত ডিপ্লয়মেন্ট সক্ষম করে।
  ​
* জেলব্রেক: নিরাপত্তা গার্ডরেলগুলি উৎকোচ করার জন্য ব্যবহৃত কৌশল, বিশেষত বড় ভাষা মডেলগুলিতে, নিষিদ্ধ সামগ্রী উৎপাদনের জন্য।
  ​
* ন্যূনতম বিশেষাধিকার: ব্যবহারকারী এবং প্রক্রিয়াগুলির জন্য শুধুমাত্র সর্বনিম্ন প্রয়োজনীয় প্রবেশাধিকার সুবিধা প্রদান করার নিরাপত্তা নীতি।
  ​
* LIME (লোকাল ইন্টারপ্রেটেবল মডেল-এগনোস্টিক এক্সপ্লানেশন): যেকোনো মেশিন লার্নিং ক্লাসিফায়ারের পূর্বাভাসকে স্থানীয়ভাবে একটি বোধগম্য মডেল দিয়ে আনুমানিকভাবে ব্যাখ্যা করার একটি প্রযুক্তি।
  ​
* মেম্বারশিপ ইনফারেন্স আক্রমণ: একটি আক্রমণ যা নির্ধারণ করার চেষ্টা করে যে একটি নির্দিষ্ট ডেটা পয়েন্ট মেশিন লার্নিং মডেল প্রশিক্ষণের জন্য ব্যবহৃত হয়েছে কিনা।
  ​
* MITRE ATLAS: কৃত্রিম বুদ্ধিমত্তা সিস্টেমের বিরুদ্ধে প্রতিপক্ষী হুমকি পরিপ্রেক্ষিত; AI সিস্টেমের বিরুদ্ধে প্রতিপক্ষী কৌশল এবং প্রক্রিয়াগুলোর একটি জ্ঞানভিত্তিক ডাটাবেজ।
  ​
* মডেল কার্ড – একটি মডেল কার্ড হলো একটি দলিল যা একটি AI মডেলের কার্যক্ষমতা, সীমাবদ্ধতা, নির্ধারিত ব্যবহারের ক্ষেত্র এবং নৈতিক বিবেচনাগুলোর বিষয়ে মানসম্মত তথ্য প্রদান করে, যা স্বচ্ছতা এবং দায়িত্বশীল AI উন্নয়নকে উৎসাহিত করে।
  ​
* মডেল এক্সট্র্যাকশন: একটি আক্রমণ যেখানে একজন প্রতিপক্ষ বারংবার একটি লক্ষ্য মডেলকে প্রশ্ন করে ফাংশনালি অনুরূপ একটি প্রতিলিপি তৈরি করে অনুমোদন ছাড়া।
  ​
* মডেল ইনভার্শন: একটি আক্রমণ যা মডেলের আউটপুট বিশ্লেষণ করে প্রশিক্ষণ ডেটা পুনর্গঠন করার চেষ্টা করে।
  ​
* মডেল লাইফসাইকেল ম্যানেজমেন্ট – AI মডেল লাইফসাইকেল ম্যানেজমেন্ট হল একটি AI মডেলের সমস্ত পর্যায়ের তদারকি করার প্রক্রিয়া, যার মধ্যে রয়েছে এর নকশা, উন্নয়ন, ব্যবস্থাপনা, পর্যবেক্ষণ, রক্ষণাবেক্ষণ এবং শেষ পর্যন্ত অবসর গ্রহণ, যাতে এটি কার্যকর এবং লক্ষ্যগুলোর সাথে সামঞ্জস্যপূর্ণ থাকে।
  ​
* মডেল পয়জনিং: প্রশিক্ষণ প্রক্রিয়ার সময় সরাসরি একটি মডেলে দুর্বলতা বা ব্যাকডোর প্রবর্তন।
  ​
* মডেল চুরি/উদ্ধৃতি: পুনরাবৃত্তি প্রশ্নের মাধ্যমে একটি মালিকানাধীন মডেলের একটি কপি বা আনুমানিক রূপ প্রাপ্ত করা।
  ​
* মাল্টি-এজেন্ট সিস্টেম: একটি সিস্টেম যা একাধিক ইন্টারঅ্যাকটিং AI এজেন্ট নিয়ে গঠিত, যাদের প্রত্যেকের ভিন্ন ক্ষমতা এবং লক্ষ্য থাকতে পারে।
  ​
* OPA (ওপেন পলিসি এজেন্ট): একটি ওপেন-সোর্স পলিসি ইঞ্জিন যা স্ট্যাক জুড়ে ঐক্যবদ্ধ পলিসি প্রয়োগ সক্ষম করে।
  ​
* প্রাইভেসি-প্রিজার্ভিং মেশিন লার্নিং (PPML): প্রশিক্ষণ ডেটার গোপনীয়তা সংরক্ষণ করে এমএল মডেলগুলি প্রশিক্ষণ এবং মোতায়েন করার কৌশল এবং পদ্ধতিগুলি।
  ​
* প্রম্পট ইনজেকশন: একটি আক্রমণ যেখানে দুষ্ট নির্দেশাবলী ইনপুটে প্রবেশ করানো হয় যাতে একটি মডেলের নির্ধারিত আচরণকে ওভাররাইড করা যায়।
  ​
* RAG (রিট্রিভ্যাল-অগমেন্টেড জেনারেশন): একটি কৌশল যা বড় ভাষার মডেলগুলিকে উন্নত করে সংশ্লিষ্ট তথ্য বাইরের জ্ঞান উৎস থেকে পুনরুদ্ধার করে প্রতিক্রিয়া তৈরি করার আগে।
  ​
* রেড-টিমিং: প্রতিশ্রুতিবদ্ধ হামলা অনুকরণ করে AI সিস্টেমগুলি সক্রিয়ভাবে পরীক্ষা করার প্রক্রিয়া যাতে দুর্বলতা চিহ্নিত করা যায়।
  ​
* SBOM (সফটওয়্যার বিল অফ ম্যাটেরিয়ালস): সফটওয়্যার বা AI মডেল নির্মাণে ব্যবহৃত বিভিন্ন উপাদানের বিবরণ এবং সরবরাহ চেইন সম্পর্কের একটি আনুষ্ঠানিক রেকর্ড।
  ​
* SHAP (SHapley Additive exPlanations): যেকোনো মেশিন লার্নিং মডেলের আউটপুট ব্যাখ্যা করতে একটি গেম থিওরেটিক পদ্ধতি, যা প্রতিটি ফিচারের প্রেডিকশনে অবদান গণনা করে।
  ​
* সরবরাহ শৃঙ্খল আক্রমণ: একটি সিস্টেমকে কম সুরক্ষিত উপাদানগুলি যেমন তৃতীয় পক্ষের লাইব্রেরি, ডেটাসেট বা প্রি-ট্রেইন্ড মডেলগুলিকে লক্ষ্য করে ক্ষতিগ্রস্ত করা।
  ​
* ট্রান্সফার লার্নিং: একটি এমন প্রযুক্তি যেখানে এক কাজের জন্য তৈরি মডেলটি দ্বিতীয় কাজের জন্য মডেলের শুরু বিন্দু হিসেবে পুনঃব্যবহৃত হয়।
  ​
* ভেক্টর ডাটাবেজ: একটি বিশেষায়িত ডাটাবেজ যা উচ্চ-মাত্রার ভেক্টর (এম্বেডিং) সংরক্ষণ এবং কার্যকরী সান্নিধ্য অনুসন্ধান করতে ডিজাইন করা হয়েছে।
  ​
* কমানোরতা স্ক্যানিং: স্বয়ংক্রিয় সরঞ্জাম যা সফটওয়্যার উপাদানসমূহে যেমন AI ফ্রেমওয়ার্ক এবং নির্ভরশীলতাগুলিতে পরিচিত নিরাপত্তা দুর্বলতাগুলি সনাক্ত করে।
  ​
* ওয়াটারমার্কিং: AI-উৎপন্ন সামগ্রীর মধ্যে অস্পষ্ট চিহ্ন প্রবেশ করানোর কৌশলসমূহ যা এর উৎস ট্র্যাক করতে বা AI-উৎপাদন সনাক্ত করতে ব্যবহৃত হয়।
  ​
* জিরো-ডে দুর্বলতা: একটি পূর্বে অজানা দুর্বলতা যা আক্রমণকারীরা ডেভেলপাররা একটি প্যাচ তৈরি ও প্রয়োগ করার আগে ব্যবহার করতে পারে।

