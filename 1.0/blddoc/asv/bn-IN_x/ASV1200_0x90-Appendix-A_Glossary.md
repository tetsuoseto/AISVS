# পরিশিষ্ট A: শব্দকোষ

>This বিশদ শব্দকোষটি AISVS-এ ব্যবহৃত মূল AI, ML, এবং নিরাপত্তা সম্পর্কিত শব্দসমূহের সংজ্ঞা প্রদান করে স্পষ্টতা ও সাধারণ বোঝাপড়া নিশ্চিত করতে।

* বিপক্ষীয় উদাহরণ: একটি ইনপুট উদ্দেশ্যমূলকভাবে প্রস্তুত করা হয় যাতে একটি এআই মডেল ভুল করতে বাধ্য হয়, সাধারণত মানুষের কাছে অনুপস্থিত সূক্ষ্ম পরিবর্তন যোগ করার মাধ্যমে।
  ​
* Adversarial Robustness – এআই-তে অ্যাডভার্সারিয়াল রোবস্টনেস বলতে একটি মডেলের সক্ষমতা বোঝায় যাতে সেটি তার পারফরম্যান্স বজায় রাখতে পারে এবং ইচ্ছা করে তৈরি করা দুষ্ট ইনপুট দ্বারা ধোঁকা খাওয়া বা প্রভাবিত হওয়া থেকে রক্ষা পায়, যেগুলো ভুল সৃষ্টির জন্য ডিজাইন করা হয়েছে।
  ​
* এজেন্ট – এআই এজেন্টগুলো হলো সফটওয়্যার সিস্টেম যা লক্ষ্য পূরণ ও ব্যবহারকারীদের পক্ষ থেকে কাজ সম্পন্ন করতে AI ব্যবহার করে। তারা যুক্তি, পরিকল্পনা ও স্মৃতি প্রদর্শন করে এবং সিদ্ধান্ত নেওয়া, শেখা ও খাপ খাইয়ে নেওয়ার জন্য একটি স্বায়ত্তশাসনের মাত্রা রাখে।
  ​
* এজেন্টিক এআই: লক্ষ্য অর্জনের জন্য কিছু মাত্রা স্বয়ংক্রিয়ভাবে কাজ করতে পারে এমন এআই সিস্টেমগুলো, যা প্রায়ই সিদ্ধান্ত নেয় এবং মানব সরাসরি হস্তক্ষেপ ছাড়া পদক্ষেপ গ্রহণ করে।
  ​
* গুণভিত্তিক অ্যাক্সেস কন্ট্রোল (ABAC): একটি অ্যাক্সেস কন্ট্রোল প্যারাডাইম যেখানে অনুমোদন সিদ্ধান্তগুলো ব্যবহারকারী, সম্পদ, ক্রিয়া এবং পরিবেশের বৈশিষ্ট্যগুলির ওপর নির্ভর করে, প্রশ্ন-সময় মূল্যায়িত হয়।
  ​
* ব্যাকডোর আক্রমণ: এক ধরনের ডেটা বিষাক্তকরণ আক্রমণ যেখানে মডেলটিকে নির্দিষ্ট ট্রিগারের ক্ষেত্রে নির্দিষ্টভাবে প্রতিক্রিয়া দেখাতে শেখানো হয়, অন্যথায় স্বাভাবিক আচরণ করে।
  ​
* বায়াস: কৃত্রিম বুদ্ধিমত্তা (AI) মডেলের আউটপুটে সিস্টেম্যাটিক ভুল যা নির্দিষ্ট গোষ্ঠীগুলোর জন্য বা নির্দিষ্ট প্রেক্ষাপটে অবিচারমূলক বা বৈষম্যমূলক ফলাফল সৃষ্টি করতে পারে।
  ​
* পক্ষপাত্য শোষণ: এআই মডেলগুলির পরিচিত পক্ষপাতের সুবিধা নিয়ে আউটপুট বা ফলাফলকে প্রভাবিত করার একটি আক্রমণ কৌশল।
  ​
* Cedar: এআই সিস্টেমগুলোর ABAC বাস্তবায়নের জন্য ব্যবহৃত অ্যামাজনের নীতি-ভাষা ও ইঞ্জিন।
  ​
* চেইন অব থট: ভাষা মডেলগুলোর যুক্তি-চিন্তা উন্নত করার একটি কৌশল, যা চূড়ান্ত উত্তর দেওয়ার আগে মধ্যবর্তী চিন্তার ধাপ তৈরি করে।
  ​
* সার্কিট ব্রেকার: নির্দিষ্ট ঝুঁকি সীমা অতিক্রম করলে এআই সিস্টেমের কার্যক্রম স্বয়ংক্রিয়ভাবে বন্ধ করে দেয় এমন ব্যবস্থা।
  ​
* তথ্য ফাঁস: AI মডেলের আউটপুট বা আচরণের মাধ্যমে সংবেদনশীল তথ্যের অনিচ্ছাকৃত প্রকাশ।
  ​
* ডেটা বিষক্রিয়া: মডেলের অখণ্ডতা নষ্ট করার জন্য প্রশিক্ষণ ডেটার ইচ্ছাকৃত দুর্নীতি, যা প্রায়ই ব্যাকডোর স্থাপন করতে বা পারফরম্যান্স হ্রাস করতে ব্যবহৃত হয়।
  ​
* ডিফারেনশিয়াল প্রাইভেসি – ডিফারেনশিয়াল প্রাইভেসিটি একটি গাণিতিকভাবে কঠোর কাঠামো যা ডেটা-সেটগুলোর সম্পর্কে পরিসংখ্যান তথ্য প্রকাশ করার সময় ব্যক্তিগত তথ্যের গোপনীয়তা রক্ষা করে। এটি ডেটা-হোল্ডারকে গ্রুপের সমষ্টিগত নিদর্শন শেয়ার করার সুযোগ দেয়, তবে নির্দিষ্ট ব্যক্তিদের সম্পর্কে তথ্য উন্মোচিত হতে পারে তা সীমিত রাখে।
  ​
* এম্বেডিংস: ডেটার ঘন ভেক্টর প্রতিনিধিত্ব (পাঠ্য, চিত্র ইত্যাদি) যা একটি উচ্চ-আয়তনের স্পেসে আর্থবোধক অর্থ ধারণ করে।
  ​
* ব্যাখ্যাযোগ্যতা – কৃত্রিম বুদ্ধিমত্তা (AI)-এ ব্যাখ্যাযোগ্যতা হলো একটি AI সিস্টেমের সেই ক্ষমতা যা সিদ্ধান্ত ও পূর্বাভাসের জন্য মানুষের বোঝার মতো কারণ সরবরাহ করতে পারে, এবং এর অভ্যন্তরীণ ক্রিয়াকলাপ সম্পর্কে অন্তর্দৃষ্টি প্রদান করে।
  ​
* ব্যাখ্যাযোগ্য এআই (XAI): নানা কৌশল ও কাঠামোর মাধ্যমে মানব-বোধগম্য ব্যাখ্যা দিতে তাদের সিদ্ধান্ত ও আচরণগুলোর জন্য ডিজাইন করা এআই সিস্টেম।
  ​
* ফেডারেটেড লার্নিং: একটি মেশিন লার্নিং পদ্ধতি যেখানে মডেলগুলি বহু বিকেন্দ্রীভূত ডিভাইস জুড়ে স্থানীয় ডেটা নমুনা ধারণ করে প্রশিক্ষিত হয়, ডেটা নিজে বিনিময় না করে।
  ​
* গার্ডরেলস: AI সিস্টেমগুলোকে ক্ষতিকর, পক্ষপাতপূর্ণ, বা অন্য কোনো অনুচিত ফলাফল তৈরি করতে বাধা দেওয়ার জন্য বাস্তবায়িত সীমাবদ্ধতা।
  ​
* হলুসিনেশন – একটি এআই মডেল ভুল বা বিভ্রান্তিকর তথ্য তৈরি করে যা তার প্রশিক্ষণ ডেটা বা বাস্তবতার ওপর ভিত্তি করে নয়।
  ​
* Human-in-the-Loop (HITL): গুরুত্বপূর্ণ সিদ্ধান্ত নেওয়ার সময়ে মানুষের নজরদারি, যাচাই, অথবা হস্তক্ষেপ প্রয়োজন হয়—এমন ডিজাইন করা সিস্টেমসমূহ।
  ​
* কোড হিসেবে অবকাঠামো (IaC): কোডের মাধ্যমে অবকাঠামো ব্যবস্থাপনা ও প্রোভিশনিং করা, ম্যানুয়াল প্রক্রিয়াগুলোর বদলে, সুরক্ষা স্ক্যানিং এবং ধারাবাহিক ডেপ্লয়মেন্ট সক্ষম করা।
  ​
* জেলব্রেক: AI সিস্টেমগুলিতে নিরাপত্তা গার্ডরেলগুলোকে এড়িয়ে যাওয়ার জন্য ব্যবহৃত কৌশল, বিশেষ করে বড় ভাষা মডেলগুলিতে, নিষিদ্ধ সামগ্রী তৈরি করতে।
  ​
* ন্যূনতম অনুমতি নীতি: ব্যবহারকারী ও প্রক্রিয়াগুলির জন্য কেবলমাত্র প্রয়োজনীয় সর্বনিম্ন প্রবেশাধিকার প্রদান করার নিরাপত্তা নীতি।
  ​
* LIME (স্থানীয় ব্যাখ্যাযোগ্য মডেল-নিরপেক্ষ ব্যাখ্যা): যে কোনো মেশিন লার্নিং ক্লাসিফায়ারের পূর্বাভাস ব্যাখ্যা করার জন্য একটি কৌশল যা এটিকে স্থানীয়ভাবে একটি ব্যাখ্যাযোগ্য মডেল দ্বারা আনুমানিক করে।
  ​
* Membership Inference Attack: একটি আক্রমণ যা নির্ধারণ করার লক্ষ্য রাখে যে একটি নির্দিষ্ট ডেটা পয়েন্ট কি মেশিন লার্নিং মডেল প্রশিক্ষিত করতে ব্যবহৃত হয়েছিল কিনা।
  ​
* MITRE ATLAS: আক্রমণাত্মক হুমকি-প্রেক্ষাপট কৃত্রিম-বুদ্ধিমত্তা সিস্টেমগুলোর জন্য; অ্যাডভারসারিয়াল কৌশল ও পদ্ধতির একটি জ্ঞানভাণ্ডার কৃত্রিম বুদ্ধিমত্তা সিস্টেমগুলোর বিরুদ্ধে।
  ​
* মডেল কার্ড – একটি নথি যা একটি এআই মডেলের কার্যক্ষমতা, সীমাবদ্ধতা, নির্ধারিত ব্যবহার এবং নৈতিক বিবেচনাগুলো সম্পর্কে মানসম্মত তথ্য প্রদান করে যাতে স্বচ্ছতা ও দায়িত্বশীল এআই উন্নয়নকে উৎসাহিত করা যায়।
  ​
* মডেল নিষ্কাশন: এমন একটি আক্রমণ যেখানে একজন প্রতিপক্ষ বারবার লক্ষ্য মডেলকে প্রশ্ন করে অনুমতি ছাড়া কার্যকরভাবে সমান নকল তৈরি করতে।
  ​
* মডেল ইনভার্শন: একটি আক্রমণ যা মডেল আউটপুট বিশ্লেষণ করে প্রশিক্ষণ ডেটা পুনর্নির্মাণের চেষ্টা করে।
  ​
* মডেল লাইফসাইকেল ম্যানেজমেন্ট – এআই মডেল লাইফসাইকেল ম্যানেজমেন্ট একটি এআই মডেলের অস্তিত্বের সব ধাপ তত্ত্বাবধান করার প্রক্রিয়া, যার মধ্যে এর নকশা, উন্নয়ন, ডিপ্লয়মেন্ট, মনিটরিং, রক্ষণাবেক্ষণ ও চূড়ান্ত অবসর অন্তর্ভুক্ত, যাতে এটি কার্যকর থাকে এবং উদ্দেশ্যগুলোর সাথে সামঞ্জস্য বজায় রাখে।
  ​
* মডেল দূষণ: প্রশিক্ষণ প্রক্রিয়াকালে একটি মডেলে সরাসরি দুর্বলতা বা ব্যাকডোর সংযোজন করা।
  ​
* মডেল চুরি: ধারাবাহিক জিজ্ঞাসার মাধ্যমে স্বত্ত্বাধিকারপ্রাপ্ত একটি মডেলের নকল বা আনুমানিক সংস্করণ বের করা।
  ​
* মাল্টি-এজেন্ট সিস্টেম: একাধিক আন্তঃক্রিয়াশীল এআই এজেন্ট দ্বারা গঠিত একটি সিস্টেম, প্রতিটি এজেন্টে সম্ভাব্যভাবে ভিন্ন ক্ষমতা ও লক্ষ্য নিয়ে থাকে।
  ​
* OPA (Open Policy Agent): একটি ওপেন-সোর্স নীতি ইঞ্জিন যা স্ট্যাক জুড়ে একীভূত নীতি প্রয়োগ সক্ষম করে।
  ​
* গোপনীয়তা-সংরক্ষণ মেশিন লার্নিং (PPML): প্রশিক্ষণ ডেটার গোপনীয়তা রক্ষা করে এমএল মডেলগুলি প্রশিক্ষণ ও বাস্তবায়নের জন্য পদ্ধতি ও কৌশলসমূহ।
  ​
* প্রম্পট ইনজেকশন: একটি আক্রমণ যেখানে ক্ষতিকারক নির্দেশনা ইনপুটগুলোর মধ্যে লুকিয়ে রাখা হয় যাতে মডেলের নির্ধারিত আচরণকে ওভাররাইড করা যায়।
  ​
* RAG (Retrieval-Augmented Generation): একটি প্রযুক্তি যা বড় ভাষা মডেলগুলিকে উত্তর জেনারেশনের আগে বাইরের জ্ঞানসূত্র থেকে প্রাসঙ্গিক তথ্য উদ্ধার করে তাদের ক্ষমতা বাড়ায়।
  ​
* রেড-টিমিং: দুর্বলতা শনাক্ত করার জন্য অ্যাডভারসারিয়াল আক্রমণগুলোর অনুকরণে এআই সিস্টেমগুলিকে সক্রিয়ভাবে পরীক্ষা করার একটি অনুশীলন।
  ​
* SBOM (Software Bill of Materials): একটি আনুষ্ঠানিক রেকর্ড যা সফটওয়্যার বা AI মডেল নির্মাণে ব্যবহৃত বিভিন্ন উপাদানের বিস্তারিত তথ্য এবং সরবরাহ শৃঙ্খলের সম্পর্কসমূহ ধারণ করে।
  ​
* SHAP (SHapley Additive exPlanations): একটি গেম থিওরেটিক পদ্ধতি যা যেকোনো মেশিন লার্নিং মডেলের আউটপুট ব্যাখ্যা করতে ব্যবহৃত হয়, পূর্বানুমানে প্রতিটি বৈশিষ্ট্যের অবদান গণনা করে।
  ​
* সরবরাহ চেইন আক্রমণ: সিস্টেমকে কম-নিরাপদ উপাদানগুলোকে লক্ষ্য করে তার সরবরাহ শৃঙ্খলেই কম্প্রোমাইজ করা, যেমন তৃতীয় পক্ষের লাইব্রেরি, ডেটাসেট, অথবা পূর্ব-প্রশিক্ষিত মডেল।
  ​
* ট্রান্সফার লার্নিং: একটি কৌশল যেখানে একটি টাস্কের জন্য তৈরি করা মডেলকে দ্বিতীয় টাসকের জন্য মডেলের শুরু পয়েন্ট হিসেবে পুনরায় ব্যবহার করা হয়।
  ​
* ভেক্টর ডেটাবেস: একটি বিশেষ ধরনের ডেটাবেস যা উচ্চ-মাত্রিক ভেক্টর (এম্বেডিংস) সংরক্ষণ করতে এবং দক্ষ সাদৃশ্য অনুসন্ধান করতে ডিজাইন করা হয়েছে।
  ​
* দুর্বলতা স্ক্যানিং: সফটওয়্যার উপাদানগুলোর মধ্যে পরিচিত নিরাপত্তা দুর্বলতাগুলো শনাক্ত করতে ব্যবহৃত স্বয়ংক্রিয় টুলসমূহ, যার মধ্যে এআই ফ্রেমওয়ার্কসমূহ ও নির্ভরতাসমূহ অন্তর্ভুক্ত।
  ​
* ওয়াটারমার্কিং: এআই-উৎপন্ন সামগ্রীতে অদৃশ্য চিহ্ন সন্নিবেশ করার কৌশলসমূহ যাতে তার উৎস শনাক্ত করা যায় অথবা এআই-সৃষ্টি সনাক্ত করা যায়।
  ​
* শূন্য-দিন দুর্বলতা: একটি পূর্বে অজানা দুর্বলতা যা আক্রমণকারীরা একটি প্যাচ তৈরি ও প্রয়োগ করার আগে কাজে লাগাতে পারে।

