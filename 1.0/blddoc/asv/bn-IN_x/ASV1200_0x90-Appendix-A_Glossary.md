# পরিশিষ্ট A: শব্দকোষ

>এই বিস্তৃত শব্দকোষটি AISVS জুড়ে ব্যবহৃত মূল AI, ML, এবং নিরাপত্তা শব্দগুলির সংজ্ঞা প্রদান করে স্পষ্টতা এবং সাধারণ বোঝাপড়া নিশ্চিত করার জন্য।

* বিরোধী উদাহরণ: একটি ইনপুট যা ইচ্ছাকৃতভাবে তৈরি করা হয় একটি AI মডেলকে ভুল করতে বাধ্য করার জন্য, প্রায়শই মানুষের জন্য অদৃশ্য সূক্ষ্ম পরিবর্তন যুক্ত করে।
  ​
* বিপরীতধর্মী স্থায়িত্ব – AI-তে বিপরীতধর্মী স্থায়িত্ব বলতে এমন একটি মডেলের ক্ষমতাকে বোঝায় যা তার কর্মক্ষমতা বজায় রাখতে পারে এবং ইচ্ছাকৃতভাবে নির্মিত দুরুক্তিপূর্ণ ইনপুট দ্বারা প্রতারণা বা প্রভাবিত হওয়া থেকে প্রতিরোধ করতে পারে, যেগুলি ত্রুটি ঘটানোর জন্য ডিজাইন করা হয়।
  ​
* এজেন্ট – AI এজেন্টগুলি হল সফ্টওয়্যার সিস্টেম যা AI ব্যবহার করে ব্যবহারকারীদের পক্ষ থেকে লক্ষ্য অর্জন এবং কাজ সম্পন্ন করে। এরা যুক্তি, পরিকল্পনা, এবং স্মৃতি প্রদর্শন করে এবং সিদ্ধান্ত নেওয়া, শেখা, এবং মানিয়ে নেওয়ার একটি স্তর স্বায়ত্তশাসন ধারণ করে।
  ​
* এজেন্টিক AI: AI সিস্টেম যা নির্দিষ্ট পরিমাণ স্বায়ত্তশাসন সহ কাজ করতে সক্ষম, লক্ষ্য অর্জনের জন্য প্রায়ই নিদের্শমানুষের সরাসরি হস্তক্ষেপ ছাড়াই সিদ্ধান্ত নেয় এবং পদক্ষেপ গ্রহণ করে।
  ​
* অ্যাট্রিবিউট-ভিত্তিক অ্যাক্সেস কন্ট্রোল (ABAC): একটি অ্যাক্সেস কন্ট্রোল প্যারাডাইম যেখানে অনুমোদনের সিদ্ধান্ত ব্যবহারকারী, সম্পদ, ক্রিয়া এবং পরিবেশের অ্যাট্রিবিউটের উপর ভিত্তি করে, যা কুয়েরি সময়ে মূল্যায়ন করা হয়।
  ​
* ব্যাকডোর আক্রমণ: একটি ধরনের ডেটা বিষাক্তকরণ আক্রমণ যেখানে মডেলকে নির্দিষ্ট ট্রিগারগুলির প্রতি নির্দিষ্টভাবে প্রতিক্রিয়া দিতে প্রশিক্ষিত করা হয়, অন্যথায় স্বাভাবিকভাবে আচরণ করে।
  ​
* পক্ষপাত: AI মডেলের আউটপুটে সিস্টেমেটিক ত্রুটিগুলি যা নির্দিষ্ট গোষ্ঠীগুলির জন্য বা নির্দিষ্ট প্রেক্ষাপটে অনায্য বা বৈষম্যমূলক ফলাফল উত্পন্ন করতে পারে।
  ​
* পক্ষপাত শোষণ: একটি আক্রমণ প্রযুক্তি যা কৃত্রিম বুদ্ধিমত্তা মডেলগুলির পরিচিত পক্ষপাতগুলি কাজে লাগিয়ে আউটপুট বা ফলাফল নিয়ন্ত্রণ করে।
  ​
* সিডার: অ্যামাজনের নীতি ভাষা এবং ইঞ্জিন যা এআই সিস্টেমের জন্য ABAC বাস্তবায়নে সূক্ষ্মভাবে নিয়ন্ত্রিত অনুমতিসমূহের জন্য ব্যবহৃত হয়।
  ​
* চেইন অফ থট: একটি কৌশল যা ভাষার মডেলগুলিতে যুক্তিবোধ উন্নত করার জন্য ব্যবহৃত হয়, যেখানে চূড়ান্ত উত্তর দেওয়ার আগে মধ্যবর্তী যুক্তিবোধের ধাপগুলো উত্পন্ন করা হয়।
  ​
* সার্কিট ব্রেকার: এমন যন্ত্র যা নির্দিষ্ট ঝুঁকি সীমা অতিক্রম করলে স্বয়ংক্রিয়ভাবে AI সিস্টেমের কার্যক্রম বন্ধ করে দেয়।
  ​
* ডাটা লিকেজ: এআই মডেলের আউটপুট বা আচরণের মাধ্যমে সংবেদনশীল তথ্যের অবাঞ্ছিত প্রকাশ।
  ​
* ডাটা পয়জনিং: মডেলের অখণ্ডতা ক্ষুণ্ন করার জন্য প্রশিক্ষণের ডেটাকে ইচ্ছাকৃতভাবে ক্ষতিগ্রস্ত করা, প্রায়ই ব্যাকডোর স্থাপন বা কার্যকারিতা হ্রাস করার জন্য।
  ​
* ডিফারেনশিয়াল প্রাইভেসি – ডিফারেনশিয়াল প্রাইভেসি হলো একটি গণিতসম্মত কঠোর কাঠামো যা ডেটাসেট সম্পর্কিত পরিসংখ্যানগত তথ্য প্রকাশ করার সময় ব্যক্তিগত তথ্যের গোপনীয়তা রক্ষা করে। এটি ডেটা ধারকের জন্য গোষ্ঠীর সামগ্রিক প্যাটার্ন শেয়ার করার অনুমতি দেয়, একই সঙ্গে নির্দিষ্ট ব্যক্তিদের সম্পর্কে তথ্য ফাঁস হওয়া সীমাবদ্ধ করে।
  ​
* এম্বেডিংস: তথ্যের ঘন ভেক্টর উপস্থাপন (টেক্সট, ছবি ইত্যাদি) যা উচ্চ-মাত্রিক স্থানে অর্থবোধক অর্থ ধারণ করে।
  ​
* ব্যাখ্যাযোগ্যতা – AI-তে ব্যাখ্যাযোগ্যতা হলো এমন একটি ক্ষমতা যার মাধ্যমে AI সিস্টেম তার সিদ্ধান্ত এবং পূর্বাভাসের জন্য মানুষের বোঝার যোগ্য কারণ প্রদান করতে পারে, যা এর অভ্যন্তরীণ কাজের পদ্ধতি সম্পর্কে অন্তর্দৃষ্টি দেয়।
  ​
* ব্যাখ্যাযোগ্য AI (XAI): বিভিন্ন কৌশল এবং কাঠামোর মাধ্যমে তাদের সিদ্ধান্ত এবং আচরণের মানুষের বোধগম্য ব্যাখ্যা প্রদানের জন্য ডিজাইন করা AI সিস্টেম।
  ​
* ফেডারেটেড লার্নিং: একটি মেশিন লার্নিং পদ্ধতি যেখানে মডেলগুলো একাধিক বিকেন্দ্রীভূত ডিভাইসে স্থানীয় ডেটা নমুনাগুলি ধরে প্রশিক্ষিত হয়, ডেটা নিজে বিনিময় না করে।
  ​
* গার্ডরেলস: বাধানিৱৃত্তি যা AI সিস্টেমগুলি থেকে ক্ষতিকারক, পক্ষপাতদুষ্ট বা অন্য কোনো অনাকাঙ্ক্ষিত আউটপুট উৎপাদন রোধ করতে প্রয়োগ করা হয়।
  ​
* হ্যালুসিনেশন – একটি AI হ্যালুসিনেশন বোঝায় এমন একটি ঘটনা যেখানে একটি AI মডেল তার প্রশিক্ষণ ডেটা বা বাস্তব তথ্যের উপর ভিত্তি না করে ভুল বা বিভ্রান্তিকর তথ্য তৈরি করে।
  ​
* হিউম্যান-ইন-দ্য-লুপ (HITL): সিস্টেমগুলি যা গুরুত্বপূর্ণ সিদ্ধান্ত গ্রহণের মুহূর্তে মানব নজরদারি, যাচাই বা হস্তক্ষেপের প্রয়োজনীয়তা অনুযায়ী ডিজাইন করা হয়েছে।
  ​
* ইনফ্রাস্ট্রাকচার অ্যাজ কোড (IaC): ম্যানুয়াল প্রক্রিয়ার পরিবর্তে কোডের মাধ্যমে ইনফ্রাস্ট্রাকচার পরিচালনা ও প্রোভিশনিং করা, যা নিরাপত্তা স্ক্যানিং এবং সঙ্গতিপূর্ণ ডিপ্লয়মেন্ট সক্ষম করে।
  ​
* জেলব্রেক: AI সিস্টেমগুলিতে, বিশেষ করে বৃহত্তর ভাষা মডেলগুলিতে সুরক্ষা রক্ষাকবচগুলি পাস করার জন্য ব্যবহৃত পদ্ধতি, যা নিষিদ্ধ বিষয়বস্তু উত্পাদন করে।
  ​
* লিস্ট প্রিভিলেজ: ব্যবহারকারী এবং প্রক্রিয়াগুলির জন্য শুধুমাত্র সর্বনিম্ন প্রয়োজনীয় অ্যাক্সেস অধিকার প্রদানের নিরাপত্তা নীতি।
  ​
* LIME (লোকাল ইন্টারপ্রেটেবল মডেল-অ্যাগনস্টিক এক্সপ্লেনেশন): যেকোনো মেশিন লার্নিং ক্লাসিফায়ারের পূর্বানুমানকে স্থানীয়ভাবে একটি ইনটারপ্রেটেবল মডেল দিয়ে আনুমানিক করে ব্যাখ্যা করার একটি পদ্ধতি।
  ​
* সদস্যপদ অনুসন্ধান আক্রমণ: একটি আক্রমণ যা লক্ষ্য করে নির্ধারণ করা যে একটি নির্দিষ্ট ডেটা পয়েন্ট একটি মেশিন লার্নিং মডেল প্রশিক্ষণের জন্য ব্যবহার করা হয়েছে কিনা।
  ​
* MITRE ATLAS: কৃত্রিম-বুদ্ধিমত্তা সিস্টেমের বিরুদ্ধে প্রতিদ্বন্দ্বী হুমকির পরিপ্রেক্ষিত; AI সিস্টেমের বিরুদ্ধে প্রতিদ্বন্দ্বী কৌশল এবং প্রযুক্তিগুলোর জ্ঞানভিত্তিক তথ্যভান্ডার।
  ​
* মডেল কার্ড – একটি মডেল কার্ড হলো একটি নথি যা AI মডেলের কার্যকারিতা, সীমাবদ্ধতা, উদ্দেশ্যমূলক ব্যবহার, এবং নৈতিক বিবেচনা সম্পর্কে মানসম্মত তথ্য প্রদান করে, যা স্বচ্ছতা এবং দায়িত্বশীল AI উন্নয়ন প্রচারের জন্য ব্যবহৃত হয়।
  ​
* মডেল এক্সট্র্যাকশন: একটি আক্রমণ যেখানে একজন প্রতিদ্বন্দ्वी বারম্বার একটি লক্ষ্য মডেলকে প্রশ্ন করে অননুমোদিতভাবে কার্যকরভাবে একই রকম একটি প্রতিলিপি তৈরি করে।
  ​
* মডেল ইনভার্শন: একটি আক্রমণ যা মডেল আউটপুট বিশ্লেষণ করে প্রশিক্ষণ ডেটা পুনর্গঠন করার চেষ্টা করে।
  ​
* মডেল লাইফসাইকেল ম্যানেজমেন্ট – AI মডেল লাইফসাইকেল ম্যানেজমেন্ট হল একটি AI মডেলের অস্তিত্বের সমস্ত পর্যায়ের তত্ত্বাবধান করার প্রক্রিয়া, যার মধ্যে রয়েছে এর ডিজাইন, উন্নয়ন, ডিপ্লয়মেন্ট, মনিটরিং, রক্ষণাবেক্ষণ এবং শেষ পর্যায়ে অবসান, যাতে এটি কার্যকর থাকে এবং লক্ষ্যগুলোর সাথে সামঞ্জস্য বজায় রাখে।
  ​
* মডেল পয়জনিং: প্রশিক্ষণ প্রক্রিয়ার সময় সরাসরি একটি মডেলে দুর্বলতা বা ব্যাকডোর প্রবর্তন।
  ​
* মডেল চুরি/চুরি: পুনরাবৃত্তি প্রশ্নের মাধ্যমে একটি মালিকানাধীন মডেলের একটি কপি বা আনুমানিকতা নির্গত করা।
  ​
* মাল্টি-এজেন্ট সিস্টেম: একাধিক পারস্পরিক ক্রিয়াশীল AI এজেন্ট নিয়ে গঠিত একটি সিস্টেম, যার প্রত্যেকটির ভিন্ন ক্ষমতা এবং লক্ষ্য থাকতে পারে।
  ​
* OPA (ওপেন পলিসি এজেন্ট): একটি ওপেন-সোর্স পলিসি ইঞ্জিন যা স্ট্যাক জুড়ে একীকৃত পলিসি প্রয়োগ সক্ষম করে।
  ​
* গোপনীয়তা সংরক্ষণের জন্য মেশিন লার্নিং (PPML): প্রশিক্ষণ ডেটার গোপনীয়তা রক্ষা করে ML মডেলগুলি প্রশিক্ষণ এবং মোতায়েন করার কৌশল এবং পদ্ধতি।
  ​
* প্রাম্পট ইনজেকশন: একটি আক্রমণ যেখানে মন্দ ইচ্ছার নির্দেশাবলী ইনপুটে সংযুক্ত করা হয় যাতে একটি মডেলের নির্ধারিত আচরণ অতিক্রম করা যায়।
  ​
* RAG (রিট্রিভাল-অগমেন্টেড জেনারেশন): একটি কৌশল যা বৃহৎ ভাষার মডেলগুলিকে উন্নত করে বহিঃস্থ জ্ঞান উৎস থেকে প্রাসঙ্গিক তথ্য পুনরুদ্ধার করে প্রতিক্রিয়া তৈরি করার আগে।
  ​
* রেড-টিমিং: এআই সিস্টেমগুলোকে সক্রিয়ভাবে পরীক্ষার প্রক্রিয়া যেখানে প্রতিদ্বন্দ্বী আক্রমণের সিমুলেশন করে দুর্বলতাগুলো শনাক্ত করা হয়।
  ​
* SBOM (সফটওয়্যার বিল অফ ম্যাটেরিয়ালস): একটি আনুষ্ঠানিক রেকর্ড যা সফটওয়্যার বা AI মডেল তৈরি করার জন্য ব্যবহৃত বিভিন্ন উপাদানের বিবরণ এবং সরবরাহ চেইনের সম্পর্কসমূহ অন্তর্ভুক্ত করে।
  ​
* SHAP (SHapley Additive exPlanations): যেকোনো মেশিন লার্নিং মডেলের আউটপুট ব্যাখ্যা করার জন্য একটি গেম থিওরেটিক পদ্ধতি যা প্রতিটি ফিচারের অবদানের পরিমাপ করে পূর্বাভাসে।
  ​
* সরবরাহ চেইন আক্রমণ: এর সরবরাহ চেইনের কম সুরক্ষিত উপাদানগুলি যেমন তৃতীয় পক্ষ লাইব্রেরি, ডেটাসেট বা প্রি-ট্রেইンド মডেলগুলিকে লক্ষ করে একটি সিস্টেমের নিরাপত্তা ভঙ্গ করা।
  ​
* ট্রান্সফার লার্নিং: একটি প্রযুক্তি যেখানে একটি নির্দিষ্ট কাজের জন্য তৈরি মডেলকে দ্বিতীয় কাজের জন্য মডেলের সূচনা বিন্দু হিসেবে পুনরায় ব্যবহার করা হয়।
  ​
* ভেক্টর ডাটাবেস: একটি বিশেষায়িত ডাটাবেস যা উচ্চ-মাত্রিক ভেক্টর (এম্বেডিংস) সংরক্ষণ এবং দক্ষ সাদৃশ্য অনুসন্ধান সম্পাদনের জন্য ডিজাইন করা হয়েছে।
  ​
* অসুরক্ষা স্ক্যানিং: স্বয়ংক্রিয় টুলগুলি যা সফটওয়্যার উপাদানগুলিতে পরিচিত নিরাপত্তা দুর্বলতাগুলি সনাক্ত করে, যার মধ্যে AI ফ্রেমওয়ার্ক এবং নির্ভরতা অন্তর্ভুক্ত।
  ​
* ওয়াটারমার্কিং: এআই-উৎপাদিত সামগ্রীতে অদৃশ্য চিহ্ন প্রবেশ করানোর প্রযুক্তি যা তার উৎস ট্র্যাক করতে বা এআই উৎপাদন সনাক্ত করতে ব্যবহৃত হয়।
  ​
* জিরো-ডে দুর্বলতা: একটি পূর্বে অজানা দুর্বলতা যা আক্রমণকারীরা ব্যবহার করতে পারে ডেভেলপাররা একটি প্যাচ তৈরি এবং স্থাপন করার আগে।

