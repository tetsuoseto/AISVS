# 付録A：用語集

この包括的な用語集は、AISVS全体で使用される主要なAI、ML、およびセキュリティ用語の定義を提供し、明確さと共通理解を確保します。

* 敵対的サンプル：人間には認識できない微妙な摂動を加えて、AIモデルに誤りを犯させることを意図的に作成された入力。
  ​
* 敵対的ロバストネス – 敵対的ロバストネスとは、AIモデルが意図的に作成された悪意ある入力によってエラーを引き起こされることなく、その性能を維持し続ける能力を指します。
  ​
* エージェント – AIエージェントは、ユーザーの代わりに目標を追求しタスクを完了するためにAIを使用するソフトウェアシステムです。彼らは推論、計画、記憶を示し、意思決定、学習、適応を行う一定の自律性を持っています。
  ​
* エージェンシックAI：目標を達成するためにある程度の自律性を持って動作できるAIシステムであり、しばしば直接の人間の介入なしに意思決定や行動を行うもの。
  ​
* 属性ベースアクセス制御（ABAC）：ユーザー、リソース、アクション、環境の属性に基づいて認可決定を行い、クエリ時に評価されるアクセス制御のパラダイム。
  ​
* バックドア攻撃：モデルが特定のトリガーに対して特定の反応を示すように訓練され、それ以外の状況では通常通りに動作するデータポイズニング攻撃の一種。
  ​
* バイアス：特定のグループや特定の文脈で不公平または差別的な結果をもたらす可能性のある、AIモデルの出力における体系的な誤り。
  ​
* バイアス悪用：AIモデルに存在する既知のバイアスを利用して、出力や結果を操作する攻撃手法。
  ​
* Cedar: AIシステムのABAC実装に使用される、細粒度の権限管理のためのAmazonのポリシー言語およびエンジン。
  ​
* Chain of Thought: 最終的な回答を生成する前に中間的な推論ステップを生成することで、言語モデルの推論能力を向上させる手法。
  ​
* サーキットブレーカー：特定のリスク閾値を超えた場合にAIシステムの動作を自動的に停止するメカニズム。
  ​
* 機密推論サービス：信頼される実行環境（TEE）または同等の機密計算メカニズム内でAIモデルを実行し、モデルの重みおよび推論データが暗号化され、封印され、不正アクセスや改ざんから保護されることを保証する推論サービス。
  ​
* 機密ワークロード：コード、データ、およびモデルをホストや共用テナントのアクセスから保護するために、ハードウェアによる隔離、メモリ暗号化、およびリモート認証を備えた信頼できる実行環境（TEE）内で実行されるAIワークロード（例：トレーニング、推論、前処理）。
  ​
* データ漏洩：AIモデルの出力や挙動を通じて機密情報が意図せず露出すること。
  ​
* データポイズニング：モデルの完全性を損なうためにトレーニングデータを意図的に破損させること。これによりバックドアを仕込んだり、性能を低下させたりすることが多い。
  ​
* 差分プライバシー – 差分プライバシーは、個々のデータ主体のプライバシーを保護しながらデータセットに関する統計情報を公開するための数学的に厳密なフレームワークです。これにより、データ保持者は特定の個人に関する情報の漏洩を制限しつつ、グループの集約的なパターンを共有することが可能になります。
  ​
* 埋め込み：意味的な意味を高次元空間で捉えるデータ（テキスト、画像など）の密なベクトル表現。
  ​
* 説明可能性 – 説明可能性とは、AIシステムがその決定や予測に対して人間が理解できる理由を提供し、その内部の動作に関する洞察を示す能力を指します。
  ​
* 説明可能なAI（XAI）：さまざまな技術やフレームワークを通じて、その決定や振る舞いに対する人間に理解可能な説明を提供するよう設計されたAIシステム。
  ​
* フェデレーテッドラーニング：複数の分散されたデバイスがローカルのデータサンプルを保持したまま、データ自体を交換することなくモデルをトレーニングする機械学習のアプローチ。
  ​
* 定式化：ハイパーパラメータ、トレーニング構成、前処理手順、またはビルドスクリプトなど、成果物やデータセットを生成するために使用されるレシピまたは方法。
  ​
* ガードレール：AIシステムが有害、偏った、またはその他望ましくない出力を生成するのを防ぐために実装された制約。
  ​
* 幻覚 – AI幻覚とは、AIモデルがトレーニングデータや事実に基づかない誤ったまたは誤解を招く情報を生成する現象を指します。
  ​
* ヒューマン・イン・ザ・ループ（HITL）：重要な意思決定の局面で人間の監督、検証、または介入を必要とするように設計されたシステム。
  ​
* インフラストラクチャ・アズ・コード（IaC）：手動プロセスの代わりにコードを使ってインフラストラクチャの管理とプロビジョニングを行い、セキュリティスキャンや一貫したデプロイメントを可能にする手法。
  ​
* Jailbreak：特に大規模言語モデルにおいて、禁止されたコンテンツを生成するためにAIシステムの安全ガードレールを回避する技術。
  ​
* 最小権限：ユーザーやプロセスに対して必要最低限のアクセス権のみを付与するというセキュリティ原則。
  ​
* LIME（局所的解釈可能モデル非依存型説明）：任意の機械学習分類器の予測を、解釈可能なモデルで局所的に近似することによって説明する技術。
  ​
* メンバーシップ推論攻撃: 特定のデータポイントが機械学習モデルの訓練に使用されたかどうかを判断することを目的とした攻撃。
  ​
* MITRE ATLAS：人工知能システムに対する敵対的脅威環境；AIシステムに対する敵対的戦術と技術のナレッジベース。
  ​
* モデルカード – モデルカードは、AIモデルの性能、制限、意図された使用法、および倫理的配慮に関する標準化された情報を提供する文書であり、透明性と責任あるAI開発を促進するためのものです。
  ​
* モデル抽出攻撃：攻撃者が対象モデルに繰り返しクエリを送信し、許可なく機能的に類似したコピーを作成する攻撃。
  ​
* モデルインバージョン：モデルの出力を分析してトレーニングデータを再構築しようとする攻撃。
  ​
* モデルライフサイクル管理 – AIモデルライフサイクル管理とは、AIモデルの設計、開発、展開、監視、保守、最終的な廃止までの全段階を監督し、モデルが効果的で目的に合致し続けることを保証するプロセスです。
  ​
* モデルポイズニング：トレーニングプロセス中にモデルに直接脆弱性やバックドアを導入すること。
  ​
* モデル盗用/窃盗：繰り返しのクエリを通じて、独自モデルのコピーまたは近似を抽出すること。
  ​
* マルチエージェントシステム：複数の相互作用するAIエージェントから構成され、それぞれ異なる能力や目標を持つ可能性があるシステム。
  ​
* OPA（Open Policy Agent）：スタック全体で統一されたポリシー適用を可能にするオープンソースのポリシーエンジン。
  ​
* 来歴：遺物やデータセットの系統および管理連鎖であり、その起源、取扱者、転送経路、および整合性の証拠（例：チェックサム、署名）を含むもの。
  ​
* プライバシー保護機械学習（PPML）：トレーニングデータのプライバシーを保護しながら機械学習モデルを学習および展開するための技術と方法。
  ​
* プロンプトインジェクション：モデルの意図された動作を上書きするために悪意のある指示が入力に埋め込まれる攻撃。
  ​
* RAG（Retrieval-Augmented Generation）：応答を生成する前に外部の知識ソースから関連情報を検索することで、大規模言語モデルを強化する技術。
  ​
* レッドチーミング：脆弱性を特定するために、敵対的な攻撃をシミュレートしてAIシステムを積極的にテストする手法。
  ​
* SLSA プロベナンス：アーティファクトがどこで、いつ、どのように作成されたかを記録する、SLSA フレームワークによって定義されたメタデータ（例：ソースリポジトリ、コミットハッシュ、ビルドシステム、およびパラメータ）。
  ​
* SBOM（ソフトウェア部品表）：ソフトウェアやAIモデルを構築する際に使用されるさまざまなコンポーネントの詳細およびサプライチェーンの関係を含む正式な記録。
  ​
* SHAP（SHapley Additive exPlanations）：各特徴量の予測への寄与度を計算することで、任意の機械学習モデルの出力を説明するゲーム理論に基づく手法。
  ​
* 強力な認証：少なくとも二つの要素（知識、所有、固有性）およびFIDO2/WebAuthn、証明書ベースのサービス認証、短期間有効なトークンなどのフィッシング耐性メカニズムを必要とし、資格情報の盗難やリプレイ攻撃に対抗する認証。
  ​
* サプライチェーン攻撃：サードパーティのライブラリやデータセット、事前学習モデルなど、サプライチェーン内のセキュリティが低い要素を標的にしてシステムを侵害すること。
  ​
* 転移学習：あるタスクのために開発されたモデルを、別のタスクのモデルの出発点として再利用する技術。
  ​
* ベクターデータベース：高次元ベクトル（埋め込み）を格納し、効率的な類似検索を行うために設計された特化型データベース。
  ​
* 脆弱性スキャン：AIフレームワークや依存関係を含むソフトウェアコンポーネント内の既知のセキュリティ脆弱性を特定する自動化ツール。
  ​
* ウォーターマーキング：AI生成コンテンツに目に見えないマーカーを埋め込み、その起源を追跡したりAI生成を検出したりする技術。
  ​
* ゼロデイ脆弱性：開発者がパッチを作成・展開する前に攻撃者が悪用できる、これまで知られていなかった脆弱性。

