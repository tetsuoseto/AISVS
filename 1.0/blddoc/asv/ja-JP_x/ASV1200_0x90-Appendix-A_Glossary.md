# 付録A: 用語集

>This この包括的な用語集は、AISVS 全体で使用される主要な人工知能（AI）、機械学習（ML）、およびセキュリティ用語の定義を提供し、明確さと共通理解を確保します。

* 敵対的サンプル: AIモデルが誤作動するように意図的に作成された入力であり、しばしば人間には知覚できない微小な摂動を加えることによって引き起こされる。
  ​
* 敵対的頑健性 – AIにおける敵対的頑健性とは、モデルの性能を維持し、誤作動を引き起こすことを意図して作成された悪意ある入力によってだまされたり操作されたりするのを防ぐ能力を指す。
  ​
* エージェント – AIエージェントは、ユーザーに代わって目標を追求し、タスクを完了させるためにAIを用いたソフトウェアシステムです。彼らは推論、計画、記憶を示し、意思決定を行い、学習し、適応する自律性をある程度持っています。
  ​
* エージェント性を持つAI: 目標を達成するために一定の自律性を持って動作できるAIシステムであり、しばしば人間の直接的な介入なしに意思決定を行い、行動を起こす。
  ​
* 属性ベースアクセス制御（ABAC）：ユーザー、リソース、アクション、環境の属性に基づいて認可決定を行い、クエリ時に評価されるアクセス制御のパラダイム。
  ​
* バックドア攻撃: モデルが特定のトリガーに対して特定の応答をするよう訓練され、その他は通常通り動作するデータポイズニング攻撃の一種。
  ​
* バイアス: AIモデルの出力における系統的な偏りは、特定の集団や特定の文脈において不公平または差別的な結果を生じさせる可能性がある。
  ​
* バイアスの悪用: 人工知能モデルに存在する既知のバイアスを利用して、出力や結果を操作する攻撃手法。
  ​
* Cedar: AI システムの ABAC を実装する際に用いられる、細粒度の権限を扱う Amazon のポリシー言語とエンジン。
  ​
* 思考の連鎖（Chain of Thought）：言語モデルの推論を改善するための手法で、最終的な回答を生成する前に中間の推論ステップを生成する。
  ​
* サーキットブレーカー: 特定のリスク閾値を超えた場合に、AIシステムの運用を自動的に停止させる仕組み。
  ​
* データ漏洩：AIモデルの出力や挙動を通じた機密情報の意図しない露出。
  ​
* データ汚染: モデルの完全性を損なうために、学習データを意図的に改ざんする行為。多くはバックドアの設置や性能の低下を狙う。
  ​
* 差分プライバシーは、データセットに関する統計情報を公開する際に個々のデータ主体のプライバシーを保護する、数学的に厳密な枠組みです。これにより、データ保有者はグループ全体の集計パターンを共有しつつ、特定の個人に関する情報の漏えいを制限できます。
  ​
* 埋め込み表現：テキストや画像などのデータを高次元空間で意味を捉える密なベクトル表現。
  ​
* 説明可能性 – AIにおける説明可能性は、AIシステムがその意思決定と予測に対して人間が理解できる理由を提供する能力であり、内部動作への洞察を提供します。
  ​
* 説明可能な人工知能（XAI）：決定と行動に対して人間が理解できる説明を提供するために、さまざまな技術とフレームワークを用いて設計された人工知能システム。
  ​
* 連邦学習（フェデレーテッドラーニング）: ローカルデータサンプルを保持する複数の分散デバイス上で、データ自体を交換することなくモデルを訓練する機械学習のアプローチ。
  ​
* ガードレール: AI システムが有害な、偏った、またはその他望ましくない出力を生成するのを防ぐために実装された制約。
  ​
* 幻覚 – AIの幻覚とは、AIモデルが訓練データや事実の現実に基づかない、誤ったまたは誤解を招く情報を生成する現象を指します。
  ​
* ヒューマン・イン・ザ・ループ（HITL）：重要な意思決定のポイントで、人間の監視、検証、または介入を必要とするよう設計されたシステム。
  ​
* Infrastructure as Code (IaC): コードを用いてインフラストラクチャを管理・プロビジョニングすることで、手動プロセスを排除し、セキュリティスキャンを可能にし、一貫したデプロイを実現します。
  ​
* ジャイルブレイク: AIシステムの安全ガードレールを回避するために用いられる手法、特に大規模言語モデルにおいて、禁止コンテンツを生成することを目的とする。
  ​
* 最小権限の原則: ユーザーおよびプロセスに対して、必要最小限のアクセス権のみを付与するセキュリティ原則。
  ​
* LIME（局所的に解釈可能なモデル-非依存の説明）: 任意の機械学習分類器の予測を、解釈可能なモデルで局所的に近似することによって説明する手法。
  ​
* メンバーシップ推論攻撃: 特定のデータポイントが機械学習モデルの訓練に使用されたかどうかを判定することを目的とする攻撃。
  ​
* MITRE ATLAS: 人工知能システムに対する敵対的脅威の全体像; 人工知能システムに対する敵対的な戦術と技術の知識ベース。
  ​
* モデルカード – モデルカードとは、AIモデルの性能、制限、意図された用途、倫理的配慮について標準化された情報を提供する文書です。
  ​
* モデル抽出攻撃: 攻撃者が標的モデルに対して繰り返しクエリを送信し、許可なく機能的に類似したコピーを作成する。
  ​
* モデル反転攻撃: モデルの出力を分析して学習データを再構築しようとする攻撃。
  ​
* モデルライフサイクル管理 – AIモデルのライフサイクル管理は、AIモデルの存在における全段階を監督するプロセスであり、設計、開発、デプロイ、監視、保守、そして最終的な退役を含むことで、AIモデルが有効で目的に沿っている状態を維持することを目的とします。
  ​
* モデルポイズニング：トレーニング過程でモデルに直接脆弱性やバックドアを注入すること。
  ​
* モデル盗用：専有モデルのコピーまたは近似を、繰り返しのクエリを通じて抽出すること。
  ​
* マルチエージェントシステム: 複数の相互作用するAIエージェントで構成され、各エージェントは潜在的に異なる能力と目標を持つ。
  ​
* OPA（Open Policy Agent）：スタック全体にわたる統一的なポリシー適用を可能にするオープンソースのポリシーエンジン。
  ​
* プライバシー保護機械学習（PPML）：学習データのプライバシーを保護しながら、機械学習モデルを訓練・展開するための技術と手法。
  ​
* プロンプトインジェクション攻撃: 入力に悪意のある指示が埋め込まれ、モデルの意図した挙動を上書きする攻撃。
  ​
* RAG（Retrieval-Augmented Generation、情報検索を組み込んだ生成）: 応答を生成する前に外部知識ソースから関連情報を取得して大規模言語モデルを強化する技術。
  ​
* レッドチーミング: AIシステムを対象に、敵対的な攻撃をシミュレートして脆弱性を特定するための積極的なテストの実践。
  ​
* SBOM（Software Bill of Materials）: ソフトウェアやAIモデルの構築に使用されるさまざまな部品の詳細とサプライチェーンの関係を含む公式な記録。
  ​
* SHAP（SHapley Additive exPlanations）：予測に対する各特徴量の寄与度を計算することによって、任意の機械学習モデルの出力を説明するゲーム理論的アプローチ。
  ​
* サプライチェーン攻撃: サプライチェーン内の安全性が低い要素を標的とすることでシステムを侵害する行為。例として、サードパーティ製ライブラリ、データセット、または事前学習済みモデルなどが挙げられる。
  ​
* 転移学習: あるタスクのために開発されたモデルを、別のタスクのモデルの出発点として再利用する手法。
  ​
* ベクトルデータベース： 高次元ベクトル（埋め込み）を格納し、効率的な類似検索を実行するよう設計された特殊なデータベース。
  ​
* 脆弱性スキャン: AIフレームワークや依存関係を含むソフトウェア部品の既知のセキュリティ脆弱性を識別する自動ツール。
  ​
* 透かし技術: AI生成コンテンツに知覚できないマーカーを埋め込み、その出所を追跡したりAI生成であることを検出したりする手法。
  ​
* ゼロデイ脆弱性: 開発者がパッチを作成して配布する前に、攻撃者が悪用できる、以前は未知であった脆弱性。

