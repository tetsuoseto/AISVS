# 付録A：用語集

この包括的な用語集は、AISVS全体で使用される主要なAI、ML、およびセキュリティ用語の定義を提供し、明確さと共通理解を確保します。

* 敵対的サンプル：人間には知覚できない微妙な摂動を加えることで、AIモデルに誤りを引き起こさせるために意図的に作成された入力。
  ​
* 敵対的ロバストネス – AIにおける敵対的ロバストネスとは、意図的に作成された悪意のある入力によってエラーを引き起こされることなく、モデルがその性能を維持し抵抗する能力を指します。
  ​
* エージェント – AIエージェントは、ユーザーに代わって目標を追求し、タスクを完了するためにAIを使用するソフトウェアシステムです。これらは推論、計画、記憶を示し、意思決定、学習、適応を行う一定の自律性を持っています。
  ​
* エージェンティックAI：ある程度の自律性を持ち、目標を達成するために動作できるAIシステムであり、しばしば人間による直接の介入なしに意思決定と行動を行う。
  ​
* 属性ベースアクセス制御（ABAC）：ユーザー、リソース、操作、および環境の属性に基づいて権限の決定を行い、クエリ時に評価されるアクセス制御のパラダイム。
  ​
* バックドア攻撃：モデルが特定のトリガーに対して特定の応答を示すように訓練され、それ以外の場合は通常通りに動作するタイプのデータ汚染攻撃。
  ​
* バイアス：特定のグループや特定のコンテキストにおいて、不公平または差別的な結果をもたらす可能性のあるAIモデルの出力における体系的な誤り。
  ​
* バイアス悪用：AIモデルに存在する既知のバイアスを利用して、出力や結果を操作する攻撃技術。
  ​
* Cedar: AIシステムのABAC実装に使用される、細粒度の権限管理のためのAmazonのポリシー言語およびエンジン。
  ​
* Chain of Thought: 最終的な回答を出す前に、中間的な推論ステップを生成することで言語モデルの推論能力を改善する技術。
  ​
* サーキットブレーカー：特定のリスクしきい値を超えた場合にAIシステムの動作を自動的に停止するメカニズム。
  ​
* データ漏洩：AIモデルの出力や挙動を通じた機密情報の意図しない露出。
  ​
* データポイズニング：バックドアを仕込んだり性能を劣化させたりするために、モデルの整合性を損なうことを目的としてトレーニングデータを意図的に改ざんすること。
  ​
* 差分プライバシー – 差分プライバシーは、個々のデータ被験者のプライバシーを保護しながら、データセットに関する統計情報を公開するための数学的に厳密なフレームワークです。これにより、データ保持者は特定の個人に関する情報の漏洩を制限しつつ、グループの集計パターンを共有することが可能になります。
  ​
* 埋め込み：データ（テキスト、画像など）の意味的な意味を高次元空間で捉える密なベクトル表現。
  ​
* 説明可能性 – AIにおける説明可能性とは、AIシステムがその決定や予測に対して人間が理解できる理由を提供し、内部の仕組みについての洞察を示す能力のことです。
  ​
* 説明可能なAI（XAI）：さまざまな技術やフレームワークを通じて、その意思決定や行動について人間が理解可能な説明を提供するように設計されたAIシステム。
  ​
* フェデレーテッドラーニング：データ自体を交換することなく、複数の分散型デバイス上のローカルデータサンプルを保持しながらモデルを学習させる機械学習のアプローチ。
  ​
* ガードレール：AIシステムが有害であったり、偏ったり、その他望ましくない出力を生成するのを防ぐために実装される制約。
  ​
* 幻覚 – AI幻覚とは、AIモデルが訓練データや事実に基づかない誤ったまたは誤解を招く情報を生成する現象を指します。
  ​
* ヒューマンインザループ（HITL）：重要な意思決定ポイントで人間の監視、検証、または介入を必要とするよう設計されたシステム。
  ​
* インフラストラクチャー・アズ・コード（IaC）：手動プロセスの代わりにコードを通じてインフラストラクチャーを管理およびプロビジョニングし、セキュリティスキャンと一貫したデプロイメントを可能にすること。
  ​
* Jailbreak: 特に大規模言語モデルにおいて、AIシステムの安全ガードレールを回避して禁止されたコンテンツを生成するために使用される技術。
  ​
* 最小権限: ユーザーおよびプロセスに対して必要最低限のアクセス権のみを付与するセキュリティ原則。
  ​
* LIME（ローカル解釈可能モデル非依存性説明）：任意の機械学習分類器の予測を、解釈可能なモデルで局所的に近似することにより説明する手法。
  ​
* メンバーシップ推論攻撃：特定のデータポイントが機械学習モデルのトレーニングに使用されたかどうかを判定することを目的とした攻撃。
  ​
* MITRE ATLAS: 人工知能システムに対する敵対的脅威の全体像；AIシステムに対する敵対的戦術と技術のナレッジベース。
  ​
* モデルカード – モデルカードは、AIモデルのパフォーマンス、制限、意図された用途、および倫理的考慮事項に関する標準化された情報を提供し、透明性と責任あるAI開発を促進する文書です。
  ​
* モデル抽出：攻撃者がターゲットモデルに繰り返しクエリを送信し、許可なく機能的に類似したコピーを作成する攻撃。
  ​
* モデル反転攻撃：モデルの出力を解析して学習データを再構築しようとする攻撃。
  ​
* モデルライフサイクル管理 – AIモデルライフサイクル管理は、AIモデルの設計、開発、展開、監視、保守、そして最終的な引退を含むすべての段階を監督し、その効果を維持し目標に合致していることを確保するプロセスです。
  ​
* モデルポイズニング：トレーニングプロセス中にモデルに直接脆弱性やバックドアを導入すること。
  ​
* モデル盗用/窃取：繰り返しのクエリによって、専有モデルのコピーまたは近似を抽出すること。
  ​
* マルチエージェントシステム：複数の相互作用するAIエージェントで構成され、それぞれが異なる能力や目標を持つ可能性があるシステム。
  ​
* OPA（Open Policy Agent）：スタック全体で統一されたポリシー適用を可能にするオープンソースのポリシーエンジン。
  ​
* プライバシー保護機械学習（PPML）：トレーニングデータのプライバシーを保護しながら、MLモデルをトレーニングおよび展開するための技術と方法。
  ​
* プロンプトインジェクション：悪意のある命令が入力に埋め込まれ、モデルの意図された動作を上書きする攻撃。
  ​
* RAG（Retrieval-Augmented Generation）：応答を生成する前に外部の知識ソースから関連情報を取得することで、大規模言語モデルを強化する手法。
  ​
* レッドチーミング：AIシステムの脆弱性を特定するために、敵対的攻撃をシミュレートして積極的にテストする手法。
  ​
* SBOM（ソフトウェア部品表）：ソフトウェアやAIモデルの構築に使用されるさまざまなコンポーネントの詳細およびサプライチェーンの関係を含む正式な記録。
  ​
* SHAP（SHapley Additive exPlanations）：各特徴量の予測への寄与度を計算することで、任意の機械学習モデルの出力を説明するゲーム理論に基づくアプローチ。
  ​
* サプライチェーン攻撃：サードパーティのライブラリ、データセット、または事前学習済みモデルなど、サプライチェーン内のセキュリティが弱い要素を狙ってシステムを侵害すること。
  ​
* 転移学習：あるタスクのために開発されたモデルを、別のタスクのモデルの出発点として再利用する技術。
  ​
* ベクターデータベース：高次元ベクトル（埋め込み）を保存し、効率的な類似検索を実行するために設計された専門的なデータベース。
  ​
* 脆弱性スキャン：AIフレームワークや依存関係を含むソフトウェアコンポーネントの既知のセキュリティ脆弱性を特定する自動化ツール。
  ​
* ウォーターマーキング：AI生成コンテンツに埋め込まれる識別不可能なマーカーを用いて、その起源を追跡したり、AI生成を検出したりする技術。
  ​
* ゼロデイ脆弱性：開発者がパッチを作成および展開する前に攻撃者が悪用できる、以前に知られていなかった脆弱性。

