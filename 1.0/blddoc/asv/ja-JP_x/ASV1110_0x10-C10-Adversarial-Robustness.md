# 10 敵対的ロバストネス & プライバシー保護

## 統制目標

AIモデルは、回避攻撃、推論攻撃、抽出攻撃、またはポイズニング攻撃に直面しても、信頼性を維持し、プライバシーを保護し、乱用耐性を確保する。

---

## 10.1 モデルのアラインメントと安全性

有害な内容やポリシー違反の出力を防ぐ。

|   #    | 説明                                                                                              | レベル | 役割  |
| :----: | ----------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.1.1 | アラインメント テスト-スイート（レッドチームのプロンプト、ジャイルブレイク・プローブ、禁止コンテンツ）がバージョン管理下に置かれており、すべてのモデルリリース時に実行されることを検証する。 |  1  | D/V |
| 10.1.2 | 拒否および安全完了のガードレールが適用されていることを検証する。                                                                |  1  |  D  |
| 10.1.3 | 自動評価システムが有害コンテンツの割合を測定し、設定された閾値を超える回帰を検出してフラグを付けることを検証する。                                       |  2  | D/V |
| 10.1.4 | ジャイルブレイク対策トレーニングが文書化され、再現可能であることを検証する。                                                          |  2  |  D  |
| 10.1.5 | 形式的なポリシー遵守の証明または認定監視が、重要な領域を網羅していることを検証する。                                                      |  3  |  V  |

---

## 10.2 敵対的サンプルの堅牢化

改ざんされた入力に対する耐性を高める。堅牢なアドバーサリアルトレーニングとベンチマーク評価が現在の最良の実践です。

|   #    | 説明                                                     | レベル | 役割  |
| :----: | ------------------------------------------------------ | :-: | :-: |
| 10.2.1 | プロジェクトのリポジトリに、再現性のあるシードを用いた敵対的訓練の設定が含まれていることを検証してください。 |  1  |  D  |
| 10.2.2 | 敵対的サンプル検出が本番パイプラインで遮断アラートを発生させることを検証する。                |  2  | D/V |
| 10.2.4 | 認定済みの頑健性証明または区間境界証明が、少なくとも最重要クラスをカバーしていることを検証する。       |  3  |  V  |
| 10.2.5 | 回帰テストが適応的攻撃を用いて、測定可能な頑健性の低下がないことを確認する。                 |  3  |  V  |

---

## 10.3 メンバーシップ-推論 緩和

訓練データに特定のレコードが含まれていたかどうかを判断する能力を制限する。差分プライバシーと信頼度スコアのマスキングは、現時点で知られている防御策の中で最も効果的である。

|   #    | 説明                                                                      | レベル | 役割  |
| :----: | ----------------------------------------------------------------------- | :-: | :-: |
| 10.3.1 | クエリごとのエントロピー正則化または温度スケーリングが過剰に自信を持つ予測を低減することを検証する。                      |  1  |  D  |
| 10.3.2 | 機微データセットに対して、トレーニングが ε-有界な差分プライベート最適化を採用していることを検証してください。                |  2  |  D  |
| 10.3.3 | 攻撃シミュレーション（シャドーモデルまたはブラックボックス）が、ホールドアウトデータ上で攻撃AUC ≤ 0.60を示すことを検証してください。 |  2  |  V  |

---

## 10.4 モデル-反転耐性

機微属性の再構築を防ぐ。最近の調査は、出力の切り捨てと微分プライバシーの保証を実用的な防御として強調している。

|   #    | 説明                                                     | レベル | 役割  |
| :----: | ------------------------------------------------------ | :-: | :-: |
| 10.4.1 | 機微属性が直接出力されないことを検証してください。必要に応じて、バケットまたは一方向変換を使用してください。 |  1  |  D  |
| 10.4.2 | 同一のプリンシパルからの繰り返しの適応的クエリがクエリレート制限によって抑制されることを検証する。      |  1  | D/V |
| 10.4.3 | モデルがプライバシー保護ノイズを用いて訓練されていることを検証する。                     |  2  |  D  |

---

## 10.5 モデル抽出防御

無断複製を検出し、抑止します。透かし技術とクエリパターン分析を推奨します。

|   #    | 説明                                                                 | レベル | 役割  |
| :----: | ------------------------------------------------------------------ | :-: | :-: |
| 10.5.1 | 推論ゲートウェイが、モデルの記憶閾値に合わせて調整されたグローバルおよび APIキーごとのレート制限を適用していることを検証する。  |  1  |  D  |
| 10.5.2 | クエリ-エントロピーと入力-多様性の統計量が自動抽出検出器に供給されることを検証する。                        |  2  | D/V |
| 10.5.3 | 脆弱な水印または確率的水印が、疑われるクローンに対して、p < 0.01 で ≤ 1 000 回のクエリで証明できることを検証する。 |  2  |  V  |
| 10.5.4 | ウォーターマークキーとトリガーセットがハードウェアセキュリティモジュールに格納され、毎年ローテーションされることを確認してください。 |  3  |  D  |
| 10.5.5 | 抽出アラートイベントに不正なクエリが含まれており、インシデント対応プレイブックと統合されていることを検証してください。        |  3  |  V  |

---

## 10.6 推論-時間 汚染-データ 検出

バックドアが仕込まれた入力と汚染された入力を識別し、無害化する。

|   #    | 説明                                                          | レベル | 役割  |
| :----: | ----------------------------------------------------------- | :-: | :-: |
| 10.6.1 | モデル推論の前に、入力が異常検出器（例：STRIP、consistency-scoring）を通過することを検証する。 |  1  |  D  |
| 10.6.2 | 検出器の閾値が、クリーン/毒化された検証セット上で調整され、偽陽性を5%未満に抑えることを確認する。          |  1  |  V  |
| 10.6.3 | 汚染されたとフラグ付けされた入力が、ソフトブロックおよび人間の審査ワークフローをトリガーすることを検証してください。  |  2  |  D  |
| 10.6.4 | 検出器が適応的なトリガーなしバックドア攻撃に対してストレステストされていることを検証する。               |  2  |  V  |
| 10.6.5 | 検出有効性指標がログに記録され、最新の脅威インテリジェンスを用いて定期的に再評価されていることを確認してください。   |  3  |  D  |

---

## 10.7 動的 セキュリティ ポリシー 適応

脅威情報と行動分析に基づくリアルタイムのセキュリティポリシー更新。

|   #    | 説明                                                               | レベル | 役割  |
| :----: | ---------------------------------------------------------------- | :-: | :-: |
| 10.7.1 | エージェントを再起動することなくセキュリティポリシーを動的に更新できることを、ポリシーのバージョン整合性を維持したまま検証する。 |  1  | D/V |
| 10.7.2 | ポリシー更新が認可されたセキュリティ担当者によって暗号署名され、適用前に検証されることを確認する。                |  2  | D/V |
| 10.7.3 | 動的ポリシー変更が、正当化、承認チェーン、ロールバック手順を含む完全な監査証跡として記録されることを検証する。          |  2  | D/V |
| 10.7.4 | 適応型セキュリティ機構が、リスク文脈と行動パターンに基づいて脅威検知の感度を調整することを検証します。              |  3  | D/V |
| 10.7.5 | ポリシーの適応決定が説明可能であることを検証し、セキュリティチームの審査のための証跡を含める。                  |  3  | D/V |

---

## 10.8 リフレクションに基づくセキュリティ分析

エージェントの自己反省とメタ認知分析を通じたセキュリティ検証。

|   #    | 説明                                                             | レベル | 役割  |
| :----: | -------------------------------------------------------------- | :-: | :-: |
| 10.8.1 | エージェントのリフレクション機構が、意思決定と行動に対するセキュリティ重視の自己評価を含んでいることを検証する。       |  1  | D/V |
| 10.8.2 | リフレクション出力が検証され、敵対的入力による自己評価機構の改ざんを防止することを確認する。                 |  2  | D/V |
| 10.8.3 | メタ認知セキュリティ分析が、エージェントの推論プロセスにおける潜在的なバイアス、操作、または妥協を識別できることを検証する。 |  2  | D/V |
| 10.8.4 | リフレクションを用いたセキュリティ警告が、強化監視および人手介入の可能性があるワークフローをトリガーすることを検証する。   |  3  | D/V |
| 10.8.5 | セキュリティに関する洞察からの継続的な学習が、脅威検知を向上させ、正当な機能を低下させないことを検証する。          |  3  | D/V |

---

## 10.9 進化 & 自己改善 セキュリティ

自己改変と進化が可能なエージェントシステムのセキュリティ対策

|   #    | 説明                                                 | レベル | 役割  |
| :----: | -------------------------------------------------- | :-: | :-: |
| 10.9.1 | 自己修正機能が指定された安全領域に限定され、形式検証の境界が適用されていることを検証する。      |  1  | D/V |
| 10.9.2 | 実装前に進化提案がセキュリティ影響評価を受けることを確認する。                    |  2  | D/V |
| 10.9.3 | 自己改善メカニズムには、整合性検証を伴うロールバック機能が含まれていることを検証する。        |  2  | D/V |
| 10.9.4 | メタ学習のセキュリティが、改善アルゴリズムに対する敵対的な改変を防ぐことを検証する。         |  3  | D/V |
| 10.9.5 | 再帰的自己改善が形式的安全性制約によって有界であることを、収束性に関する数学的証明を用いて検証する。 |  3  | D/V |

---

### 参考文献

* [MITRE ATLAS adversary tactics for ML](https://atlas.mitre.org/)
* [NIST AI Risk Management Framework 1.0, 2023](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
* [OWASP Top 10 for LLM Applications, 2025](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
* [Adversarial Training: A Survey — Zhao et al., 2024](https://arxiv.org/abs/2410.15042)
* [RobustBench adversarial-robustness benchmark](https://robustbench.github.io/)
* [Membership-Inference & Model-Inversion Risk Survey, 2025](https://www.sciencedirect.com/science/article/abs/pii/S0950705125003867)
* [PURIFIER: Confidence-Score Defense against MI Attacks — AAAI 2023](https://ojs.aaai.org/index.php/AAAI/article/view/26289)
* [Model-Inversion Attacks & Defenses Survey — AI Review, 2025](https://link.springer.com/article/10.1007/s10462-025-11248-0)
* [Comprehensive Defense Framework Against Model Extraction — IEEE TDSC 2024](https://doi.org/10.1109/TDSC.2023.3261327)
* [Fragile Model Watermarking Survey — 2025](https://www.sciencedirect.com/science/article/abs/pii/S0165168425002026)
* [Data Poisoning in Deep Learning: A Survey — Zhao et al., 2025](https://arxiv.org/abs/2503.22759)
* [BDetCLIP: Multimodal Prompting Backdoor Detection — Niu et al., 2024](https://arxiv.org/abs/2405.15269)

