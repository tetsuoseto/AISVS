# C13 פיקוח אנושי, אחריות וממשל

## מטרת בקרה

פרק זה מספק דרישות לשמירה על פיקוח אנושי ושרשרות אחריות ברורות במערכות בינה מלאכותית, תוך הבטחת הסבריות, שקיפות וניהול אתי לאורך כל מחזור חיי ה-AI.

---

## C13.1 מנגנוני ניתוק חירום ושליטה מחדש

ספק דרכי כיבוי או החזרה לאחור כאשר נצפה התנהגות לא בטוחה של מערכת ה-AI.

|   #    | תיאור                                                                            | רמה | תפקיד |
| :----: | -------------------------------------------------------------------------------- | :-: | :---: |
| 13.1.1 | אמת שקיים מנגנון השבתה ידני המאפשר לעצור מיידית את הפעלת מודל ה-AI והתפוקות שלו. |  1  |  D/V  |
| 13.1.2 | אמת כי בקרות ההחלפה נגישות רק לאנשים מורשים.                                     |  1  |   D   |
| 13.1.3 | אמת כי נהלי החזרה לאחור יכולים להשיב לגרסאות מודל קודמות או לפעולות במצב בטוח.   |  3  |  D/V  |
| 13.1.4 | לאמת שמנגנוני ההשתלטות נבדקים באופן קבוע.                                        |  3  |   V   |

---

## C13.2 נקודות ביקורת החלטה עם מעורבות אנושית

לדרוש אישורים אנושיים כאשר הסיכונים חורגים מספי הסיכון שהוגדרו מראש.

|   #    | תיאור                                                                                                 | רמה | תפקיד |
| :----: | ----------------------------------------------------------------------------------------------------- | :-: | :---: |
| 13.2.1 | וודא שהחלטות AI בסיכון גבוה דורשות אישור מפורש מבני אדם לפני ביצוע.                                   |  1  |  D/V  |
| 13.2.2 | ודא שספי הסיכון מוגדרים בבירור ומפעילים אוטומטית תהליכי סקירת אדם.                                    |  1  |   D   |
| 13.2.3 | אמת ששלבי החלטה רגישים לזמן כוללים נהלי גיבוי כאשר לא ניתן לקבל אישור אנושי בתוך מסגרות הזמן הנדרשות. |  2  |   D   |
| 13.2.4 | אמת כי נהלי ההסלמה מגדירים רמות סמכות ברורות עבור סוגי החלטות שונים או קטגוריות סיכון, אם רלוונטי.    |  3  |  D/V  |

---

## C13.3 שרשרת האחריות ואפשרות ביקורת

תעד פעולות של מפעיל והרשאות של המודל.

|   #    | תיאור                                                                                                  | רמה | תפקיד |
| :----: | ------------------------------------------------------------------------------------------------------ | :-: | :---: |
| 13.3.1 | וודא שכל החלטות מערכת ה-AI וההתערבויות האנושיות מתועדות עם חותמות זמן, זהויות משתמשים והנמקות להחלטות. |  1  |  D/V  |
| 13.3.2 | אמת כי יומני ביקורת אינם ניתנים לזיוף וכללו מנגנוני אימות שלמות.                                       |  2  |   D   |

---

## C13.4 טכניקות בינה מלאכותית מובנת (Explainable-AI)

חשיבות תכונות שטח, ניגודים-מציאות, והסברים מקומיים.

|   #    | תיאור                                                                                                                                | רמה | תפקיד |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------ | :-: | :---: |
| 13.4.1 | וודא שמערכות בינה מלאכותית מספקות הסברים בסיסיים להחלטותיהן בפורמט קריא לבני אדם.                                                    |  1  |  D/V  |
| 13.4.2 | אמת כי איכות ההסבר מאושרת דרך מחקרי הערכה אנושית ומדדים.                                                                             |  2  |   V   |
| 13.4.3 | אמת שציוני חשיבות התכונות או שיטות שיוך (SHAP, LIME, וכו') זמינים להחלטות קריטיות.                                                   |  3  |  D/V  |
| 13.4.4 | אמת שהסברים נגדיים (counterfactual explanations) מראים כיצד ניתן לשנות את הקלטים כדי לשנות תוצאות, אם הדבר ישים למקרה השימוש ולתחום. |  3  |   V   |

---

## C13.5 כרטיסי דגם וחשיפות שימוש

תחזק כרטיסי מודלים לשימוש מיועד, מדדי ביצועים, ושיקולים אתיים.

|   #    | תיאור                                                                                                                                      | רמה | תפקיד |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------------ | :-: | :---: |
| 13.5.1 | וודא שכרטיסי המודל מתעדים את מקרי השימוש המיועדים, המגבלות, ומצבי הכשל הידועים.                                                            |  1  |   D   |
| 13.5.2 | אשר כי מדדי הביצועים במקרים שונים ישימים נחשפים.                                                                                           |  1  |  D/V  |
| 13.5.3 | וידא כי השיקולים האתיים, הערכות הטיה, הערכות הוגנות, מאפייני נתוני האימון והמגבלות הידועות של נתוני האימון מתועדכנים ומעודכנים באופן קבוע. |  2  |   D   |
| 13.5.4 | אמתו כי כרטיסי המודל נשלטים בגרסאות ומתוחזקים לאורך מחזור חיי המודל עם מעקב אחר שינויים.                                                   |  2  |  D/V  |

---

## C13.6 כימות אי-ודאות

להפיץ ציוני ביטחון או מדדי אנטרופיה בתגובות.

|   #    | תיאור                                                                             | רמה | תפקיד |
| :----: | --------------------------------------------------------------------------------- | :-: | :---: |
| 13.6.1 | וודא שמערכות בינה מלאכותית מספקות ציוני ביטחון או מדדים של אי-ודאות עם הפלט שלהן. |  1  |   D   |
| 13.6.2 | אמת כי ספי אי-ודאות מפעילים בדיקה אנושית נוספת או דרכי החלטה חלופיות.             |  2  |  D/V  |
| 13.6.3 | אמתו ששיטות כימות אי-הוודאות מכוילות ומאומתות מול נתוני אמת קרקעית.               |  2  |   V   |
| 13.6.4 | ווידא כי הפצת חוסר הוודאות נשמרת לאורך תהליכי עבודה מרובי שלבים של בינה מלאכותית. |  3  |  D/V  |

---

## C13.7 דוחות שקיפות המיועדים למשתמשים

לספק גילויים תקופתיים על תקריות, הסטות, ושימוש בנתונים.

|   #    | תיאור                                                                                           | רמה | תפקיד |
| :----: | ----------------------------------------------------------------------------------------------- | :-: | :---: |
| 13.7.1 | וודא כי מדיניות השימוש בנתונים ופרקטיקות ניהול הסכמת המשתמשים מועברות בצורה ברורה לבעלי העניין. |  1  |  D/V  |
| 13.7.2 | וודא כי נערכות הערכות השפעה של בינה מלאכותית ותוצאותיהן כלולות בדיווח.                          |  2  |  D/V  |
| 13.7.3 | וודא שדוחות שקיפות שמתפרסמים באופן קבוע חושפים מקרי אירועי AI ומדדים תפעוליים בפירוט סביר.      |  2  |  D/V  |

### ביבליוגרפיה

* [EU Artificial Intelligence Act — Regulation (EU) 2024/1689 (Official Journal, 12 July 2024)](https://eur-lex.europa.eu/eli/reg/2024/1689/oj)
* [ISO/IEC 23894:2023 — Artificial Intelligence — Guidance on Risk Management](https://www.iso.org/standard/77304.html)
* [ISO/IEC 42001:2023 — AI Management Systems Requirements](https://www.iso.org/standard/81230.html)
* [NIST AI Risk Management Framework 1.0](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
* [NIST SP 800-53 Revision 5 — Security and Privacy Controls](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf)
* [A Unified Approach to Interpreting Model Predictions (SHAP, ICML 2017)](https://arxiv.org/abs/1705.07874)
* [Model Cards for Model Reporting (Mitchell et al., 2018)](https://arxiv.org/abs/1810.03993)
* [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning (Gal & Ghahramani, 2016)](https://arxiv.org/abs/1506.02142)
* [ISO/IEC 24029-2:2023 — Robustness of Neural Networks — Methodology for Formal Methods](https://www.iso.org/standard/79804.html)
* [IEEE 7001-2021 — Transparency of Autonomous Systems](https://standards.ieee.org/ieee/7001/6929/)
* [GDPR — Article 5 "Transparency Principle" (Regulation (EU) 2016/679)](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX%3A32016R0679)
* [Human Oversight under Article 14 of the EU AI Act (Fink, 2025)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5147196)

