# נספח A: מילון המונחים

>This מילון מונחים מקיף זה מספק הגדרות של מונחי בינה מלאכותית, למידת מכונה ואבטחה מרכזיים המשמשים בכל AISVS כדי להבטיח בהירות והבנה משותפת.

* דוגמת יריבה: דוגמה שנעשית בכוונה כדי לגרום למודל בינה מלאכותית לטעות, לעיתים על ידי הוספת הפרעות עדינות שאינן נראות לבני אדם.
  ​
* עמידות נגד התקפות אדברסיאליות – העמידות האדברסיאלית בבינה מלאכותית מתייחסת ליכולת של מודל לשמר את ביצועיו ולהתנגד להטעיה או לניצול על ידי קלטים מכוונים, מזיקים שנועדו לגרום לשגיאות.
  ​
* Agent – סוכני בינה מלאכותית הם מערכות תוכנה שמשתמשות בבינה מלאכותית כדי להשיג מטרות ולבצע משימות בשם המשתמשים. הם מציגים יכולת הסקה, תכנון וזיכרון ויש להם רמה של אוטונומיה לקבל החלטות, ללמוד ולהסתגל.
  ​
* בינה מלאכותית סוכנת: מערכות בינה מלאכותית המסוגלות לפעול בעצמאות מסוימת כדי להשיג מטרות, ולעיתים קרובות מחליטות ומבצעות פעולות ללא התערבות ישירה של האדם.
  ​
* בקרת גישה על פי תכונות (ABAC): פרדיגמה לבקרת גישה שבה ההחלטות על ההרשאה מבוססות על תכונות של המשתמש, המשאב, הפעולה והסביבה, ומוערכות בזמן השאילתה.
  ​
* התקפת שער אחורי: סוג של מתקפת זיהום נתונים שבה המודל מאומן להגיב בצורה מסוימת לטריגרים מסוימים, בעוד שהוא מתנהג באופן רגיל בכל שאר המקרים.
  ​
* הטיה: טעויות שיטתיות בפלטים של מודל בינה מלאכותית שיכולות להוביל לתוצאות לא הוגנות או מפלות עבור קבוצות מסוימות או בהקשרים מסוימים.
  ​
* ניצול הטיות: טכניקת תקיפה שמנצלת הטיות ידועות בדגמי בינה מלאכותית כדי להשפיע על פלטים או תוצאות.
  ​
* Cedar: שפת המדיניות של אמזון והמנוע שלה להרשאות מדויקות המשמשות ליישום ABAC עבור מערכות בינה מלאכותית.
  ​
* שרשרת חשיבה: טכניקה לשיפור ההסקה במודלים לשוניים על ידי יצירת שלבי הסקה ביניים לפני מתן תשובה סופית.
  ​
* מפסקי מעגלים: מנגנונים שעוצרים באופן אוטומטי את פעולת מערכות הבינה המלאכותית כאשר ספים מסוימים של סיכון עוברים.
  ​
* דליפת נתונים: חשיפה בלתי מכוונת של מידע רגיש באמצעות פלטי מודל של בינה מלאכותית או התנהגות המודל.
  ​
* זיהום נתונים: השחתה מכוונת של נתוני האימון כדי לפגוע באמינות המודל, לעיתים קרובות כדי להתקין דלתות אחוריות או להחליש את הביצועים.
  ​
* פרטיות דיפרנציאלית – פרטיות דיפרנציאלית היא מסגרת מדויקת מבחינה מתמטית לפרסום מידע סטטיסטי על מאגרי נתונים תוך שמירה על פרטיותם של נושאי הנתונים. היא מאפשרת לבעל הנתונים לשתף דפוסים מצטברים של הקבוצה, תוך הגבלת המידע שנחשף לגבי יחידים ספציפיים.
  ​
* הטמעות: ייצוגים וקטוריים צפופים של נתונים (טקסט, תמונות וכן הלאה) אשר תופסים משמעות סמנטית במרחב ממדים גבוה.
  ​
* הסבריות – הסבריות בבינה מלאכותית היא היכולת של מערכת בינה מלאכותית לספק סיבות שהאדם יכול להבין עבור החלטותיה ותחזיותיה, ולהציע תובנות על אופן פעולתה הפנימי.
  ​
* בינה מלאכותית שניתנת להסבר (XAI): מערכות בינה מלאכותית שנועדו לספק הסברים שניתן להבינם על ידי בני אדם להחלטותיהן ולהתנהגותן באמצעות מגוון טכניקות ומסגרות.
  ​
* למידה פדרלית: גישה של למידת מכונה שבה מודלים מאומנים על פני מספר מכשירים מבוזרים שמחזיקים דגימות נתונים מקומיות, ללא חילופי הנתונים עצמם.
  ​
* מסגרות בטיחות: הגבלות שמיושמות כדי למנוע ממערכות בינה מלאכותית לייצר פלטים מזיקים, מוטים או בלתי רצויים אחרים.
  ​
* הזיה – הזיה של בינה מלאכותית מתייחסת לתופעה שבה מודל בינה מלאכותית מייצר מידע שגוי או מטעה שאינו מבוסס על נתוני האימון שלו או על המציאות העובדתית.
  ​
* Human-in-the-Loop (HITL): מערכות המיועדות לדרוש פיקוח אנושי, אימות, או התערבות בנקודות החלטה קריטיות.
  ​
* תשתיות כקוד (IaC): ניהול והקמת תשתיות באמצעות קוד במקום תהליכים ידניים, המאפשרים סריקת אבטחה ופריסות עקביות.
  ​
* פריצה: טכניקות המשמשות לעקיפת מגבלות הבטיחות במערכות בינה מלאכותית, במיוחד במודלים לשוניים גדולים, כדי לייצר תוכן אסור.
  ​
* עקרון ההרשאות המינימליות: העקרון הביטחוני המעניק רק את זכויות הגישה המינימליות הנדרשות למשתמשים ולתהליכים.
  ​
* LIME (הסברים מקומיים שניתנים לפירוש שאינם תלויים בדגם): טכניקה להסביר את התחזיות של כל מסווג בלמידת מכונה על ידי קירובו באופן מקומי למודל שניתן לפירוש.
  ​
* התקפת הסקת השתייכות: התקפה שמטרתה לקבוע האם נתון מסוים שימש לאימון מודל למידת מכונה.
  ​
* MITRE ATLAS: נוף איומי יריבים למערכות בינה מלאכותית; מאגר ידע על טקטיקות וטכניקות יריביות כנגד מערכות בינה מלאכותית.
  ​
* כרטיס מודל – מסמך שמספק מידע תקני על הביצועים של מודל בינה מלאכותית, על המגבלות, על השימושים המתוכננים ועל ההיבטים האתיים, למען שקיפות ופיתוח אחראי של בינה מלאכותית.
  ​
* העתקת מודל: מתקפה שבה התוקף שוב ושוב שולח שאילתות למודל היעד כדי ליצור עותק תפקודי הדומה למודל ללא הרשאה.
  ​
* היפוך מודל: התקפה שנועדה לשחזר נתוני אימון באמצעות ניתוח פלטי המודל.
  ​
* ניהול מחזור החיים של מודל – ניהול מחזור החיים של מודל בינה מלאכותית הוא התהליך של פיקוח על כל שלבי קיומו של מודל בינה מלאכותית, כולל העיצוב, הפיתוח, הפריסה, הניטור, התחזוקה והפרישה הסופית, כדי להבטיח שהוא נשאר יעיל ומתואם עם היעדים.
  ​
* הרעלת מודל: הוספת חולשות או דלתות אחוריות ישירות למודל במהלך תהליך האימון.
  ​
* גניבת מודל: חילוץ עותק או הערכה של מודל קנייני באמצעות שאילתות חוזרות.
  ​
* מערכת מרובת-סוכנים: מערכת המורכבת ממספר סוכני בינה מלאכותית אשר פועלים זה עם זה, כאשר לכל אחד מהם יכולות ומטרות שונות.
  ​
* OPA (Open Policy Agent): מנוע מדיניות בקוד-פתוח שמאפשר אכיפה אחידה של המדיניות לאורך כל הסטאק.
  ​
* למידת מכונה לשמירה על פרטיות (PPML): טכניקות ושיטות לאימון ולהפעלת מודלים של למידת מכונה תוך הגנת פרטיות נתוני האימון.
  ​
* הזרקת פרומפט: התקפה שבה הוראות זדוניות מוטמעות בקלטים כדי לעקוף את התנהגות המודל המתוכננת.
  ​
* RAG (Retrieval-Augmented Generation): טכניקה המשפרת מודלים גדולים של שפה על ידי אחזור מידע רלוונטי ממקורות ידע חיצוניים לפני יצירת התגובה.
  ​
* Red-Teaming: התרגול של בדיקות מעשיות של מערכות בינה מלאכותית על ידי הדמיית מתקפות יריביות כדי לזהות פגיעויות.
  ​
* SBOM (רשימת מרכיבים של תוכנה): תיעוד פורמלי המכיל את הפרטים וקשרי שרשרת האספקה של מרכיבים שונים המשמשים בבניית תוכנה או מודלי בינה מלאכותית.
  ​
* SHAP (SHapley Additive exPlanations): גישה תורת המשחקים להסביר את הפלט של כל מודל למידת מכונה על ידי חישוב התרומה של כל תכונה לחיזוי.
  ​
* התקפת שרשרת האספקה: פגיעה במערכת על ידי מיקוד באלמנטים פחות מאובטחים בשרשרת האספקה שלה, כגון ספריות מטעם צד שלישי, סטים של נתונים, או מודלים מאומנים מראש.
  ​
* למידה טרנספר: טכניקה בה מודל שפותח עבור משימה אחת משמש כנקודת התחלה למודל עבור משימה שנייה.
  ​
* מסד נתונים וקטורי: מסד נתונים ייעודי לאחסון וקטורים בעלי ממדים גבוהים (הטמעות) וביצוע חיפושי דמיון מהירים.
  ​
* סריקת פגיעויות: כלים אוטומטיים שמזהים פגיעויות אבטחה ידועות במרכיבי תוכנה, כולל מסגרות בינה מלאכותית ותלויות תוכנה.
  ​
* הטבעת סימני מים: טכניקות להטמעת סמנים בלתי נראים בתוכן שנוצר על ידי בינה מלאכותית כדי לעקוב אחר מקורו או לזהות יצירה שנוצרה על ידי בינה מלאכותית.
  ​
* פגיעות אפס-יום: פגיעה שלא הייתה ידועה קודם שניתן לנצל אותה על ידי מתקפים לפני שמפתחי התוכנה יוצרים ומפיצים תיקון.

