# 附錄 A：術語表

>This 這本全面詞彙表提供在 AISVS 內使用的關鍵 AI、ML 與安全術語的定義，以確保清晰與共同理解。

* 對抗性樣本：一個經過刻意設計的輸入，旨在使 AI 模型犯錯，通常是透過加入對人類而言微小且難以察覺的擾動。
  ​
* 對抗性魯棒性 – AI 中的對抗性魯棒性指的是模型在維持其性能的同時，能抵抗被故意設計、具有惡意的輸入所欺騙或操控，從而造成錯誤的能力。
  ​
* 智能代理 – 人工智慧代理是利用人工智慧來代表用戶追求目標並完成任務的軟體系統。它們具備推理、規劃和記憶，並具備一定的自主性，能夠做出決策、學習與適應。
  ​
* 具自主性之人工智慧系統： 能在一定程度上實現自主運作以達成目標，通常在無需直接人類介入的情況下做出決策並採取行動。
  ​
* 基於屬性的存取控制（ABAC）：一種存取控制範式，其授權決策基於使用者、資源、操作與環境的屬性，於查詢時進行評估。
  ​
* 後門攻擊：是一種資料中毒攻擊，模型被訓練成在遇到特定觸發時以特定方式回應，而在其他情況下表現正常。
  ​
* 偏差：在 AI 模型輸出中的系統性錯誤，可能在某些群體或特定情境中導致不公平或歧視性的結果。
  ​
* 偏差利用：一種攻擊技術，利用 AI 模型中已知的偏差來操控輸出或結果。
  ​
* Cedar：亞馬遜的策略語言與引擎，用於實現 AI 系統中 ABAC 的細粒度權限。
  ​
* 思考鏈：透過在產出最終答案之前產生中間推理步驟，以提升語言模型的推理能力的一種技術。
  ​
* 斷路機制：在超過特定風險閾值時自動停止人工智慧系統的運作。
  ​
* 資料洩漏：透過 AI 模型的輸出或行為而造成的敏感資訊非預期曝光。
  ​
* 資料中毒：故意破壞訓練資料以危及模型完整性，常見於安裝後門或降低效能。
  ​
* 差分隱私 – 差分隱私是一個在數學上嚴謹的框架，用於在保護個人資料主體隱私的同時，釋放關於資料集的統計資訊。它使資料持有者能夠分享群體的聚合模式，同時限制洩漏有關特定個人的資訊。
  ​
* 嵌入：數據（文本、圖像等）的密集向量表示，在高維空間中捕捉語義。
  ​
* 可解釋性 – AI 中的可解釋性是指 AI 系統能提供人類可理解的其決策與預測之理由，並對其內部運作提供見解。
  ​
* 可解釋的人工智慧（XAI）：AI 系統透過各種技術與框架，設計以提供人類可理解的對其決策與行為的解釋。
  ​
* 聯邦學習：一種機器學習方法，在多個去中心化裝置上訓練模型，這些裝置持有本地數據樣本，且不直接交換資料。
  ​
* 護欄：為防止 AI 系統產生有害、帶有偏見或其他不良輸出而實施的約束。
  ​
* 幻覺 – 人工智慧幻覺指的是一種現象，即人工智慧模型產生的資訊不基於其訓練資料或客觀現實，且為錯誤或具誤導性的資訊。
  ​
* 人機在迴圈中（HITL）：設計成在關鍵決策點需要人類監督、驗證或干預的系統。
  ​
* 以程式碼管理與配置基礎設施（Infrastructure as Code，IaC）：透過程式碼取代手動流程，實現安全掃描與一致的部署。
  ​
* 越獄：用於繞過 AI 系統中的安全護欄的技術，特別是在大型語言模型中，以產生禁止的內容。
  ​
* 最小特權原則：僅授予使用者與進程所需的最低存取權限。
  ​
* LIME（局部可解釋的模型無關解釋）: 一種透過在局部區域以可解釋的模型近似任意機器學習分類器的預測來解釋其預測的技術。
  ​
* 成員資格推斷攻擊：一種旨在判斷某個特定資料點是否被用來訓練機器學習模型的攻擊。
  ​
* MITRE ATLAS：面向人工智慧系統的對抗性威脅景觀；一個關於人工智慧系統的對抗性戰術與技術的知識庫。
  ​
* 模型卡片 – 模型卡片是一份文件，提供有關人工智慧模型的性能、局限、預期用途及倫理考量的標準化資訊，以促進透明度與負責任的人工智慧發展。
  ​
* 模型提取：一種攻擊，攻擊者反覆對目標模型提出查詢，以在未經授權的情況下建立一個在功能上與目標模型相似的副本。
  ​
* 模型反演：一種試圖透過分析模型輸出來重建訓練資料的攻擊。
  ​
* AI 模型生命周期管理是指監督 AI 模型整個存在期間各階段的過程，包括其設計、開發、部署、監控、維護，以及最終的退役，以確保它保持有效並與目標保持一致。
  ​
* 模型中毒：在訓練過程中直接向模型引入漏洞或後門。
  ​
* 模型竊取/盜用：透過反覆查詢提取專有模型的副本或近似值。
  ​
* 多智能體系統：由多個相互作用的 AI 代理組成的系統，每個代理都可能具有不同的能力和目標。
  ​
* OPA（Open Policy Agent）：一個開源的策略引擎，可在整個技術棧中實現統一的策略執行。
  ​
* 隱私保護機器學習（PPML）：在保護訓練數據隱私的同時，用於訓練和部署 ML 模型的技術與方法。
  ​
* 提示注入：一種在輸入中嵌入惡意指令以改變模型原本預期行為的攻擊。
  ​
* RAG（檢索增強生成）是一種透過在生成回應之前，從外部知識來源檢索相關資訊以提升大型語言模型效能的技術。
  ​
* 紅隊測試：透過模擬對抗性攻擊來識別漏洞，對 AI 系統進行主動測試的做法。
  ​
* SBOM（軟體材料清單）：一份正式的記錄，包含在建置軟體或人工智慧模型時所使用的各種元件的細節與供應鏈關係。
  ​
* SHAP（SHapley Additive exPlanations）：一種基於博弈論的方法，用於解釋任何機器學習模型的輸出，透過計算每個特徵對預測的貢獻。
  ​
* 供應鏈攻擊：透過攻擊供應鏈中較不安全的元素來入侵系統，例如第三方函式庫、資料集或預訓練模型。
  ​
* 遷移學習：在一個任務上開發的模型被重新用作另一個任務的起點。
  ​
* 向量資料庫：一種專門設計用於儲存高維向量（嵌入）並執行高效相似性搜尋的資料庫。
  ​
* 漏洞掃描：用於識別軟體元件中已知安全漏洞的自動化工具，包括 AI 框架與依賴性。
  ​
* 水印技術：在 AI 產生的內容中嵌入難以察覺的標記，以追蹤其來源或偵測 AI 的生成。
  ​
* 零日漏洞： 攻擊者在開發者建立並部署修補程式之前可以利用的先前未知漏洞。

