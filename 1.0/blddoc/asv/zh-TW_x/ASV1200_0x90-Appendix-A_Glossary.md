# 附錄 A：詞彙表

這本全面的詞彙表提供在AISVS中使用的主要人工智慧、機器學習和安全術語的定義，以確保清晰和共同理解。

* 對抗樣本：一種故意設計的輸入，旨在使人工智慧模型產生錯誤，通常是透過加入人類難以察覺的微妙擾動。
  ​
* 對抗魯棒性 – 在人工智慧中，對抗魯棒性指的是模型維持其性能並抵抗故意設計來誘發錯誤的惡意輸入欺騙或操縱的能力。
  ​
* 代理 – AI代理是使用人工智慧代表使用者追求目標並完成任務的軟體系統。它們具備推理、規劃和記憶能力，並擁有一定程度的自主權以進行決策、學習和適應。
  ​
* 代理型 AI：能以某種程度的自主性運作以達成目標的 AI 系統，通常在沒有直接人類介入的情況下做出決策並採取行動。
  ​
* 基於屬性的存取控制（ABAC）：一種存取控制範式，其中授權決策基於使用者、資源、行動及環境的屬性，並在查詢時進行評估。
  ​
* 後門攻擊：一種資料中毒攻擊，模型被訓練在遇到特定觸發條件時以特定方式回應，而在其他情況下則正常行為。
  ​
* 偏見：人工智慧模型輸出中的系統性錯誤，可能導致對某些群體或特定情境下的不公平或歧視性結果。
  ​
* 偏差利用：一種利用人工智慧模型中已知偏差來操控輸出或結果的攻擊技術。
  ​
* Cedar：亞馬遜用於實施 AI 系統的基於屬性的存取控制（ABAC）中，具有細粒度權限的策略語言及引擎。
  ​
* 思維鏈：一種透過在產生最終答案之前先生成中間推理步驟來提升語言模型推理能力的技術。
  ​
* 斷路器：當超過特定風險閾值時，自動停止人工智慧系統運作的機制。
  ​
* 資料洩漏：透過 AI 模型輸出或行為意外暴露敏感資訊。
  ​
* 資料中毒：故意破壞訓練資料以損害模型完整性，通常用於安裝後門或降低效能。
  ​
* 差分隱私 – 差分隱私是一種在數學上嚴謹的框架，用於在保護個別資料主體隱私的同時，發布關於資料集的統計信息。它使資料持有者能夠分享群體的整體模式，同時限制關於特定個體的資訊洩漏。
  ​
* 嵌入向量：數據（文本、圖像等）的稠密向量表示，能在高維空間中捕捉語義意義。
  ​
* 可解釋性 – AI中的可解釋性是指AI系統能夠提供人類可理解的決策和預測原因，從而揭示其內部運作機制的能力。
  ​
* 可解釋人工智慧（XAI）：設計用以通過各種技術和框架，為其決策及行為提供人類可理解解釋的人工智慧系統。
  ​
* 聯邦學習：一種機器學習方法，模型在多個持有本地數據樣本的分散式設備上進行訓練，而不交換數據本身。
  ​
* 防護措施：為防止人工智慧系統產生有害、偏見或其他不良輸出而實施的限制。
  ​
* 幻覺——AI幻覺是指AI模型生成與其訓練數據或事實真相不符的錯誤或誤導性信息的現象。
  ​
* 人機互動環（HITL）：設計成在關鍵決策點需要人類監督、驗證或介入的系統。
  ​
* 基礎設施即代碼（IaC）：通過代碼而非手動流程來管理和配置基礎設施，從而實現安全掃描和一致的部署。
  ​
* 越獄：用於繞過人工智慧系統中安全防護措施的技術，特別是在大型語言模型中，以生成被禁止的內容。
  ​
* 最小權限原則：一項安全原則，旨在僅授予用戶和程序必要的最低存取權限。
  ​
* LIME（局部可解釋模型無關解釋）：一種通過使用可解釋模型對任意機器學習分類器的預測進行局部近似來解釋其預測結果的技術。
  ​
* 成員推斷攻擊：一種旨在判斷特定數據點是否被用於訓練機器學習模型的攻擊。
  ​
* MITRE ATLAS：針對人工智慧系統的對抗威脅全景；一個針對AI系統的對抗戰術與技術的知識庫。
  ​
* 模型卡 – 模型卡是一份文件，提供關於人工智慧模型的效能、限制、預期用途以及倫理考量的標準化資訊，以促進透明度及負責任的人工智慧開發。
  ​
* 模型提取：一種攻擊方法，攻擊者反覆查詢目標模型，以在未經授權的情況下創建功能相似的複製品。
  ​
* 模型反演：一種通過分析模型輸出來嘗試重建訓練數據的攻擊。
  ​
* 模型生命周期管理 – AI 模型生命周期管理是監督 AI 模型整個存在階段的過程，包括其設計、開發、部署、監控、維護及最終淘汰，以確保其持續有效並符合目標。
  ​
* 模型中毒：在訓練過程中直接將漏洞或後門引入模型。
  ​
* 模型竊取／盜用：透過反覆查詢提取專有模型的副本或近似版本。
  ​
* 多代理系統：由多個相互作用的人工智慧代理組成的系統，每個代理可能具有不同的能力和目標。
  ​
* OPA（開放策略代理）：一個開源的策略引擎，能夠在整個堆疊中實現統一的策略執行。
  ​
* 隱私保護機器學習（PPML）：在保護訓練數據隱私的同時，訓練和部署機器學習模型的技術和方法。
  ​
* 提示注入：一種攻擊方式，通過在輸入中嵌入惡意指令來覆蓋模型的預期行為。
  ​
* RAG（檢索增強生成）：一種技術，通過在生成回應之前從外部知識來源檢索相關信息，以提升大型語言模型的能力。
  ​
* 紅隊演練：通過模擬對抗性攻擊來主動測試人工智慧系統，以識別其漏洞的實踐。
  ​
* SBOM（軟體物料清單）：一份正式記錄，包含用於構建軟體或人工智慧模型的各種元件的詳細資訊及供應鏈關係。
  ​
* SHAP（Shapley 加法解釋法）：一種博弈理論方法，用於通過計算每個特徵對預測結果的貢獻來解釋任何機器學習模型的輸出。
  ​
* 供應鏈攻擊：透過針對其供應鏈中安全性較低的元素，如第三方函式庫、資料集或預訓練模型來攻擊系統。
  ​
* 遷移學習：一種技術，其中為一項任務開發的模型被重用作為第二項任務模型的起點。
  ​
* 向量資料庫：一種專門設計用來儲存高維向量（嵌入）並執行高效相似度搜尋的資料庫。
  ​
* 漏洞掃描：自動化工具，用於識別軟件組件中的已知安全漏洞，包括人工智慧框架和相依性。
  ​
* 浮水印技術：在 AI 生成的內容中嵌入無法察覺的標記，以追蹤其來源或識別 AI 生成。
  ​
* 零日漏洞：一種先前未知的漏洞，攻擊者可以在開發人員製作並部署修補程式之前利用該漏洞。

