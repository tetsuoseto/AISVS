# 附錄 A：術語表

這份綜合詞彙表提供了在AISVS中使用的主要人工智慧、機器學習及安全術語的定義，以確保清晰和共通的理解。

* 對抗樣本：一種故意設計的輸入，旨在使人工智慧模型犯錯，通常通過添加人類難以察覺的微妙擾動來實現。
  ​
* 對抗魯棒性 – AI 中的對抗魯棒性指的是模型維持其性能的能力，並抵抗由蓄意設計以製造錯誤的惡意輸入所導致的欺騙或操縱。
  ​
* 代理人 – AI代理人是使用人工智慧的軟體系統，代表使用者追求目標和完成任務。它們展現推理、規劃和記憶能力，並具有一定程度的自主性以做出決策、學習和適應。
  ​
* 能動型人工智慧：能以某種程度的自主性運作，以達成目標的人工智慧系統，通常在無需直接人類介入的情況下做出決策並採取行動。
  ​
* 基於屬性的存取控制（ABAC）：一種存取控制範式，授權決策基於用戶、資源、操作和環境的屬性，並在查詢時進行評估。
  ​
* 後門攻擊：一種資料毒害攻擊，模型被訓練成在遇到特定觸發條件時做出特定反應，而在其他情況下則保持正常行為。
  ​
* 偏差：人工智慧模型輸出中的系統性錯誤，可能導致某些群體或特定情境下的不公平或歧視性結果。
  ​
* 偏見利用：一種利用人工智慧模型中已知偏見來操控輸出或結果的攻擊技術。
  ​
* Cedar：亞馬遜用於實現 AI 系統層次存取控制（ABAC）的精細權限政策語言和引擎。
  ​
* 思維鏈：一種通過在生成最終答案之前產生中間推理步驟來提升語言模型推理能力的技術。
  ​
* 斷路器：當超出特定風險閾值時，自動停止 AI 系統操作的機制。
  ​
* 資料外洩：透過人工智慧模型的輸出或行為意外揭露敏感資訊。
  ​
* 資料中毒：故意破壞訓練資料以損害模型完整性，通常是為了安裝後門或降低效能。
  ​
* 差分隱私 – 差分隱私是一個數學上嚴謹的框架，用於在保護個別數據主體隱私的同時發布有關數據集的統計信息。它允許數據持有者共享群體的整體模式，同時限制關於特定個體洩露的信息。
  ​
* 嵌入向量：資料（文字、圖片等）的稠密向量表示，能在高維空間中捕捉語義意涵。
  ​
* 可解釋性 – AI 的可解釋性是指 AI 系統能夠提供人類可理解的決策和預測原因，從而揭示其內部運作機制的能力。
  ​
* 可解釋人工智慧（XAI）：旨在通過各種技術和框架，為其決策和行為提供人類可理解解釋的人工智慧系統。
  ​
* 聯邦學習：一種機器學習方法，在多個分散設備上訓練模型，這些設備持有本地數據樣本，但不交換數據本身。
  ​
* 防護欄：為防止人工智慧系統產生有害、偏見或其他不良輸出而實施的限制措施。
  ​
* 幻覺——AI幻覺指的是AI模型產生不正確或誤導性資訊的現象，這些資訊並非基於其訓練數據或事實現實。
  ​
* 人類在回路中 (HITL)：設計為在關鍵決策點需要人類監督、驗證或介入的系統。
  ​
* 基礎設施即程式碼（IaC）：透過程式碼而非手動流程管理和配置基礎設施，從而實現安全掃描和一致性部署。
  ​
* 越獄：用於繞過人工智慧系統中安全防護措施的技術，特別是在大型語言模型中，以生成禁止內容。
  ​
* 最小權限原則：僅授予使用者和程序所需的最低限度存取權限的安全原則。
  ​
* LIME（局部可解釋模型無關解釋法）：一種通過在局部範圍內以可解釋模型近似任何機器學習分類器的預測結果的技術。
  ​
* 成員推論攻擊：一種旨在判斷特定數據點是否被用於訓練機器學習模型的攻擊。
  ​
* MITRE ATLAS：人工智能系統的對抗性威脅全貌；一個關於針對人工智能系統的對抗性策略與技術的知識庫。
  ​
* 模型卡 – 模型卡是一份文件，提供關於人工智慧模型的表現、限制、預期用途及倫理考量的標準化資訊，以促進透明度和負責任的人工智慧開發。
  ​
* 模型提取：一種攻擊方式，攻擊者反覆查詢目標模型，以未經授權的方式創建一個功能上相似的複本。
  ​
* 模型反演：一種通過分析模型輸出來嘗試重建訓練數據的攻擊。
  ​
* 模型生命周期管理 – AI模型生命周期管理是指監督AI模型存在的所有階段的過程，包括其設計、開發、部署、監控、維護及最終退役，以確保模型持續有效且符合目標。
  ​
* 模型中毒：在訓練過程中直接向模型引入漏洞或後門。
  ​
* 模型盜用/竊取：透過反覆查詢來提取專有模型的複製品或近似版本。
  ​
* 多代理系統：由多個互動的人工智慧代理組成的系統，每個代理可能具有不同的能力和目標。
  ​
* OPA（Open Policy Agent）：一個開源的政策引擎，可實現跨整個堆疊的統一政策執行。
  ​
* 隱私保護機器學習（PPML）：用於在保護訓練數據隱私的同時訓練和部署機器學習模型的技術與方法。
  ​
* 提示注入：一種攻擊方式，即在輸入中嵌入惡意指令，以覆蓋模型的預期行為。
  ​
* RAG（檢索增強生成）：一種技術，通過在生成回應之前從外部知識來源檢索相關信息來增強大型語言模型的能力。
  ​
* 紅隊演練：通過模擬對抗性攻擊來積極測試人工智能系統以識別漏洞的實踐。
  ​
* SBOM（軟體材料清單）：一份正式記錄，包含用於構建軟體或 AI 模型的各種組件的詳細資訊及其供應鏈關係。
  ​
* SHAP（Shapley 加法解釋）：一種博弈論方法，用於解釋任何機器學習模型的輸出，通過計算每個特徵對預測的貢獻。
  ​
* 供應鏈攻擊：透過針對供應鏈中較不安全的元素，例如第三方函式庫、資料集或預訓練模型，來入侵系統。
  ​
* 遷移學習：一種技術，將為一個任務開發的模型作為第二個任務模型的起點加以重用。
  ​
* 向量資料庫：專門設計用於儲存高維度向量（嵌入向量）並執行高效相似度搜尋的資料庫。
  ​
* 弱點掃描：自動化工具，用於識別軟體組件中的已知安全漏洞，包括 AI 框架和相依性。
  ​
* 水印技術：在 AI 生成內容中嵌入難以察覺的標記，以追蹤其來源或檢測 AI 生成痕跡的方法。
  ​
* 零日漏洞：指攻擊者在開發人員創建並部署補丁之前可以利用的先前未知漏洞。

