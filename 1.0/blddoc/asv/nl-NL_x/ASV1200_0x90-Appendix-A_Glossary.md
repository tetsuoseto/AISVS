# Bijlage A: Woordenlijst

>This uitgebreide glossarium biedt definities van belangrijke AI-, ML- en beveiligingstermen die door AISVS worden gebruikt om duidelijkheid en een gemeenschappelijk begrip te waarborgen.

* Adversarial Example: Een invoer die bewust is vervaardigd om een kunstmatige intelligentie-model een fout te laten maken, vaak door subtiele perturbaties toe te voegen die voor mensen onmerkbaar zijn.
  ​
* Adversariële robuustheid – Adversariële robuustheid in AI verwijst naar het vermogen van een model om zijn prestaties te behouden en bestand te blijven tegen misleiding of manipulatie door opzettelijk vervaardigde, kwaadaardige invoer die is ontworpen om fouten te veroorzaken.
  ​
* Agent – AI-agenten zijn softwaresystemen die AI gebruiken om doelen na te streven en taken namens gebruikers uit te voeren. Ze tonen redenering, planning en geheugen en hebben een mate van autonomie om beslissingen te nemen, te leren en zich aan te passen.
  ​
* Agentische AI: AI-systemen die met een zekere mate van autonomie kunnen opereren om doelen te bereiken, vaak beslissingen nemen en acties uitvoeren zonder directe menselijke tussenkomst.
  ​
* Attribuutgebaseerde toegangscontrole (ABAC): Een toegangscontroleparadigma waarbij autorisatiebeslissingen gebaseerd zijn op attributen van de gebruiker, de bron, de actie en de omgeving, geëvalueerd op het moment van de query.
  ​
* Backdoor-aanval: Een type data-poisoning-aanval waarbij het model zo getraind wordt dat het op een specifieke manier reageert op bepaalde triggers, terwijl het verder normaal gedrag vertoont.
  ​
* Vertekening: systematische fouten in de uitvoer van AI-modellen die kunnen leiden tot oneerlijke of discriminerende uitkomsten voor bepaalde groepen of in specifieke contexten.
  ​
* Bias-exploitatie: Een aanvalstechniek die gebruikmaakt van bekende biases in AI-modellen om de uitvoer of uitkomsten te manipuleren.
  ​
* Cedar: de beleidsstaal en engine van Amazon voor fijnmazige machtigingen die worden gebruikt bij de implementatie van ABAC voor AI-systemen.
  ​
* Redeneringsketen: Een techniek om het redeneren bij taalmodellen te verbeteren door tussenliggende redeneringsstappen te genereren voordat een definitief antwoord wordt geproduceerd.
  ​
* Circuitbrekers: Mechanismen die automatisch de werking van AI-systemen stilleggen wanneer specifieke risicogrensen worden overschreden.
  ​
* Datalek: Onbedoelde blootstelling van gevoelige informatie via de uitvoer van AI-modellen of hun gedrag.
  ​
* Gegevensvergiftiging: de opzettelijke corruptie van trainingsgegevens om de integriteit van het model te ondermijnen, vaak om achterdeuren te installeren of de prestaties te verslechteren.
  ​
* Differentiële privacy – Differentiële privacy is een wiskundig rigoureus kader voor het vrijgeven van statistische informatie over datasets, terwijl de privacy van individuele gegevenssubjecten wordt beschermd. Het stelt een gegevenshouder in staat om aggregatiepatronen van de groep te delen, terwijl informatie die over specifieke individuen wordt gelekt, beperkt blijft.
  ​
* Embeddings: dichte vectorrepresentaties van data (tekst, afbeeldingen, enz.) die semantische betekenis vastleggen in een hoog-dimensionale ruimte.
  ​
* Uitlegbaarheid – Uitlegbaarheid in AI is het vermogen van een AI-systeem om voor mensen begrijpelijke redenen te geven voor zijn beslissingen en voorspellingen, waardoor inzicht wordt geboden in de interne werking ervan.
  ​
* Verklaarbare AI (XAI): AI-systemen die zijn ontworpen om voor mensen begrijpelijke verklaringen te bieden voor hun beslissingen en gedrag via diverse technieken en raamwerken.
  ​
* Federated Learning: Een machine learning-benadering waarbij modellen worden getraind over meerdere gedecentraliseerde apparaten die lokale gegevensvoorbeelden bevatten, zonder de gegevens zelf uit te wisselen.
  ​
* Beveiligingskaders: Beperkingen die zijn geïmplementeerd om te voorkomen dat AI-systemen schadelijke, bevooroordeelde of anderszins ongewenste uitvoer produceren.
  ​
* Hallucinatie – Een AI-hallucinatie verwijst naar een fenomeen waarbij een AI-model onjuiste of misleidende informatie genereert die niet gebaseerd is op zijn trainingsgegevens of op de feitelijke werkelijkheid.
  ​
* Mens-in-de-lus (HITL): Systemen die ontworpen zijn om menselijk toezicht, verificatie of tussenkomst op cruciale beslissingspunten te vereisen.
  ​
* Infrastructuur als Code (IaC): Infrastructuur beheren en provisioneren via code in plaats van handmatige processen, waardoor beveiligingsscans mogelijk zijn en consistente implementaties plaatsvinden.
  ​
* Jailbreak: Technieken die worden gebruikt om veiligheidskaders in AI-systemen te omzeilen, met name in grote taalmodellen, om verboden inhoud te produceren.
  ​
* Principe van minimale rechten: Het beveiligingsprincipe waarbij aan gebruikers en processen uitsluitend de minimaal noodzakelijke toegangsrechten worden verleend.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): Een techniek om de voorspellingen van elke machine-learning-classifier uit te leggen door deze lokaal te benaderen met een interpreteerbaar model.
  ​
* Lidmaatschapsinferentie-aanval: Een aanval die erop gericht is vast te stellen of een bepaald datapunt is gebruikt om een machine-learning-model te trainen.
  ​
* MITRE ATLAS: Adversarial Threat Landscape for Artificial-Intelligence Systems; een kennisbank van adversarial tactieken en technieken tegen AI-systemen.
  ​
* Modelkaart – Een modelkaart is een document dat gestandaardiseerde informatie biedt over de prestaties van een AI-model, beperkingen, beoogde toepassingen en ethische overwegingen om transparantie en verantwoorde AI-ontwikkeling te bevorderen.
  ​
* Modelextractie: Een aanval waarbij een aanvaller herhaaldelijk een doelmodel opvraagt om een functionele vergelijkbare kopie te creëren zonder toestemming.
  ​
* Modelinversie: Een aanval die probeert trainingsgegevens te reconstrueren door modeluitgangen te analyseren.
  ​
* Modellevenscyclusbeheer – AI-modellevenscyclusbeheer is het proces waarbij toezicht wordt gehouden op alle fasen van het bestaan van een AI-model, waaronder het ontwerp, de ontwikkeling, de inzet, bewaking, onderhoud en uiteindelijk het buiten gebruik stellen, om ervoor te zorgen dat het effectief blijft en in lijn met de doelstellingen.
  ​
* Modelvergiftiging: Kwetsbaarheden of backdoors rechtstreeks in een model introduceren tijdens het trainingsproces.
  ​
* Modeldiefstal: Het extraheren van een kopie of een benadering van een proprietair model door middel van herhaalde query's.
  ​
* Multi-agentensysteem: Een systeem dat bestaat uit meerdere AI-agenten die met elkaar interageren, elk met mogelijk verschillende capaciteiten en doelen.
  ​
* OPA (Open Policy Agent): een open-source beleidsmotor die uniforme beleidshandhaving over de gehele stack mogelijk maakt.
  ​
* Privacybeschermende machine learning (PPML): Technieken en methoden om ML-modellen te trainen en in te zetten terwijl de privacy van de trainingsgegevens wordt beschermd.
  ​
* Promptinjectie: Een aanval waarbij kwaadaardige instructies in invoer worden ingebed om het beoogde gedrag van een model te wijzigen.
  ​
* RAG (Retrieval-Augmented Generation): Een techniek die grote taalmodellen verbetert door relevante informatie uit externe kennisbronnen op te halen voordat een antwoord wordt gegenereerd.
  ​
* Red-Teaming: De praktijk van actief testen van AI-systemen door het simuleren van adversariële aanvallen om kwetsbaarheden te identificeren.
  ​
* SBOM (Software Bill of Materials): Een formeel register met de details en de toeleveringsketenrelaties van diverse componenten die worden gebruikt bij het bouwen van software of AI-modellen.
  ​
* SHAP (SHapley Additive exPlanations): Een speltheoretische benadering om de uitvoer van elk machine learning-model uit te leggen door de bijdrage van elk kenmerk aan de voorspelling te berekenen.
  ​
* Leveringsketen-aanval: Een aanval op een systeem door zich te richten op minder-beveiligde elementen in de toeleveringsketen, zoals bibliotheken van derden, datasets of voorgetrainde modellen.
  ​
* Transferleren: Een techniek waarbij een model dat voor één taak is ontwikkeld, wordt hergebruikt als uitgangspunt voor een model voor een tweede taak.
  ​
* Vector Database: Een gespecialiseerde database die is ontworpen om hoogdimensionale vectoren (embeddings) op te slaan en efficiënte similariteitszoekopdrachten uit te voeren.
  ​
* Kwetsbaarhedenscan: Geautomatiseerde hulpmiddelen die bekende beveiligingskwetsbaarheden in softwarecomponenten identificeren, waaronder kunstmatige intelligentie-frameworks en afhankelijkheden.
  ​
* Watermerken: Technieken om onmerkbare markeringen in door AI gegenereerde inhoud te plaatsen om de herkomst ervan te volgen of AI-generatie te detecteren.
  ​
* Zero-day kwetsbaarheid: Een voorheen onbekende kwetsbaarheid die aanvallers kunnen misbruiken voordat ontwikkelaars een patch hebben gemaakt en uitgerold.

