# Bijlage A: Woordenlijst

Deze uitgebreide woordenlijst geeft definities van belangrijke AI-, ML- en beveiligingstermen die door de hele AISVS worden gebruikt om duidelijkheid en een gemeenschappelijk begrip te waarborgen.

* Adversariële Voorbeeld: Een invoer die opzettelijk is gemaakt om een AI-model een fout te laten maken, vaak door subtiele verstoringen toe te voegen die voor mensen onwaarneembaar zijn.
  ​
* Aanvallingsbestendigheid – Aanvallingsbestendigheid in AI verwijst naar het vermogen van een model om zijn prestaties te behouden en weerstand te bieden tegen opzettelijk vervaardigde, kwaadaardige invoer die is ontworpen om fouten te veroorzaken.
  ​
* Agent – AI-agenten zijn softwaresystemen die AI gebruiken om doelen na te streven en taken uit te voeren namens gebruikers. Ze tonen redeneervermogen, planning en geheugen en hebben een mate van autonomie om beslissingen te nemen, te leren en zich aan te passen.
  ​
* Agentische AI: AI-systemen die met een zekere mate van autonomie kunnen functioneren om doelen te bereiken, waarbij ze vaak beslissingen nemen en acties uitvoeren zonder directe menselijke tussenkomst.
  ​
* Attribute-Based Access Control (ABAC): Een toegangscontroleparadigma waarbij autorisatiebeslissingen worden genomen op basis van attributen van de gebruiker, resource, actie en omgeving, die tijdens de querytijd worden geëvalueerd.
  ​
* Backdoor-aanval: Een type datavervuilingsaanval waarbij het model wordt getraind om op een specifieke manier te reageren op bepaalde triggers, terwijl het zich anders normaal gedraagt.
  ​
* Bias: Systematische fouten in AI-modeluitvoer die kunnen leiden tot oneerlijke of discriminerende uitkomsten voor bepaalde groepen of in specifieke contexten.
  ​
* Bias Exploitatie: Een aanvalstechniek die gebruikmaakt van bekende vooroordelen in AI-modellen om outputs of uitkomsten te manipuleren.
  ​
* Cedar: Amazons beleids- en uitvoeringssysteem voor fijnmazige toestemmingen, gebruikt bij de implementatie van ABAC voor AI-systemen.
  ​
* Chain of Thought: Een techniek om het redeneren in taalmodellen te verbeteren door tussentijdse redeneringsstappen te genereren voordat een definitief antwoord wordt gegeven.
  ​
* Circuit Breakers: Mechanismen die automatisch de werking van AI-systemen stoppen wanneer specifieke risicodrempels worden overschreden.
  ​
* Gegevenslek: Onbedoelde blootstelling van gevoelige informatie via AI-modeluitvoer of gedrag.
  ​
* Datavergiftiging: Het opzettelijk corrumperen van trainingsgegevens om de integriteit van het model aan te tasten, vaak om achterdeurtjes te installeren of de prestaties te verslechteren.
  ​
* Differentiële privacy – Differentiële privacy is een mathematisch rigoureus kader voor het vrijgeven van statistische informatie over datasets, terwijl de privacy van individuele gegevenssubjecten wordt beschermd. Het stelt een gegevenshouder in staat om geaggregeerde patronen van de groep te delen, terwijl de informatie die over specifieke individuen wordt gelekt, wordt beperkt.
  ​
* Embeddings: Dichte vectorrepresentaties van gegevens (tekst, afbeeldingen, enz.) die semantische betekenis vastleggen in een hoog-dimensionale ruimte.
  ​
* Uitlegbaarheid – Uitlegbaarheid in AI is het vermogen van een AI-systeem om voor mensen begrijpelijke redenen te geven voor zijn beslissingen en voorspellingen, waarbij inzicht wordt geboden in de interne werking ervan.
  ​
* Explainable AI (XAI): AI-systemen die ontworpen zijn om begrijpelijke uitleg te geven aan mensen over hun beslissingen en gedragingen via verschillende technieken en raamwerken.
  ​
* Federated Learning: Een machine learning-benadering waarbij modellen worden getraind op meerdere gedecentraliseerde apparaten die lokale datasets bevatten, zonder dat de data zelf wordt uitgewisseld.
  ​
* Beschermingsmaatregelen: Beperkingen geïmplementeerd om te voorkomen dat AI-systemen schadelijke, bevooroordeelde of anderszins ongewenste output produceren.
  ​
* Hallucinatie – Een AI-hallucinatie verwijst naar een fenomeen waarbij een AI-model onjuiste of misleidende informatie genereert die niet is gebaseerd op zijn trainingsgegevens of feitelijke realiteit.
  ​
* Human-in-the-Loop (HITL): Systemen die zijn ontworpen om menselijke controle, verificatie of interventie te vereisen op cruciale beslissingsmomenten.
  ​
* Infrastructure als Code (IaC): Beheren en leveren van infrastructuur via code in plaats van handmatige processen, wat beveiligingsscans en consistente implementaties mogelijk maakt.
  ​
* Jailbreak: Technieken die worden gebruikt om veiligheidsmaatregelen in AI-systemen, met name in grote taalmodellen, te omzeilen om verboden inhoud te produceren.
  ​
* Minimale rechten: Het beveiligingsprincipe waarbij slechts de minimaal noodzakelijke toegangsrechten worden toegekend aan gebruikers en processen.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): Een techniek om de voorspellingen van elke machine learning-classificator uit te leggen door deze lokaal te benaderen met een interpreteerbaar model.
  ​
* Lidmaatschapsinference-aanval: een aanval die tot doel heeft te bepalen of een specifiek gegevenspunt is gebruikt om een machine learning-model te trainen.
  ​
* MITRE ATLAS: Adversarial Threat Landscape voor kunstmatige-intelligentiesystemen; een kennisbank van adversariële tactieken en technieken tegen AI-systemen.
  ​
* Modelkaart – Een modelkaart is een document dat gestandaardiseerde informatie biedt over de prestaties, beperkingen, beoogde toepassingen en ethische overwegingen van een AI-model om transparantie en verantwoordelijke AI-ontwikkeling te bevorderen.
  ​
* Model Extractie: Een aanval waarbij een aanvaller herhaaldelijk een doelmodel bevraagt om zonder toestemming een functioneel vergelijkbare kopie te maken.
  ​
* Modelinversie: Een aanval die probeert trainingsdata te reconstrueren door modeluitvoer te analyseren.
  ​
* Modellevenscyclusbeheer – AI Modellevenscyclusbeheer is het proces van het toezicht houden op alle fasen van het bestaan van een AI-model, inclusief het ontwerp, de ontwikkeling, de implementatie, het monitoren, het onderhoud en het uiteindelijke buiten gebruik stellen, om ervoor te zorgen dat het effectief blijft en in lijn is met de doelstellingen.
  ​
* Modelvergiftiging: Het direct introduceren van kwetsbaarheden of achterdeurtjes in een model tijdens het trainingsproces.
  ​
* Modeldiefstal: Het extraheren van een kopie of benadering van een propriëtair model door herhaalde queries.
  ​
* Multi-agent System: Een systeem samengesteld uit meerdere interactieve AI-agenten, elk met mogelijk verschillende capaciteiten en doelen.
  ​
* OPA (Open Policy Agent): Een open-source beleids-engine die uniforme handhaving van beleid over de hele stack mogelijk maakt.
  ​
* Privacy-behoudende Machine Learning (PPML): Technieken en methoden om ML-modellen te trainen en implementeren terwijl de privacy van de trainingsgegevens wordt beschermd.
  ​
* Promptinjectie: Een aanval waarbij kwaadaardige instructies in invoer worden ingebed om het bedoelde gedrag van een model te overschrijven.
  ​
* RAG (Retrieval-Augmented Generation): Een techniek die grote taalmodellen verbetert door relevante informatie op te halen uit externe kennisbronnen voordat een antwoord wordt gegenereerd.
  ​
* Red-Teaming: De praktijk van het actief testen van AI-systemen door het simuleren van adversariële aanvallen om kwetsbaarheden te identificeren.
  ​
* SBOM (Software Bill of Materials): Een formeel verslag met de details en toeleveringsketenrelaties van verschillende componenten die worden gebruikt bij het bouwen van software of AI-modellen.
  ​
* SHAP (SHapley Additive exPlanations): Een speltheoretische benadering om de output van elk machine learning-model uit te leggen door de bijdrage van elke eigenschap aan de voorspelling te berekenen.
  ​
* Supply Chain-aanval: Een systeem compromitteren door zich te richten op minder beveiligde elementen in de toeleveringsketen, zoals bibliotheken van derden, datasets of vooraf getrainde modellen.
  ​
* Transfer Learning: Een techniek waarbij een model dat is ontwikkeld voor één taak wordt hergebruikt als uitgangspunt voor een model voor een tweede taak.
  ​
* Vector Database: Een gespecialiseerde database ontworpen om hoog-dimensionale vectoren (embeddings) op te slaan en efficiënte gelijkeniszoektochten uit te voeren.
  ​
* Kwetsbaarheidsscanning: Geautomatiseerde tools die bekende beveiligingskwetsbaarheden in softwarecomponenten identificeren, inclusief AI-frameworks en afhankelijkheden.
  ​
* Watermarking: Technieken om onzichtbare markers in AI-gegenereerde inhoud in te bedden om de oorsprong te traceren of AI-generatie te detecteren.
  ​
* Zero-Day kwetsbaarheid: Een eerder onbekende kwetsbaarheid die aanvallers kunnen benutten voordat ontwikkelaars een patch maken en implementeren.

