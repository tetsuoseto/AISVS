# परिशिष्ट A: शब्दावली

>This विस्तृत शब्दावली एआई, एमएल, और सुरक्षा शब्दों की परिभाषाएं प्रदान करती है जो AISVS के सभी भागों में प्रयुक्त होती हैं ताकि स्पष्टता और सामान्य समझ सुनिश्चित हो सके।

* Adversarial Example: एक इनपुट जान-बूझकर ऐसा बनाया गया है ताकि एआई मॉडल गलतियाँ करे, अक्सर मनुष्यों द्वारा महसूस न किए जा सकने वाले सूक्ष्म परिवर्तनों को जोड़कर।
  ​
* विरोधी आक्रमण से स्थिरता – एआई में विरोधी आक्रमण से स्थिरता उस मॉडल की ऐसी क्षमता है जो अपने प्रदर्शन को बनाए रख सके और जानबूझकर बनाए गए दुर्भावनापूर्ण इनपुट के कारण धोखा दिए जाने या हेरफेर होने से बच सके, ताकि त्रुटियाँ न हों।
  ​
* एजेंट – एआई एजेंट सॉफ्टवेयर सिस्टम होते हैं जो उपयोगकर्ताओं की ओर से लक्ष्य प्राप्त करने और उनके लिए कार्य पूरे करने के लिए एआई का उपयोग करते हैं। वे तर्क-वितर्क, योजना बनाना, और स्मृति दिखाते हैं और निर्णय लेने, सीखने, और अनुकूलित होने के लिए एक स्वायत्तता का स्तर रखते हैं।
  ​
* एजेंटिक एआई: ऐसी एआई प्रणालियाँ जो लक्ष्य प्राप्त करने के लिए कुछ हद तक स्वायत्तता के साथ संचालित हो सकती हैं, अक्सर निर्णय लेती हैं और सीधे मानव हस्तक्षेप के बिना कार्रवाइयाँ करती हैं।
  ​
* गुण-आधारित पहुँच नियंत्रण (ABAC): एक पहुँच नियंत्रण पद्धति है जिसमें अनुमति के निर्णय उपयोगकर्ता, संसाधन, क्रिया और पर्यावरण के गुणों के आधार पर लिए जाते हैं, जिन्हें अनुरोध के समय मूल्यांकित किया जाता है।
  ​
* बैकडोर अटैक: डेटा पॉइज़निंग अटैक का एक प्रकार है जिसमें मॉडल को कुछ विशिष्ट ट्रिगर पर खास तरीके से प्रतिक्रिया करने के लिए प्रशिक्षित किया जाता है, जबकि अन्य स्थितियों में सामान्य व्यवहार करता है।
  ​
* पूर्वाग्रह: AI मॉडल के आउटपुट में व्यवस्थित त्रुटियाँ जो कुछ समूहों के लिए या विशिष्ट संदर्भों में अनुचित या भेदभावपूर्ण परिणाम दे सकती हैं।
  ​
* पूर्वाग्रह शोषण: एक आक्रमण तकनीक जो एआई मॉडलों में ज्ञात पूर्वाग्रहों का लाभ उठाकर आउटपुट या परिणामों को हेरफेर करती है।
  ​
* सीडर: अमेज़ॉन की नीति-भाषा और इंजन जो AI प्रणालियों के ABAC को लागू करने के लिए सूक्ष्म-स्तरीय अनुमतियाँ प्रदान करते हैं।
  ​
* विचार-श्रृंखला: भाषा मॉडलों में तर्क को बेहतर बनाने के लिए एक तकनीक है, जिसमें अंतिम उत्तर देने से पहले मध्यवर्ती तर्क के चरण बनाए जाते हैं।
  ​
* सर्किट ब्रेकर: ऐसे तंत्र जो विशिष्ट जोखिम सीमा पार होने पर AI प्रणाली के संचालन को स्वचालित रूप से रोक देते हैं।
  ​
* डेटा लीक: संवेदनशील जानकारी के अनजाने खुलासे एआई मॉडल आउटपुट या व्यवहार के माध्यम से।
  ​
* डेटा पॉइज़निंग: प्रशिक्षण डेटा का जानबूझकर दूषण ताकि मॉडल की अखंडता को समझौता किया जा सके, अक्सर बैकडोर स्थापित करने या प्रदर्शन को घटाने के लिए।
  ​
* डिफरेंशियल गोपनीयता – डिफरेंशियल गोपनीयता एक गणितीय रूप से सख्त ढांचा है जो डेटासेट्स के बारे में सांख्यिकीय जानकारी जारी करने के लिए बनायी गयी है, जबकि व्यक्तिगत डेटा विषयों की गोपनीयता की सुरक्षा करता है। यह डेटा धारक को समूह के समेकित पैटर्न साझा करने में सक्षम बनाता है, जबकि विशिष्ट व्यक्तियों के बारे में लीक होने वाली जानकारी को सीमित करता है।
  ​
* एंबेडिंग्स: डेटा (टेक्स्ट, छवियाँ, आदि) के घनत्वपूर्ण वेक्टर प्रतिनिधित्व हैं जो उच्च-आयामी स्पेस में सार्थक अर्थ को पकड़ते हैं।
  ​
* व्याख्यात्मकता – AI में स्पष्टता वह क्षमता है जिसमें एक AI प्रणाली अपने निर्णयों और भविष्यवाणियों के लिए मानव-समझ में आने योग्य कारण प्रदान कर सके, जो इसके आंतरिक कार्यप्रणाली पर प्रकाश डालते हैं।
  ​
* व्याख्यायोग्य एआई (XAI): ऐसी एआई प्रणालियाँ जो अपने निर्णयों और आचरण के लिए मानव-समझ में आने योग्य स्पष्टीकरण प्रदान करने के लिए विभिन्न तकनीकों और फ्रेमवर्क के साथ डिज़ाइन की गई हैं।
  ​
* फेडरेटेड लर्निंग: एक मशीन लर्निंग-आधारित दृष्टिकोण है जिसमें मॉडलों को स्थानीय डेटा नमूनों को रखते हुए कई विकेंद्रीकृत उपकरणों पर प्रशिक्षित किया जाता है, डेटा को एक-दूसरे के साथ साझा किए बिना।
  ​
* गार्डरेल्स: AI प्रणालियों को हानिकारक, पक्षपाती, या अन्यथा अवांछित परिणाम पैदा करने से रोकने के लिए लागू की गई बाधाएं।
  ​
* भ्रम – कृत्रिम बुद्धिमत्ता के भ्रम से जुड़ी एक ऐसी घटना है जिसमें एक एआई मॉडल गलत या भ्रामक जानकारी उत्पन्न करता है जो इसके प्रशिक्षण डेटा या वास्तविकता पर आधारित नहीं है।
  ​
* Human-in-the-Loop (HITL): ऐसी प्रणालियाँ जो निर्णायक निर्णय बिंदुओं पर मानवीय निगरानी, सत्यापन या हस्तक्षेप की आवश्यकता के लिए डिज़ाइन की गई हैं।
  ​
* कोड के रूप में अवसंरचना (IaC): मैनुअल प्रक्रियाओं के बजाय कोड के माध्यम से अवसंरचना का प्रबंधन और प्रावधान करना, सुरक्षा स्कैनिंग सक्षम बनाना और डिप्लॉयमेंट को सुसंगत बनाना।
  ​
* जेलब्रेक: कृत्रिम बुद्धिमत्ता (एआई) प्रणालियों में सुरक्षा गार्डरेल्स को दरकिनार करने के लिए इस्तेमाल की जाने वाली तकनीकें, विशेषकर बड़े भाषा मॉडल में, ताकि निषिद्ध सामग्री उत्पन्न हो सके।
  ​
* न्यूनतम विशेषाधिकार: उपयोगकर्ताओं और प्रक्रियाओं के लिए केवल आवश्यक न्यूनतम पहुँच अधिकार प्रदान करने वाला सुरक्षा सिद्धांत।
  ​
* LIME (स्थानीय इंटरप्रिटेबल मॉडल-एग्नॉस्टिक एक्सप्लेनेशंस): किसी भी मशीन लर्निंग क्लासिफायर की भविष्यवाणियों की व्याख्या करने के लिए एक तकनीक है जिसे स्थानीय स्तर पर एक इंटरप्रिटेबल मॉडल के साथ आकलन करके किया जाता है।
  ​
* Membership Inference Attack: एक ऐसा हमला है जिसका उद्देश्य यह निर्धारित करना है कि क्या किसी विशिष्ट डेटा बिंदु को मशीन लर्निंग मॉडल को प्रशिक्षित करने के लिए इस्तेमाल किया गया था।
  ​
* MITRE ATLAS: आर्टिफिशियल-इंटेलिजेंस प्रणालियों के लिए विरोधी खतरे का परिदृश्य; एआई प्रणालियों के विरुद्ध विरोधी रणनीतियाँ और तकनीकों का एक ज्ञान-आधार।
  ​
* मॉडल कार्ड – A मॉडल कार्ड एक दस्तावेज़ है जो एआई मॉडल के प्रदर्शन, सीमाओं, उद्देश्यित उपयोगों, और नैतिक विचारों के बारे में मानकीकृत जानकारी प्रदान करता है ताकि पारदर्शिता और जिम्मेदार एआई विकास को बढ़ावा दिया जा सके।
  ​
* मॉडल एक्सट्रैक्शन: एक हमला जिसमें एक विरोधी बार-बार लक्षित मॉडल से प्रश्न पूछकर बिना अनुमति के एक कार्यात्मक रूप से समान प्रतिलिपि बनाता है।
  ​
* Model Inversion: एक हमला जो मॉडल के आउटपुट का विश्लेषण करके प्रशिक्षण डेटा को पुनः प्राप्त करने का प्रयास करता है।
  ​
* मॉडल जीवनचक्र प्रबंधन – एआई मॉडल जीवनचक्र प्रबंधन एक ऐसी प्रक्रिया है जो एआई मॉडल के अस्तित्व के सभी चरणों की देखरेख करती है, जिसमें इसका डिज़ाइन, विकास, तैनाती, निगरानी, रखरखाव और अंततः सेवानिवृत्ति शामिल है, ताकि यह सुनिश्चित किया जा सके कि यह प्रभावी बना रहे और उद्देश्यों के अनुरूप रहे।
  ​
* मॉडल विषाक्तिकरण: प्रशिक्षण प्रक्रिया के दौरान सीधे एक मॉडल में कमजोरियाँ या बैकडोर डालना।
  ​
* मॉडल चोरी: बार-बार क्वेरी करके एक स्वामित्व वाला मॉडल की कॉपी या उसका अनुमान निकालना।
  ​
* बहु-एजेंट सिस्टम: एक ऐसी प्रणाली जिसमें कई परस्पर क्रिया करने वाले एआई एजेंट होते हैं, प्रत्येक के पास संभवतः भिन्न क्षमताएं और लक्ष्य होते हैं।
  ​
* OPA (Open Policy Agent): एक ओपन-सोर्स नीति इंजन है जो पूरे स्टैक में एकीकृत नीति प्रवर्तन सक्षम बनाता है।
  ​
* Privacy-Preserving Machine Learning (PPML): प्रशिक्षण डेटा की गोपनीयता की सुरक्षा करते हुए ML मॉडलों को प्रशिक्षित और तैनात करने के लिए तकनीकें और तरीके।
  ​
* प्रॉम्प्ट इंजेक्शन: ऐसे हमले में दुर्भावनापूर्ण निर्देश इनपुटों में छिपाए जाते हैं ताकि मॉडल के निर्धारित व्यवहार को ओवरराइड किया जा सके।
  ​
* RAG (Retrieval-Augmented Generation): एक ऐसी तकनीक जो बड़े भाषा मॉडल को प्रतिक्रिया बनाने से पहले बाहरी ज्ञान स्रोतों से प्रासंगिक जानकारी प्राप्त करके उनकी क्षमता बढ़ाती है।
  ​
* रेड-टीमिंग: एआई प्रणालियों का सक्रिय रूप से परीक्षण करके विरोधी आक्रमणों का अनुकरण कर कमजोरियाँ पहचानने की पद्धति।
  ​
* SBOM (Software Bill of Materials): सॉफ्टवेयर या एआई मॉडल बनाने में प्रयुक्त विभिन्न घटकों के विवरण और उनकी आपूर्ति श्रृंखला के संबंधों का एक औपचारिक रिकॉर्ड है।
  ​
* SHAP (SHapley Additive exPlanations): किसी भी मशीन लर्निंग मॉडल के आउटपुट की व्याख्या करने के लिए खेल-थ्योरी-आधारित एक दृष्टिकोण है, जो पूर्वानुमान में प्रत्येक फ़ीचर के योगदान की गणना करके उनका योगदान दिखाता है।
  ​
* आपूर्ति श्रृंखला हमला: ऐसी प्रणाली को उसके आपूर्ति श्रृंखला के कम-सुरक्षित तत्वों को लक्षित कर समझौता करना, जैसे तृतीय-पक्ष लाइब्रेरीज़, डेटासेट, या पूर्व-प्रशिक्षित मॉडल।
  ​
* ट्रांसफर लर्निंग: एक ऐसी तकनीक जिसमें एक कार्य के लिए विकसित किया गया मॉडल दूसरे कार्य के लिए शुरुआती बिंदु के रूप में फिर से उपयोग किया जाता है।
  ​
* वेक्टर डेटाबेस: उच्च-आयामी वेक्टर (एम्बेडिंग्स) स्टोर करने और कुशल समानता खोजों को संचालित करने के लिए डिज़ाइन किया गया एक विशिष्ट डेटाबेस है।
  ​
* कमज़ोरी स्कैनिंग: ऐसे स्वचालित उपकरण जो सॉफ्टवेयर घटकों में ज्ञात सुरक्षा कमजोरियाँ पहचानते हैं, जिनमें एआई फ्रेमवर्क और निर्भरताएँ शामिल हैं।
  ​
* वॉटरमार्किंग: एआई-जनित सामग्री में अदृश्य निशान डालने की तकनीकें ताकि इसके स्रोत का पता लगाया जा सके या एआई-जनित होने की पहचान की जा सके।
  ​
* जीरो-डे खामी: एक ऐसी पहले से अज्ञात खामी जिसे हमलावर डेवलपर्स पैच बनाने और लागू करने से पहले शोषण कर सकते हैं।

