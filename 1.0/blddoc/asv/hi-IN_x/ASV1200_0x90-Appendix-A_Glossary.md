# परिशिष्ट A: शब्दावली

यह व्यापक शब्दकोश AISVS में उपयोग किए जाने वाले प्रमुख AI, ML, और सुरक्षा शब्दों की परिभाषाएं प्रदान करता है ताकि स्पष्टता और सामान्य समझ सुनिश्चित की जा सके।

* विरोधी उदाहरण: एक इनपुट जो जानबूझकर इस प्रकार बनाया गया है कि यह एक AI मॉडल को गलती करने पर मजबूर कर दे, अक्सर इसमें ऐसे सूक्ष्म परिवर्तन शामिल होते हैं जो मनुष्यों को दिखाई नहीं देते।
  ​
* प्रतिद्वंदी मजबूती – AI में प्रतिद्वंदी मजबूती का मतलब है किसी मॉडल की वह क्षमता जिससे वह अपनी प्रदर्शन क्षमता बनाए रख सके और जानबूझकर तैयार किए गए, हानिकारक इनपुट द्वारा धोखा खाने या बदल दिए जाने से बच सके, जो त्रुटियाँ उत्पन्न करने के लिए बनाए जाते हैं।
  ​
* एजेंट – एआई एजेंट ऐसे सॉफ़्टवेयर सिस्टम होते हैं जो उपयोगकर्ताओं की ओर से लक्ष्यों को प्राप्त करने और कार्यों को पूरा करने के लिए एआई का उपयोग करते हैं। ये तर्क, योजना, और स्मृति दिखाते हैं और निर्णय लेने, सीखने, और अनुकूलन करने के लिए एक स्तर की स्वायत्तता रखते हैं।
  ​
* एजेंटिक AI: AI सिस्टम जो कुछ हद तक स्वायत्तता के साथ काम कर सकते हैं ताकि लक्ष्य प्राप्त कर सकें, अक्सर बिना सीधे मानव हस्तक्षेप के निर्णय लेते और कार्य करते हैं।
  ​
* एट्रिब्यूट-आधारित एक्सेस कंट्रोल (ABAC): एक एक्सेस कंट्रोल मॉडल जहां प्राधिकरण निर्णय उपयोगकर्ता, संसाधन, क्रिया, और परिवेश के एट्रिब्यूट्स के आधार पर किए जाते हैं, जिन्हें प्रश्न समय पर मूल्यांकित किया जाता है।
  ​
* बैकडोर हमला: डेटा पॉइज़निंग हमलों का एक प्रकार जहां मॉडल को विशेष ट्रिगर्स के लिए एक विशिष्ट तरीके से प्रतिक्रिया करने के लिए प्रशिक्षित किया जाता है जबकि अन्यथा सामान्य रूप से कार्य करता है।
  ​
* पक्षपात: AI मॉडल आउटपुट में व्यवस्थित त्रुटियाँ जो कुछ समूहों या विशिष्ट संदर्भों में अनुचित या भेदभावपूर्ण परिणामों को जन्म दे सकती हैं।
  ​
* पक्षपात शोषण: एक हमले की तकनीक जो AI मॉडलों में ज्ञात पक्षपात का लाभ उठाकर आउटपुट या परिणामों को नियंत्रित करती है।
  ​
* Cedar: अमेज़न की नीति भाषा और इंजन जो AI सिस्टम के लिए ABAC लागू करने में सूक्ष्म-ग्रैन्युलर अनुमति के लिए इस्तेमाल होती है।
  ​
* चेन ऑफ थॉट: भाषा मॉडल में तर्क सुधारने की एक तकनीक है जो अंतिम उत्तर देने से पहले मध्यवर्ती तर्क कदम उत्पन्न करती है।
  ​
* सर्किट ब्रेकर्स: ऐसे तंत्र जो स्वचालित रूप से AI सिस्टम के ऑपरेशंस को तब रोक देते हैं जब विशिष्ट जोखिम सीमाएं पार कर ली जाती हैं।
  ​
* डेटा लीक: एआई मॉडल के आउटपुट या व्यवहार के माध्यम से संवेदनशील जानकारी का अनचाहा प्रकटीकरण।
  ​
* डेटा पॉइज़निंग: प्रशिक्षण डेटा को जानबूझकर भ्रष्ट करना ताकि मॉडल की अखंडता को नुकसान पहुंचाया जा सके, अक्सर बैकडोर स्थापित करने या प्रदर्शन को ख़राब करने के लिए।
  ​
* डिफरेंशियल प्राइवेसी – डिफरेंशियल प्राइवेसी एक गणितीय रूप से कठोर ढांचा है जो डाटासेट्स के बारे में सांख्यिकीय जानकारी जारी करते समय व्यक्तिगत डेटा विषयों की गोपनीयता की रक्षा करता है। यह डेटा धारक को समूह के समष्टिगत पैटर्न साझा करने में सक्षम बनाता है जबकि विशिष्ट व्यक्तियों के बारे में लीकी जाने वाली जानकारी को सीमित करता है।
  ​
* एम्बेडिंग्स: डेटा (टेक्स्ट, छवियां, आदि) के घने वेक्टर प्रतिनिधित्व जो उच्च-आयामी स्थान में अर्थपूर्ण अर्थ को कैप्चर करते हैं।
  ​
* व्याख्यात्मकता – एआई में व्याख्यात्मकता एक एआई सिस्टम की वह क्षमता है जो इसके निर्णयों और पूर्वानुमानों के लिए मानव-सुलभ कारण प्रदान करती है, जिससे इसके आंतरिक कामकाज की जानकारी मिलती है।
  ​
* व्याख्यायोग्य एआई (XAI): एआई सिस्टम जो अपनी निर्णय और व्यवहारों के लिए मानव-सुलभ व्याख्याएं प्रदान करने के लिए विभिन्न तकनीकों और ढांचों के माध्यम से डिज़ाइन किए जाते हैं।
  ​
* फेडरेटेड लर्निंग: एक मशीन लर्निंग दृष्टिकोण जहाँ मॉडल कई विकेन्द्रीकृत उपकरणों पर स्थानीय डेटा नमूनों को रखते हुए प्रशिक्षित किए जाते हैं, बिना डेटा को स्वयं आदान-प्रदान किए।
  ​
* गार्डरेल्स: प्रतिबंध जो एआई सिस्टम को हानिकारक, पक्षपाती, या अन्यथा अवांछित परिणाम उत्पन्न करने से रोकने के लिए लागू किए जाते हैं।
  ​
* हेलुसिनेशन – एक AI हेलुसिनेशन उस घटना को संदर्भित करता है जहां एक AI मॉडल त्रुटिपूर्ण या भ्रमित करने वाली जानकारी उत्पन्न करता है जो इसके प्रशिक्षण डेटा या वास्तविक तथ्यात्मक आधार पर नहीं होती।
  ​
* ह्यूमन-इन-द-लूप (HITL): ऐसे सिस्टम जिन्हें महत्वपूर्ण निर्णय बिंदुओं पर मानव निरीक्षण, सत्यापन, या हस्तक्षेप की आवश्यकता होती है।
  ​
* इन्फ्रास्ट्रक्चर एज कोड (IaC): मैनुअल प्रक्रियाओं के बजाय कोड के माध्यम से इन्फ्रास्ट्रक्चर का प्रबंधन और प्रावधान करना, जिससे सुरक्षा स्कैनिंग और सुसंगत परिनियोजन सक्षम होते हैं।
  ​
* जेलब्रेक: AI सिस्टमों में सुरक्षा गार्डरेल्स को बायपास करने के लिए इस्तेमाल की जाने वाली तकनीकें, विशेष रूप से बड़े भाषा मॉडल में, ताकि प्रतिबंधित सामग्री उत्पन्न की जा सके।
  ​
* न्यूनतम विशेषाधिकार: उपयोगकर्ताओं और प्रक्रियाओं के लिए केवल आवश्यक न्यूनतम पहुँच अधिकार प्रदान करने का सुरक्षा सिद्धांत।
  ​
* LIME (लोकल इंटरप्रेटेबल मॉडल-एग्नोस्टिक एक्सप्लनेशंस): किसी भी मशीन लर्निंग क्लासिफायर के प्रेडिक्शंस को समझाने की एक तकनीक है जो इसे लोकली एक इंटरप्रेटेबल मॉडल के साथ लगभग दर्शाती है।
  ​
* सदस्यता पूर्वानुमान हमला: एक हमला जिसका उद्देश्य यह निर्धारित करना होता है कि कोई विशिष्ट डेटा पॉइंट मशीन लर्निंग मॉडल को प्रशिक्षित करने में उपयोग किया गया था या नहीं।
  ​
* MITRE ATLAS: कृत्रिम-बुद्धिमत्ता प्रणालियों के लिए विरोधी खतरा परिदृश्य; AI प्रणालियों के खिलाफ विरोधी रणनीतियों और तकनीकों का ज्ञान आधार।
  ​
* मॉडल कार्ड – एक मॉडल कार्ड एक दस्तावेज़ होता है जो AI मॉडल के प्रदर्शन, सीमाओं, इच्छित उपयोगों, और नैतिक विचारों के बारे में मानकीकृत जानकारी प्रदान करता है ताकि पारदर्शिता और जिम्मेदार AI विकास को बढ़ावा दिया जा सके।
  ​
* मॉडल एक्सट्रैक्शन: एक हमला जहाँ एक विरोधी बार-बार एक लक्षित मॉडल से प्रश्न करता है ताकि बिना अनुमति के एक कार्यात्मक रूप से समान प्रति बनाई जा सके।
  ​
* मॉडल इनवर्जन: एक हमला जो मॉडल के आउटपुट का विश्लेषण करके प्रशिक्षण डेटा को पुनर्निर्मित करने का प्रयास करता है।
  ​
* मॉडल लाइफसाइकल प्रबंधन – एआई मॉडल लाइफसाइकल प्रबंधन एक एआई मॉडल के अस्तित्व के सभी चरणों की निगरानी करने की प्रक्रिया है, जिसमें इसके डिज़ाइन, विकास, तैनाती, निगरानी, रखरखाव और अंततः सेवानिवृत्ति शामिल हैं, ताकि यह सुनिश्चित किया जा सके कि यह प्रभावी रहता है और उद्देश्यों के साथ संरेखित रहता है।
  ​
* मॉडल पॉइसनिंग: प्रशिक्षण प्रक्रिया के दौरान सीधे मॉडल में कमजोरियां या बैकडोर्स पेश करना।
  ​
* मॉडल चोरी/चोरी: बार-बार क्वेरी के माध्यम से एक स्वामित्व वाली मॉडल की कॉपी या अनुमान निकालना।
  ​
* मल्टी-एजेंट सिस्टम: एक सिस्टम जिसमें कई इंटरैक्टिंग AI एजेंट शामिल होते हैं, जिनमें से प्रत्येक की संभावित रूप से अलग क्षमताएं और लक्ष्य होते हैं।
  ​
* OPA (ओपन पॉलिसी एजेंट): एक ओपन-सोर्स पॉलिसी इंजन जो स्टैक के पार एकीकृत पॉलिसी प्रवर्तन सक्षम करता है।
  ​
* प्राइवेसी-प्रेजर्विंग मशीन लर्निंग (PPML): प्रशिक्षण डेटा की गोपनीयता की रक्षा करते हुए ML मॉडल को प्रशिक्षित और तैनात करने की तकनीकें और विधियाँ।
  ​
* प्रॉम्प्ट इंजेक्शन: एक ऐसा हमला जहां मॉडल के इच्छित व्यवहार को ओवरराइड करने के लिए इनपुट्स में दुर्भावनापूर्ण निर्देश एम्बेड किए जाते हैं।
  ​
* RAG (रिट्रीवल-ऑगमेंटेड जनरेशन): एक तकनीक जो बाहरी ज्ञान स्रोतों से प्रासंगिक जानकारी पुनः प्राप्त करके बड़े भाषा मॉडलों को उत्तर उत्पन्न करने से पहले बेहतर बनाती है।
  ​
* रेड-टीमिंग: कमजोरियों की पहचान करने के लिए प्रतिद्वंदी हमलों का अनुकरण करके एआई सिस्टम्स का सक्रिय परीक्षण करने की प्रक्रिया।
  ​
* SBOM (सॉफ्टवेयर बिल ऑफ मटेरियल्स): एक औपचारिक रिकॉर्ड जिसमें सॉफ़्टवेयर या AI मॉडल बनाने में उपयोग किए गए विभिन्न घटकों का विवरण और सप्लाई चेन संबंध शामिल होते हैं।
  ​
* SHAP (शैप्ली एडिटिव एक्सप्लैनेशंस): किसी भी मशीन लर्निंग मॉडल के आउटपुट की व्याख्या करने के लिए एक गेम थ्योरी आधारित दृष्टिकोण, जो प्रत्येक फीचर के योगदान की गणना करके भविष्यवाणी को समझाता है।
  ​
* सप्लाई चेन हमला: किसी प्रणाली को उसके सप्लाई चेन के कम सुरक्षा वाले तत्वों को निशाना बनाकर समझौता करना, जैसे कि तृतीय-पक्ष पुस्तकालय, डेटासेट, या पूर्व-प्रशिक्षित मॉडल।
  ​
* ट्रांसफर लर्निंग: एक तकनीक जिसमें एक कार्य के लिए विकसित मॉडल को दूसरे कार्य के लिए एक मॉडल के प्रारंभिक बिंदु के रूप में पुन: उपयोग किया जाता है।
  ​
* वेक्तर डेटाबेस: एक विशिष्ट डेटाबेस जो उच्च-आयामी वेक्तरों (एम्बेडिंग्स) को संग्रहीत करने और कुशल समानता खोजों को निष्पादित करने के लिए डिज़ाइन किया गया है।
  ​
* सुरक्षा कमजोरियों का स्कैनिंग: स्वचालित उपकरण जो ज्ञात सुरक्षा कमजोरियों की पहचान करते हैं सॉफ़्टवेयर घटकों में, जिसमें AI फ्रेमवर्क और डिपेंडेंसी शामिल हैं।
  ​
* वॉटरमार्किंग: AI-जनित सामग्री में उसके स्रोत का पता लगाने या AI जनरेशन का पता लगाने के लिए अभिज्ञात चिह्नों को एम्बेड करने की तकनीकें।
  ​
* ज़ीरो-डे भेद्यता: एक पूर्व में अज्ञात भेद्यता जिसे हमलावर उस समय एक्सप्लॉइट कर सकते हैं जब तक डेवलपर्स एक पैच तैयार और डिप्लॉय नहीं कर देते।

