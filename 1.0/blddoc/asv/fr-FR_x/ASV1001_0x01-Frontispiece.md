# Frontispice

## À propos de la norme

La norme de vérification de la sécurité de l’intelligence artificielle (AISVS) est un catalogue communautaire des exigences de sécurité que les data scientists, ingénieurs MLOps, architectes logiciels, développeurs, testeurs, professionnels de la sécurité, fournisseurs d’outils, régulateurs et consommateurs peuvent utiliser pour concevoir, construire, tester et vérifier des systèmes et applications d’IA fiables. Elle fournit un langage commun pour spécifier les contrôles de sécurité tout au long du cycle de vie de l’IA — depuis la collecte des données et le développement des modèles jusqu’au déploiement et à la surveillance continue — afin que les organisations puissent mesurer et améliorer la résilience, la confidentialité et la sécurité de leurs solutions d’IA.

## Droits d'auteur et licence

Version 0.1 (Premier brouillon public - Travail en cours), 2025  

![license](../images/license.png)

Droits d'auteur © 2025 Le projet AISVS.  

Publié sous la[Creative Commons Attribution‑ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).  
Pour toute réutilisation ou distribution, vous devez clairement communiquer les conditions de licence de ce travail aux autres.

## Chefs de projet

|            |                         |
| ---------- | ----------------------- |
| Jim Manico | Aras “Russ” Memisyazici |

## Contributeurs et réviseurs

|                                    |                             |
| ---------------------------------- | --------------------------- |
| https://github.com/ottosulin       | https://github.com/mbhatt1  |
| https://github.com/vineethsai      | https://github.com/cciprofm |
| https://github.com/deepakrpandey12 |                             |

---

AISVS est une toute nouvelle norme créée spécifiquement pour répondre aux défis uniques de sécurité des systèmes d’intelligence artificielle. Bien qu’elle s’inspire des meilleures pratiques de sécurité plus générales, chaque exigence de l’AISVS a été développée de zéro pour refléter le paysage des menaces liées à l’IA et aider les organisations à construire des solutions d’IA plus sûres et plus résilientes.

