# Annexe A : Glossaire

>Ce glossaire complet fournit les définitions des termes clés en IA, ML et sécurité utilisés dans l'ensemble de l'AISVS pour garantir la clarté et une compréhension commune.

* Exemple adversarial : Une entrée délibérément conçue pour provoquer une erreur dans un modèle d’IA, souvent en ajoutant des perturbations subtiles imperceptibles par les humains.
  ​
* Robustesse adversariale – La robustesse adversariale en IA fait référence à la capacité d'un modèle à maintenir ses performances et à résister à la manipulation ou à la tromperie par des entrées malveillantes délibérément conçues pour provoquer des erreurs.
  ​
* Agent – Les agents d'IA sont des systèmes logiciels qui utilisent l'IA pour poursuivre des objectifs et accomplir des tâches au nom des utilisateurs. Ils démontrent des capacités de raisonnement, de planification et de mémoire, et disposent d'un certain niveau d'autonomie pour prendre des décisions, apprendre et s'adapter.
  ​
* IA agentique : systèmes d'IA capables de fonctionner avec un certain degré d'autonomie pour atteindre des objectifs, prenant souvent des décisions et agissant sans intervention humaine directe.
  ​
* Contrôle d'accès basé sur les attributs (ABAC) : un paradigme de contrôle d'accès où les décisions d'autorisation sont basées sur les attributs de l'utilisateur, de la ressource, de l'action et de l'environnement, évalués au moment de la requête.
  ​
* Attaque par porte dérobée : un type d'attaque par empoisonnement de données où le modèle est entraîné à répondre de manière spécifique à certains déclencheurs tout en se comportant normalement dans les autres cas.
  ​
* Biais : erreurs systémiques dans les résultats des modèles d’IA pouvant conduire à des résultats injustes ou discriminatoires pour certains groupes ou dans des contextes spécifiques.
  ​
* Exploitation des biais : une technique d'attaque qui exploite les biais connus dans les modèles d'IA pour manipuler les résultats ou les sorties.
  ​
* Cedar : le langage de politique et le moteur d'Amazon pour des permissions granulaires utilisés dans la mise en œuvre de l'ABAC pour les systèmes d'IA.
  ​
* Chaîne de pensée : une technique pour améliorer le raisonnement dans les modèles de langage en générant des étapes intermédiaires de raisonnement avant de produire une réponse finale.
  ​
* Disjoncteurs : Mécanismes qui arrêtent automatiquement les opérations des systèmes d'IA lorsque des seuils de risque spécifiques sont dépassés.
  ​
* Fuite de données : exposition involontaire d'informations sensibles par les sorties ou le comportement d'un modèle d'IA.
  ​
* Empoisonnement des données : La corruption délibérée des données d'entraînement afin de compromettre l'intégrité du modèle, souvent pour installer des portes dérobées ou dégrader les performances.
  ​
* Confidentialité différentielle – La confidentialité différentielle est un cadre mathématiquement rigoureux pour la diffusion d'informations statistiques sur des ensembles de données tout en protégeant la vie privée des individus. Elle permet à un détenteur de données de partager des tendances agrégées du groupe tout en limitant les informations divulguées concernant des individus spécifiques.
  ​
* Incrustations : Représentations vectorielles denses des données (texte, images, etc.) qui capturent le sens sémantique dans un espace à haute dimension.
  ​
* Explicabilité – L'explicabilité en IA est la capacité d'un système d'IA à fournir des raisons compréhensibles par les humains pour ses décisions et prédictions, offrant des insights sur son fonctionnement interne.
  ​
* IA explicable (XAI) : systèmes d'IA conçus pour fournir des explications compréhensibles par l'humain concernant leurs décisions et comportements grâce à diverses techniques et cadres.
  ​
* Apprentissage Fédéré : une approche d'apprentissage automatique où les modèles sont entraînés sur plusieurs dispositifs décentralisés détenant des échantillons de données locaux, sans échanger les données elles-mêmes.
  ​
* Garde-fous : Contraintes mises en place pour empêcher les systèmes d'IA de produire des résultats nuisibles, biaisés ou autrement indésirables.
  ​
* Hallucination – Une hallucination d’IA fait référence à un phénomène où un modèle d’IA génère des informations incorrectes ou trompeuses qui ne sont pas basées sur ses données d’entraînement ni sur la réalité factuelle.
  ​
* Humain dans la boucle (HITL) : systèmes conçus pour nécessiter une supervision, une vérification ou une intervention humaine aux points de décision cruciaux.
  ​
* Infrastructure as Code (IaC) : Gestion et provisionnement de l'infrastructure par le code plutôt que par des processus manuels, permettant le scan de sécurité et des déploiements cohérents.
  ​
* Jailbreak : Techniques utilisées pour contourner les garde-fous de sécurité dans les systèmes d'IA, en particulier dans les grands modèles de langage, afin de produire du contenu interdit.
  ​
* Moindre privilège : Le principe de sécurité consistant à accorder uniquement les droits d'accès minimaux nécessaires aux utilisateurs et aux processus.
  ​
* LIME (Explications Locales Interprétables Indépendantes du Modèle) : Une technique permettant d'expliquer les prédictions de tout classificateur d'apprentissage automatique en l'approximation localement avec un modèle interprétable.
  ​
* Attaque d'inférence d'appartenance : une attaque visant à déterminer si un point de données spécifique a été utilisé pour entraîner un modèle d'apprentissage automatique.
  ​
* MITRE ATLAS : Paysage des menaces adverses pour les systèmes d'intelligence artificielle ; une base de connaissances des tactiques et techniques adverses contre les systèmes d'IA.
  ​
* Fiche de modèle – Une fiche de modèle est un document qui fournit des informations standardisées sur les performances, les limites, les usages prévus et les considérations éthiques d'un modèle d'IA afin de favoriser la transparence et le développement responsable de l'IA.
  ​
* Extraction de modèle : une attaque où un adversaire interroge de manière répétée un modèle cible afin de créer une copie fonctionnellement similaire sans autorisation.
  ​
* Inversion de modèle : une attaque qui tente de reconstruire les données d'entraînement en analysant les sorties du modèle.
  ​
* Gestion du cycle de vie des modèles – La gestion du cycle de vie des modèles d'IA est le processus de supervision de toutes les étapes de l'existence d'un modèle d'IA, y compris sa conception, son développement, son déploiement, sa surveillance, sa maintenance et sa retraite éventuelle, afin de garantir qu'il reste efficace et aligné sur les objectifs.
  ​
* Empoisonnement de modèle : introduction de vulnérabilités ou de portes dérobées directement dans un modèle pendant le processus d'entraînement.
  ​
* Vol de modèle : Extraction d'une copie ou d'une approximation d'un modèle propriétaire par des requêtes répétées.
  ​
* Système multi-agent : Un système composé de plusieurs agents d’IA interactifs, chacun pouvant avoir des capacités et des objectifs différents.
  ​
* OPA (Open Policy Agent) : Un moteur de politique open-source qui permet une application unifiée des politiques à travers la pile.
  ​
* Apprentissage automatique respectueux de la vie privée (PPML) : Techniques et méthodes pour entraîner et déployer des modèles d'apprentissage automatique tout en protégeant la confidentialité des données d'entraînement.
  ​
* Injection de prompt : une attaque où des instructions malveillantes sont intégrées dans les entrées pour dépasser le comportement prévu d’un modèle.
  ​
* RAG (Génération Augmentée par Recherche) : Une technique qui améliore les grands modèles de langage en récupérant des informations pertinentes à partir de sources de connaissances externes avant de générer une réponse.
  ​
* Red-Teaming : La pratique de tester activement les systèmes d'IA en simulant des attaques adverses afin d'identifier les vulnérabilités.
  ​
* SBOM (Liste des composants logiciels) : Un enregistrement formel contenant les détails et les relations de la chaîne d'approvisionnement des différents composants utilisés dans la création de logiciels ou de modèles d'IA.
  ​
* SHAP (SHapley Additive exPlanations) : Une approche basée sur la théorie des jeux pour expliquer la sortie de n'importe quel modèle d'apprentissage automatique en calculant la contribution de chaque caractéristique à la prédiction.
  ​
* Attaque de la chaîne d'approvisionnement : compromettre un système en ciblant des éléments moins sécurisés de sa chaîne d'approvisionnement, tels que des bibliothèques tierces, des ensembles de données ou des modèles pré-entraînés.
  ​
* Apprentissage par transfert : une technique où un modèle développé pour une tâche est réutilisé comme point de départ pour un modèle sur une deuxième tâche.
  ​
* Base de données vectorielle : une base de données spécialisée conçue pour stocker des vecteurs de haute dimension (embeddings) et effectuer des recherches de similarité efficaces.
  ​
* Analyse des vulnérabilités : Outils automatisés qui identifient les vulnérabilités de sécurité connues dans les composants logiciels, y compris les frameworks d'IA et les dépendances.
  ​
* Filigrane : Techniques pour intégrer des marqueurs imperceptibles dans le contenu généré par l'IA afin d'en tracer l'origine ou de détecter la génération par IA.
  ​
* Vulnérabilité Zero-Day : Une vulnérabilité auparavant inconnue que les attaquants peuvent exploiter avant que les développeurs ne créent et déploient un correctif.

