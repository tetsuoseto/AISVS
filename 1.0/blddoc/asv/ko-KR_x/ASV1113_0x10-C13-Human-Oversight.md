# C13 인간 감독, 책임 및 거버넌스

## 통제 목표

이 장은 AI 시스템에서 인간의 감독 유지와 명확한 책임 체인을 확보하기 위한 요구사항을 제시하며, AI 생애주기 전반에 걸친 설명가능성, 투명성 및 윤리적 관리가 보장되도록 한다.

---

## C13.1 킬-스위치 및 오버라이드 메커니즘

AI 시스템의 안전하지 않은 동작이 관찰될 때 종료 또는 롤백 경로를 제공하십시오.

|   #    | 설명                                                                 | 레벨  | 역할  |
| :----: | ------------------------------------------------------------------ | :-: | :-: |
| 13.1.1 | 수동 차단 스위치 메커니즘이 존재하는지 확인하고, 인공지능 모델의 추론 및 출력을 즉시 중단할 수 있는지 검증하십시오. |  1  | D/V |
| 13.1.2 | 오버라이드 제어가 권한이 부여된 인원만 접근 가능하도록 되어 있는지 확인하십시오.                      |  1  |  D  |
| 13.1.3 | 롤백 절차가 이전 모델 버전으로 되돌리거나 안전 모드로 작동하는지 확인합니다.                        |  3  | D/V |
| 13.1.4 | 오버라이드 메커니즘이 정기적으로 테스트되는지 확인하십시오.                                   |  3  |  V  |

---

## C13.2 사람-관여-루프 의사결정 체크포인트

위험 수준이 미리 정의된 임계치를 초과할 때 인간의 승인을 요구한다.

|   #    | 설명                                                                          | 레벨  | 역할  |
| :----: | --------------------------------------------------------------------------- | :-: | :-: |
| 13.2.1 | 고위험 AI 의사결정은 실행되기 전에 명시적인 인간 승인이 필요하다는 것을 확인하십시오.                           |  1  | D/V |
| 13.2.2 | 위험 임계값이 명확하게 정의되어 있고 자동으로 인간 검토 워크플로우를 트리거하는지 확인합니다.                        |  1  |  D  |
| 13.2.3 | 필요한 시간 내에 인간의 승인을 얻지 못하는 경우 시간에 민감한 의사결정에 대한 대체 절차가 있는지 확인합니다.              |  2  |  D  |
| 13.2.4 | 해당하는 경우에 한하여 에스컬레이션 절차가 서로 다른 의사결정 유형 또는 위험 범주에 대해 명확한 권한 수준을 정의하는지 확인하십시오. |  3  | D/V |

---

## C13.3 책임 연쇄 패턴 및 감사성

운영자 조치 및 모델 결정 기록

|   #    | 설명                                                               | 레벨  | 역할  |
| :----: | ---------------------------------------------------------------- | :-: | :-: |
| 13.3.1 | 모든 AI 시스템 결정 및 인간 개입이 타임스탬프, 사용자 신원 및 의사 결정 근거와 함께 로깅되는지 확인하십시오. |  1  | D/V |
| 13.3.2 | 감사 로그가 변조될 수 없음을 확인하고 무결성 검증 메커니즘을 포함하십시오.                       |  2  |  D  |

---

## C13.4 설명 가능한-AI 기법

표면 특성 중요도, 반사실들, 그리고 로컬 설명들.

|   #    | 설명                                                                       | 레벨  | 역할  |
| :----: | ------------------------------------------------------------------------ | :-: | :-: |
| 13.4.1 | AI 시스템이 결정에 대해 기본적인 설명을 사람이 읽기 쉬운 형식으로 제공하는지 확인하십시오.                     |  1  | D/V |
| 13.4.2 | 설명 품질이 인간 평가 연구 및 지표를 통해 검증되는지 확인하십시오.                                   |  2  |  V  |
| 13.4.3 | 중요한 의사 결정에 대해 특성 중요도 점수나 기여도 산정 방법(SHAP, LIME 등)을 사용할 수 있는지 확인합니다.       |  3  | D/V |
| 13.4.4 | 해당 사용 사례와 도메인에 적용 가능하다면, 반사실적 설명이 입력값을 어떻게 수정하면 결과를 바꿀 수 있는지 보여주는지 확인한다. |  3  |  V  |

---

## C13.5 모델 카드 및 사용 정보 공개

의도된 사용, 성능 지표 및 윤리적 고려 사항에 대한 모델 카드를 유지하십시오.

|   #    | 설명                                                                                    | 레벨  | 역할  |
| :----: | ------------------------------------------------------------------------------------- | :-: | :-: |
| 13.5.1 | 모델 카드가 의도된 사용 사례, 한계 및 알려진 실패 모드를 문서화하는지 확인하십시오.                                      |  1  |  D  |
| 13.5.2 | 다양한 적용 가능한 사용 사례에 대한 성능 지표가 공개되어 있는지 확인하십시오.                                          |  1  | D/V |
| 13.5.3 | 다음이 문서화되고 정기적으로 업데이트되는지 확인하십시오: 윤리적 고려사항, 편향 평가, 공정성 평가, 학습 데이터의 특징 및 알려진 학습 데이터의 한계. |  2  |  D  |
| 13.5.4 | 모델 카드가 버전 관리되고 변경 추적이 가능하도록 모델 수명주기 전반에 걸쳐 유지 관리되는지 확인합니다.                            |  2  | D/V |

---

## C13.6 불확실성 정량화

응답에서 신뢰도 점수나 엔트로피 지표를 전파합니다.

|   #    | 설명                                                | 레벨  | 역할  |
| :----: | ------------------------------------------------- | :-: | :-: |
| 13.6.1 | AI 시스템이 출력과 함께 신뢰도 점수나 불확실성 지표를 제공하는지 확인하십시오.     |  1  |  D  |
| 13.6.2 | 불확실성 임계값이 추가적인 인간 검토 또는 대안적 의사결정 경로를 촉발하는지 확인합니다. |  2  | D/V |
| 13.6.3 | 불확실성 정량화 방법이 그라운드 트루스 데이터에 대해 보정되고 검증되었는지 확인하십시오. |  2  |  V  |
| 13.6.4 | 다단계 AI 워크플로우를 통해 불확실성 전파가 유지되는지 확인합니다.            |  3  | D/V |

---

## C13.7 사용자용 투명성 보고서

사건, 드리프트 및 데이터 사용에 대한 주기적인 공시를 제공합니다.

|   #    | 설명                                                            | 레벨  | 역할  |
| :----: | ------------------------------------------------------------- | :-: | :-: |
| 13.7.1 | 이해관계자들에게 데이터 사용 정책 및 사용자 동의 관리 관행이 명확하게 전달되는지 확인합니다.          |  1  | D/V |
| 13.7.2 | AI 영향 평가가 수행되고 그 결과가 보고서에 포함되는지 확인합니다.                        |  2  | D/V |
| 13.7.3 | 정기적으로 게시되는 투명성 보고서가 AI 사고와 운영 지표를 합리적인 수준으로 상세하게 공시하는지 확인합니다. |  2  | D/V |

### 참고문헌

* [EU Artificial Intelligence Act — Regulation (EU) 2024/1689 (Official Journal, 12 July 2024)](https://eur-lex.europa.eu/eli/reg/2024/1689/oj)
* [ISO/IEC 23894:2023 — Artificial Intelligence — Guidance on Risk Management](https://www.iso.org/standard/77304.html)
* [ISO/IEC 42001:2023 — AI Management Systems Requirements](https://www.iso.org/standard/81230.html)
* [NIST AI Risk Management Framework 1.0](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
* [NIST SP 800-53 Revision 5 — Security and Privacy Controls](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf)
* [A Unified Approach to Interpreting Model Predictions (SHAP, ICML 2017)](https://arxiv.org/abs/1705.07874)
* [Model Cards for Model Reporting (Mitchell et al., 2018)](https://arxiv.org/abs/1810.03993)
* [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning (Gal & Ghahramani, 2016)](https://arxiv.org/abs/1506.02142)
* [ISO/IEC 24029-2:2023 — Robustness of Neural Networks — Methodology for Formal Methods](https://www.iso.org/standard/79804.html)
* [IEEE 7001-2021 — Transparency of Autonomous Systems](https://standards.ieee.org/ieee/7001/6929/)
* [GDPR — Article 5 "Transparency Principle" (Regulation (EU) 2016/679)](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX%3A32016R0679)
* [Human Oversight under Article 14 of the EU AI Act (Fink, 2025)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5147196)

