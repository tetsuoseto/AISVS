# C13 인간 감독, 책임 및 거버넌스

## 제어 목표

이 장에서는 AI 시스템에서 인간의 감독 유지와 명확한 책임 체인 확보를 위한 요구사항을 제공하며, AI 수명 주기 전반에 걸쳐 설명 가능성, 투명성 및 윤리적 관리를 보장합니다.

---

## C13.1 킬 스위치 및 오버라이드 메커니즘

AI 시스템의 위험한 동작이 관찰될 때 종료 또는 롤백 경로를 제공하십시오.

|   #    | 설명                                                     | 레벨  | 역할  |
| :----: | ------------------------------------------------------ | :-: | :-: |
| 13.1.1 | AI 모델 추론 및 출력을 즉시 중단할 수 있는 수동 킬스위치 메커니즘이 존재하는지 확인하십시오. |  1  | D/V |
| 13.1.2 | 재정의 제어가 허가된 인원만 접근할 수 있는지 확인하십시오.                      |  1  |  D  |
| 13.1.3 | 롤백 절차가 이전 모델 버전 또는 안전 모드 작동으로 복원할 수 있는지 검증하십시오.        |  3  | D/V |
| 13.1.4 | 오버라이드 메커니즘이 정기적으로 테스트되는지 확인하십시오.                       |  3  |  V  |

---

## C13.2 인간 참여 결정 검증 지점

사전 정의된 위험 임계값을 초과할 때 인간의 승인을 요구합니다.

|   #    | 설명                                                                   | 레벨  | 역할  |
| :----: | -------------------------------------------------------------------- | :-: | :-: |
| 13.2.1 | 고위험 AI 결정은 실행 전에 명시적인 인간 승인을 필요로 한다는 것을 확인하십시오.                      |  1  | D/V |
| 13.2.2 | 위험 임계값이 명확하게 정의되어 있으며 자동으로 인간 검토 워크플로를 트리거하는지 확인하십시오.                |  1  |  D  |
| 13.2.3 | 요구된 시간 내에 인간의 승인을 받을 수 없을 경우 시간에 민감한 결정에 대해 대체 절차가 있는지 확인하십시오.       |  2  |  D  |
| 13.2.4 | 해당되는 경우, 에스컬레이션 절차가 다양한 의사결정 유형이나 위험 범주에 대해 명확한 권한 수준을 정의하는지 확인하십시오. |  3  | D/V |

---

## C13.3 책임 연쇄 및 감사 가능성

연산자 작업 및 모델 결정을 기록하십시오.

|   #    | 설명                                                               | 레벨  | 역할  |
| :----: | ---------------------------------------------------------------- | :-: | :-: |
| 13.3.1 | 모든 AI 시스템 결정과 인간 개입이 타임스탬프, 사용자 신원, 그리고 결정 근거와 함께 기록되었는지 확인하십시오. |  1  | D/V |
| 13.3.2 | 감사 로그가 변조될 수 없으며 무결성 검증 메커니즘을 포함하고 있는지 확인하십시오.                   |  2  |  D  |

---

## C13.4 설명 가능한 AI 기법

표면 특징 중요도, 카운터팩추얼, 그리고 지역 설명.

|   #    | 설명                                                                        | 레벨  | 역할  |
| :----: | ------------------------------------------------------------------------- | :-: | :-: |
| 13.4.1 | AI 시스템이 그들의 결정에 대해 사람이 이해할 수 있는 형식으로 기본적인 설명을 제공하는지 확인하십시오.               |  1  | D/V |
| 13.4.2 | 설명 품질이 인간 평가 연구 및 지표를 통해 검증되었는지 확인하십시오.                                   |  2  |  V  |
| 13.4.3 | 중요한 결정에 대해 특성 중요도 점수 또는 귀속 방법(SHAP, LIME 등)이 사용 가능한지 확인하십시오.              |  3  | D/V |
| 13.4.4 | 반사실적 설명이 입력을 어떻게 수정하여 결과를 변경할 수 있는지 보여주는지, 사용 사례 및 도메인에 적용 가능한 경우 확인하십시오. |  3  |  V  |

---

## C13.5 모델 카드 및 사용 공지

의도된 사용, 성능 지표 및 윤리적 고려사항에 대한 모델 카드를 유지하십시오.

|   #    | 설명                                                                              | 레벨  | 역할  |
| :----: | ------------------------------------------------------------------------------- | :-: | :-: |
| 13.5.1 | 모델 카드가 의도된 사용 사례, 한계 및 알려진 실패 모드를 문서화했는지 확인하세요.                                 |  1  |  D  |
| 13.5.2 | 다양한 적용 가능한 사용 사례 전반에 걸친 성능 지표가 공개되어 있는지 확인합니다.                                  |  1  | D/V |
| 13.5.3 | 윤리적 고려사항, 편향 평가, 공정성 평가, 학습 데이터 특성 및 알려진 학습 데이터 한계가 문서화되어 정기적으로 업데이트되는지 확인하십시오. |  2  |  D  |
| 13.5.4 | 모델 카드가 버전 관리되고 변경 추적과 함께 모델 수명 주기 전반에 걸쳐 유지되는지 확인하십시오.                          |  2  | D/V |

---

## C13.6 불확실성 정량화

응답에서 신뢰도 점수 또는 엔트로피 측정값을 전파합니다.

|   #    | 설명                                                | 레벨  | 역할  |
| :----: | ------------------------------------------------- | :-: | :-: |
| 13.6.1 | AI 시스템이 출력과 함께 신뢰도 점수 또는 불확실성 측정값을 제공하는지 확인하세요.   |  1  |  D  |
| 13.6.2 | 불확실성 임계값이 추가적인 인간 검토 또는 대체 의사결정 경로를 유발하는지 확인하십시오. |  2  | D/V |
| 13.6.3 | 불확실성 정량화 방법이 실제 데이터에 대해 보정되고 검증되었는지 확인하십시오.       |  2  |  V  |
| 13.6.4 | 불확실성 전파가 다단계 AI 워크플로우 전반에 걸쳐 유지되는지 확인합니다.         |  3  | D/V |

---

## C13.7 사용자 대상 투명성 보고서

사고, 변화, 데이터 사용에 대한 정기적인 공개를 제공하십시오.

|   #    | 설명                                                              | 레벨  | 역할  |
| :----: | --------------------------------------------------------------- | :-: | :-: |
| 13.7.1 | 데이터 사용 정책과 사용자 동의 관리 관행이 이해관계자에게 명확하게 전달되었는지 확인하십시오.            |  1  | D/V |
| 13.7.2 | AI 영향 평가가 수행되고 그 결과가 보고서에 포함되는지 확인하십시오.                         |  2  | D/V |
| 13.7.3 | 투명성 보고서가 정기적으로 공개되어 AI 사고와 운영 지표를 합리적인 수준의 세부사항으로 공개하는지 확인하십시오. |  2  | D/V |

### 참고 문헌

* [EU Artificial Intelligence Act — Regulation (EU) 2024/1689 (Official Journal, 12 July 2024)](https://eur-lex.europa.eu/eli/reg/2024/1689/oj)
* [ISO/IEC 23894:2023 — Artificial Intelligence — Guidance on Risk Management](https://www.iso.org/standard/77304.html)
* [ISO/IEC 42001:2023 — AI Management Systems Requirements](https://www.iso.org/standard/81230.html)
* [NIST AI Risk Management Framework 1.0](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
* [NIST SP 800-53 Revision 5 — Security and Privacy Controls](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf)
* [A Unified Approach to Interpreting Model Predictions (SHAP, ICML 2017)](https://arxiv.org/abs/1705.07874)
* [Model Cards for Model Reporting (Mitchell et al., 2018)](https://arxiv.org/abs/1810.03993)
* [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning (Gal & Ghahramani, 2016)](https://arxiv.org/abs/1506.02142)
* [ISO/IEC 24029-2:2023 — Robustness of Neural Networks — Methodology for Formal Methods](https://www.iso.org/standard/79804.html)
* [IEEE 7001-2021 — Transparency of Autonomous Systems](https://standards.ieee.org/ieee/7001/6929/)
* [GDPR — Article 5 "Transparency Principle" (Regulation (EU) 2016/679)](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX%3A32016R0679)
* [Human Oversight under Article 14 of the EU AI Act (Fink, 2025)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5147196)

