# 부록 A: 용어집

이 포괄적인 용어집은 명확성과 공통된 이해를 보장하기 위해 AISVS 전반에 사용되는 주요 AI, ML 및 보안 용어들의 정의를 제공합니다.

* 적대적 예제: 사람이 인지할 수 없는 미세한 교란을 추가하여 AI 모델이 실수를 하도록 의도적으로 만든 입력 데이터.
  ​
* 적대적 강인성 – AI에서 적대적 강인성은 모델이 의도적으로 오류를 발생시키기 위해 조작된 악의적 입력에 의해 속거나 조작되지 않고 그 성능을 유지하는 능력을 의미합니다.
  ​
* 에이전트 – AI 에이전트는 사용자를 대신하여 목표를 추구하고 작업을 완료하기 위해 AI를 사용하는 소프트웨어 시스템입니다. 이들은 추론, 계획, 기억 능력을 보이며 의사결정, 학습, 적응을 할 수 있는 일정 수준의 자율성을 갖추고 있습니다.
  ​
* 에이전틱 AI: 일정 수준의 자율성을 가지고 목표를 달성하기 위해 작동하며, 종종 직접적인 인간 개입 없이 결정을 내리고 행동을 취할 수 있는 AI 시스템.
  ​
* 속성 기반 접근 제어(ABAC): 사용자, 자원, 동작 및 환경의 속성을 기반으로 권한 부여 결정을 내리며, 쿼리 시점에 평가되는 접근 제어 패러다임.
  ​
* 백도어 공격: 모델이 특정 트리거에 대해 특정 방식으로 반응하도록 학습되는 데이터 중독 공격의 한 유형으로, 그 외에는 정상적으로 동작하는 경우.
  ​
* 편향: 특정 그룹이나 특정 상황에서 불공정하거나 차별적인 결과를 초래할 수 있는 AI 모델 출력의 체계적인 오류.
  ​
* 편향 악용: AI 모델의 알려진 편향을 이용하여 출력이나 결과를 조작하는 공격 기법.
  ​
* Cedar: AI 시스템용 ABAC 구현에 사용되는 세분화된 권한을 위한 Amazon의 정책 언어 및 엔진.
  ​
* 연쇄 사고: 최종 답변을 생성하기 전에 중간 추론 단계를 생성하여 언어 모델의 추론 능력을 향상시키는 기술.
  ​
* 서킷 브레이커: 특정 위험 임계값을 초과할 경우 AI 시스템 작동을 자동으로 중단하는 메커니즘.
  ​
* 데이터 유출: AI 모델 출력 또는 동작을 통해 민감한 정보가 의도치 않게 노출되는 것.
  ​
* 데이터 포이즈닝: 모델의 무결성을 훼손하기 위해 학습 데이터를 고의로 오염시키는 행위로, 주로 백도어를 설치하거나 성능을 저하시킬 목적으로 수행됩니다.
  ​
* 차등 프라이버시 – 차등 프라이버시는 개별 데이터 주체의 프라이버시를 보호하면서 데이터셋에 대한 통계 정보를 공개하기 위한 수학적으로 엄격한 프레임워크입니다. 이는 데이터 보유자가 특정 개인에 대한 정보 유출을 제한하면서 집단의 집계 패턴을 공유할 수 있도록 합니다.
  ​
* 임베딩: 고차원 공간에서 의미론적 의미를 포착하는 데이터(텍스트, 이미지 등)의 밀집 벡터 표현입니다.
  ​
* 설명 가능성 – AI에서 설명 가능성은 AI 시스템이 결정과 예측에 대해 사람도 이해할 수 있는 이유를 제공하여 내부 작동 방식에 대한 통찰을 제공하는 능력입니다.
  ​
* 설명 가능한 AI (XAI): 다양한 기법과 프레임워크를 통해 인간이 이해할 수 있는 설명을 제공하도록 설계된 AI 시스템.
  ​
* 페더레이티드 러닝: 데이터 자체를 교환하지 않고 로컬 데이터 샘플을 보유한 여러 분산된 장치에서 모델을 학습하는 기계 학습 접근 방식.
  ​
* 가드레일: AI 시스템이 해롭거나 편향되었거나 기타 바람직하지 않은 출력을 생성하지 못하도록 방지하기 위해 구현된 제약 조건.
  ​
* 환각 – AI 환각은 AI 모델이 학습 데이터나 사실적 현실에 근거하지 않은 잘못되거나 오해의 소지가 있는 정보를 생성하는 현상을 의미합니다.
  ​
* 휴먼 인 더 루프(HITL): 중요한 의사결정 지점에서 인간의 감독, 검증 또는 개입을 필요로 하도록 설계된 시스템.
  ​
* 인프라스트럭처 코드화(IaC): 수동 프로세스 대신 코드를 통해 인프라를 관리 및 프로비저닝하여 보안 스캔과 일관된 배포를 가능하게 하는 것.
  ​
* 탈옥(Jailbreak): 특히 대규모 언어 모델에서 인공지능 시스템의 안전 방책을 우회하여 금지된 콘텐츠를 생성하는 데 사용되는 기술.
  ​
* 최소 권한 원칙: 사용자와 프로세스에 대해 필요한 최소한의 접근 권한만 부여하는 보안 원칙.
  ​
* LIME (국소 해석 가능 모델-독립적 설명): 해석 가능한 모델로 국소적으로 근사하여 모든 머신러닝 분류기의 예측을 설명하는 기법입니다.
  ​
* 멤버십 추론 공격: 특정 데이터 포인트가 머신러닝 모델 학습에 사용되었는지 여부를 판단하는 것을 목표로 하는 공격.
  ​
* MITRE ATLAS: 인공지능 시스템에 대한 적대적 위협 환경; AI 시스템에 대한 적대적 전술과 기법의 지식 기반.
  ​
* 모델 카드 – 모델 카드는 AI 모델의 성능, 한계, 의도된 용도 및 윤리적 고려 사항에 대한 표준화된 정보를 제공하여 투명성과 책임 있는 AI 개발을 촉진하는 문서입니다.
  ​
* 모델 추출: 공격자가 권한 없이 대상 모델에 반복적으로 쿼리하여 기능적으로 유사한 복제본을 생성하는 공격.
  ​
* 모델 역전: 모델 출력을 분석하여 학습 데이터를 복원하려는 공격.
  ​
* 모델 수명 주기 관리 – AI 모델 수명 주기 관리는 AI 모델의 설계, 개발, 배포, 모니터링, 유지보수 및 최종 폐기 등 모든 단계를 감독하는 과정으로, 모델이 효과적이고 목표에 부합하도록 보장하는 것입니다.
  ​
* 모델 포이즈닝: 훈련 과정 중에 모델에 취약점이나 백도어를 직접 도입하는 행위.
  ​
* 모델 도용/도난: 반복적인 쿼리를 통해 독점 모델의 복제본 또는 근사치를 추출하는 행위.
  ​
* 멀티 에이전트 시스템: 서로 상호작용하는 여러 AI 에이전트로 구성된 시스템으로, 각 에이전트는 잠재적으로 서로 다른 역량과 목표를 가질 수 있습니다.
  ​
* OPA (Open Policy Agent): 스택 전반에 걸쳐 통합된 정책 집행을 가능하게 하는 오픈 소스 정책 엔진입니다.
  ​
* 프라이버시 보호 머신러닝(PPML): 훈련 데이터의 프라이버시를 보호하면서 ML 모델을 학습하고 배포하는 기술과 방법.
  ​
* 프롬프트 인젝션: 악의적인 명령이 입력에 삽입되어 모델의 의도된 동작을 무시하도록 하는 공격.
  ​
* RAG (검색 보강 생성): 응답을 생성하기 전에 외부 지식 소스에서 관련 정보를 검색하여 대형 언어 모델을 향상시키는 기술입니다.
  ​
* 레드-팀링: 취약점을 식별하기 위해 적대적 공격을 시뮬레이션하여 AI 시스템을 적극적으로 테스트하는 실습.
  ​
* SBOM (소프트웨어 자재 명세서): 소프트웨어나 AI 모델을 구축하는 데 사용되는 다양한 구성 요소들의 세부 사항과 공급망 관계를 포함하는 공식 기록.
  ​
* SHAP (Shapley 가법 설명): 각 특징이 예측에 기여하는 정도를 계산하여 모든 머신러닝 모델의 출력을 설명하는 게임 이론적 접근 방법.
  ​
* 공급망 공격: 타사 라이브러리, 데이터세트 또는 사전 학습된 모델과 같은 공급망 내 보안 수준이 낮은 요소를 대상으로 시스템을 침해하는 것.
  ​
* 전이 학습: 한 작업을 위해 개발된 모델을 두 번째 작업의 모델 시작점으로 재사용하는 기법.
  ​
* 벡터 데이터베이스: 고차원 벡터(임베딩)를 저장하고 효율적인 유사도 검색을 수행하도록 설계된 전문 데이터베이스입니다.
  ​
* 취약점 스캐닝: AI 프레임워크 및 종속성을 포함한 소프트웨어 구성요소에서 알려진 보안 취약점을 식별하는 자동화 도구.
  ​
* 워터마킹: AI 생성 콘텐츠에 출처 추적이나 AI 생성 감지를 위해 인지할 수 없는 마커를 삽입하는 기술.
  ​
* 제로데이 취약점: 개발자가 패치를 생성하고 배포하기 전에 공격자가 악용할 수 있는 이전에 알려지지 않은 취약점.

