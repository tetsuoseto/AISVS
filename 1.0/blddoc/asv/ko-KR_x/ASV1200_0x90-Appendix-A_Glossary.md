# 부록 A: 용어집

>This 이 포괄적 용어집은 AISVS 전반에 걸쳐 사용되는 주요 AI, ML 및 보안 용어의 정의를 제공하여 명확성과 공통 이해를 보장합니다.

* 적대적 예제: AI 모델이 실수를 하도록 의도적으로 설계된 입력으로, 종종 인간이 인지할 수 없는 미세한 섭동이 추가되어 발생한다.
  ​
* 적대적 강건성 – AI에서의 적대적 강건성은 모델의 성능을 유지하고 의도적으로 설계된 악의적 입력에 의해 속거나 조작당하지 않도록 하는 능력을 말한다.
  ​
* 에이전트 – 인공지능 에이전트는 사용자를 대신해 목표를 추구하고 작업을 수행하기 위해 인공지능을 사용하는 소프트웨어 시스템입니다. 이들은 추론, 계획, 기억을 보이며, 의사결정을 내리고 학습하며 적응할 수 있는 자율성의 수준을 가집니다.
  ​
* 에이전트형 인공지능: 목표를 달성하기 위해 일정 수준의 자율성으로 작동하는 인공지능 시스템으로, 종종 인간의 직접 개입 없이 의사결정을 내리고 행동을 취합니다.
  ​
* 속성 기반 접근 제어(ABAC): 사용자, 리소스, 작업, 환경의 속성을 바탕으로 권한 부여 결정이 내려지는 접근 제어 패러다임으로, 쿼리 시점에 평가됩니다.
  ​
* 백도어 공격: 데이터 중독 공격의 한 유형으로, 모델이 특정 트리거에 대해 특정 방식으로 응답하도록 학습되며, 그 외의 경우에는 일반적으로 정상적으로 작동한다.
  ​
* 편향: 특정 집단이나 특정 맥락에서 불공정하거나 차별적인 결과를 초래할 수 있는 AI 모델 출력의 체계적 오류.
  ​
* 편향 악용: AI 모델의 알려진 편향을 이용해 출력이나 결과를 조작하는 공격 기법.
  ​
* Cedar: AI 시스템용 ABAC 구현에 사용되는 세밀한 권한을 위한 아마존의 정책 언어 및 엔진.
  ​
* Chain of Thought: 언어 모델의 추론 능력을 향상시키기 위한 기술로, 최종 답변을 제시하기 전에 중간 추론 단계를 생성하는 방법.
  ​
* 회로 차단기: 특정 위험 임계값을 초과했을 때 AI 시스템 작동을 자동으로 중단시키는 메커니즘.
  ​
* 데이터 유출: AI 모델의 출력이나 동작을 통해 민감한 정보가 의도치 않게 노출되는 현상.
  ​
* 데이터 포이징: 모델 무결성을 해치기 위해 학습 데이터를 의도적으로 오염시키는 행위로, 종종 백도어를 설치하거나 성능 저하를 유도한다.
  ​
* 차등 프라이버시 – 차등 프라이버시는 데이터 세트에 대한 통계 정보를 공개하면서도 개별 데이터 주체의 프라이버시를 보호하기 위한 수학적으로 엄밀한 프레임워크이다. 이는 데이터 소유자가 특정 개인에 대해 누설되는 정보를 제한하는 한편 그룹의 집계 패턴을 공유할 수 있도록 한다.
  ​
* 임베딩: 텍스트, 이미지 등과 같은 데이터의 고차원 공간에서 의미를 포착하는 밀집 벡터 표현.
  ​
* 설명가능성 – AI에서의 설명가능성은 AI 시스템이 그 결정과 예측에 대해 인간이 이해할 수 있는 이유를 제시하는 능력으로, 내부 작동 방식에 대한 통찰을 제공한다.
  ​
* 설명가능한 인공지능(XAI): 의사결정과 행동에 대해 인간이 이해할 수 있는 설명을 제공하도록 설계된 AI 시스템으로, 다양한 기법과 프레임워크를 통해 구현된다.
  ​
* 연합 학습: 로컬 데이터 샘플을 보유한 다수의 분산된 장치에서 데이터를 서로 교환하지 않고 모델을 학습시키는 기계 학습 접근 방식.
  ​
* 가드레일: AI 시스템이 해롭거나 편향되거나 그 밖의 바람직하지 않은 출력을 생성하지 못하도록 구현된 제약.
  ​
* 환각 – AI의 환각은 AI 모델이 학습 데이터나 사실에 기반하지 않는 잘못되거나 오해를 일으키는 정보를 생성하는 현상을 가리킵니다.
  ​
* Human-in-the-Loop (HITL): 핵심 의사결정 시점에서 인간의 감독, 검증 또는 개입을 필요로 하도록 설계된 시스템.
  ​
* 코드로서의 인프라(IaC): 수동 프로세스가 아닌 코드를 통해 인프라를 관리하고 프로비저닝하며, 보안 스캐닝을 가능하게 하고 일관된 배포를 보장합니다.
  ​
* 탈옥: AI 시스템의 안전 가드레일을 우회하기 위해 사용되는 기법들로, 특히 대형 언어 모델에서 금지된 콘텐츠를 생성하기 위한 목적으로 사용된다.
  ​
* 최소 권한 원칙: 사용자와 프로세스에 필요한 최소한의 접근 권한만 부여하는 보안 원칙.
  ​
* LIME(로컬 해석 가능한 모델-독립적 설명): 어떤 머신러닝 분류기의 예측을 해석하기 위한 기술로, 이를 해석 가능한 로컬 모델로 근사하여 설명합니다.
  ​
* 멤버십 추론 공격: 특정 데이터 포인트가 기계 학습 모델의 훈련에 사용되었는지 여부를 판단하는 것을 목표로 하는 공격.
  ​
* MITRE ATLAS: 인공지능 시스템을 위한 적대적 위협 양상; 인공지능 시스템에 대한 적대적 전술 및 기법의 지식 기반.
  ​
* 모델 카드 – 모델 카드는 AI 모델의 성능, 한계, 의도된 사용 및 윤리적 고려사항에 대한 표준화된 정보를 제공하여 투명성과 책임 있는 AI 개발을 촉진하는 문서입니다.
  ​
* 모델 추출: 악의적 공격자가 타깃 모델에 반복적으로 질의하여 무단으로 기능적으로 유사한 사본을 만들어내는 공격.
  ​
* 모델 역추론 공격: 모델의 출력 분석을 통해 학습 데이터를 재구성하려고 시도하는 공격.
  ​
* 모델 수명주기 관리 – AI 모델 수명주기 관리는 AI 모델의 생애 전반의 모든 단계를 감독하는 과정으로, 설계, 개발, 배포, 모니터링, 유지 관리 및 최종 은퇴를 포함하여 그것이 효과적이고 목표에 부합하도록 보장하는 것을 의미합니다.
  ​
* 모델 포이즈닝: 학습 과정에서 모델에 직접 취약점이나 백도어를 주입하는 것.
  ​
* 모델 도용/절도: 반복적인 질의를 통해 독점 모델의 복제본이나 근사치를 추출하는 행위.
  ​
* 다중 에이전트 시스템: 서로 상호 작용하는 여러 인공지능 에이전트로 구성된 시스템으로, 각 에이전트는 잠재적으로 서로 다른 능력과 목표를 가질 수 있습니다.
  ​
* OPA(Open Policy Agent): 스택 전반에 걸친 통합 정책 시행을 가능하게 하는 오픈 소스 정책 엔진.
  ​
* 프라이버시 보존 기계 학습(PPML): 학습 데이터를 보호하는 동시에 ML 모델을 학습하고 배포하는 기술과 방법.
  ​
* 프롬프트 주입: 악의적인 지시가 입력에 포함되어 모델의 의도된 동작을 무력화하는 공격.
  ​
* RAG (Retrieval-Augmented Generation): 응답을 생성하기 전에 외부 지식 소스에서 관련 정보를 검색하여 대형 언어 모델의 성능을 향상시키는 기술.
  ​
* 레드팀 테스트: 취약점을 식별하기 위해 적대적 공격을 시뮬레이션하여 AI 시스템을 적극적으로 테스트하는 관행.
  ​
* SBOM(Software Bill of Materials): 소프트웨어 또는 AI 모델 구축에 사용되는 다양한 구성 요소의 세부 정보와 공급망 관계를 담은 공식 기록.
  ​
* SHAP (SHapley Additive exPlanations): 예측에 대한 각 특성의 기여도를 계산하여 어떤 기계 학습 모델의 출력을 설명하는 게임 이론적 접근 방식.
  ​
* 공급망 공격: 서드파티 라이브러리, 데이터셋 또는 사전 학습된 모델과 같이 공급망의 보안이 취약한 요소를 겨냥하여 시스템을 침해하는 공격.
  ​
* 전이 학습: 하나의 과제를 위해 개발된 모델을 두 번째 과제의 시작점으로 재사용하는 기법.
  ​
* 벡터 데이터베이스: 고차원 벡터(임베딩)를 저장하고 효율적인 유사도 검색을 수행하도록 설계된 특수한 데이터베이스입니다.
  ​
* 취약점 스캐닝: 소프트웨어 구성 요소에서 알려진 보안 취약점을 식별하는 자동화 도구로, AI 프레임워크와 의존성을 포함합니다.
  ​
* 워터마킹: AI가 생성한 콘텐츠에 눈에 띄지 않는 표식을 삽입해 출처를 추적하거나 AI 생성 여부를 감지하는 기술.
  ​
* 제로데이 취약점: 개발자가 패치를 만들고 배포하기 전에 공격자가 악용할 수 있는, 이전에 알려지지 않은 취약점.

