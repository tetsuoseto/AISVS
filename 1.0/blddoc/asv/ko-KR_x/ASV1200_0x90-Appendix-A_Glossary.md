# 부록 A: 용어 사전

>이 종합 용어집은 AISVS 전반에 사용되는 주요 AI, ML 및 보안 용어들의 정의를 제공하여 명확성과 공통 이해를 보장합니다.

* 적대적 예제: 사람에게는 인식할 수 없는 미세한 교란을 추가하여 AI 모델이 오류를 일으키도록 의도적으로 만들어진 입력.
  ​
* 적대적 강인성 – AI에서 적대적 강인성은 모델이 성능을 유지하고 의도적으로 제작된 악의적인 입력에 의해 속거나 조작되는 것을 저항하는 능력을 의미합니다.
  ​
* 에이전트 – AI 에이전트는 사용자를 대신하여 목표를 추구하고 작업을 완료하기 위해 AI를 사용하는 소프트웨어 시스템입니다. 이들은 추론, 계획, 기억 능력을 보이며, 의사결정, 학습 및 적응을 할 수 있는 자율성을 가지고 있습니다.
  ​
* 에이전틱 AI: 일정 정도 자율적으로 작동하여 목표를 달성할 수 있는 AI 시스템으로, 종종 직접적인 인간 개입 없이 의사결정과 행동을 수행합니다.
  ​
* 속성 기반 접근 제어(ABAC): 사용자, 자원, 작업 및 환경의 속성을 기반으로 권한 결정을 내리는 접근 제어 패러다임으로, 쿼리 시점에 평가됩니다.
  ​
* 백도어 공격: 모델이 특정 트리거에 대해서는 특정 방식으로 반응하도록 학습되며, 그 외에는 정상적으로 동작하는 데이터 중독 공격의 한 유형입니다.
  ​
* 편향: 특정 그룹이나 특정 상황에서 불공정하거나 차별적인 결과를 초래할 수 있는 AI 모델 출력의 체계적인 오류.
  ​
* 바이어스 활용: AI 모델의 알려진 바이어스를 이용하여 출력이나 결과를 조작하는 공격 기법.
  ​
* Cedar: AI 시스템의 ABAC 구현에 사용되는 세밀한 권한 관리를 위한 아마존의 정책 언어 및 엔진.
  ​
* Chain of Thought: 최종 답변을 생성하기 전에 중간 추론 단계를 생성하여 언어 모델의 추론 능력을 향상시키는 기법.
  ​
* 회로 차단기: 특정 위험 임계값을 초과할 때 AI 시스템 작동을 자동으로 중단시키는 메커니즘.
  ​
* 데이터 누출: AI 모델 출력물이나 동작을 통해 민감한 정보가 의도치 않게 노출되는 현상.
  ​
* 데이터 포이즈닝: 백도어 설치 또는 성능 저하를 목적으로 모델 무결성을 손상시키기 위해 학습 데이터를 의도적으로 변조하는 행위.
  ​
* 차등 개인정보 보호 – 차등 개인정보 보호는 개별 데이터 주체의 개인정보를 보호하면서 데이터 세트에 대한 통계 정보를 공개할 수 있는 수학적으로 엄격한 프레임워크입니다. 이는 데이터 보유자가 특정 개인에 대한 정보 누출을 제한하면서 그룹의 집계된 패턴을 공유할 수 있도록 합니다.
  ​
* 임베딩: 의미를 고차원 공간에서 포착하는 데이터(텍스트, 이미지 등)의 밀집 벡터 표현.
  ​
* 설명가능성 – AI에서 설명가능성이란 AI 시스템이 그 결정과 예측에 대해 인간이 이해할 수 있는 이유를 제공하여 내부 작동 방식에 대한 통찰을 제공하는 능력을 의미합니다.
  ​
* 설명 가능한 AI (XAI): 다양한 기법과 프레임워크를 통해 그들의 결정과 행동에 대해 인간이 이해할 수 있는 설명을 제공하도록 설계된 AI 시스템.
  ​
* 연합 학습: 데이터를 교환하지 않고 각기 분산된 장치들이 보유한 로컬 데이터 샘플을 기반으로 모델을 학습하는 머신러닝 접근 방식.
  ​
* 가드레일: AI 시스템이 해롭거나 편향되었거나 기타 바람직하지 않은 결과물을 생성하는 것을 방지하기 위해 구현된 제약 조건.
  ​
* 환각 – AI 환각이란 AI 모델이 학습 데이터나 실제 사실에 기반하지 않은 잘못되거나 오해의 소지가 있는 정보를 생성하는 현상을 의미합니다.
  ​
* 휴먼 인 더 루프(HITL): 중요한 의사결정 시점에서 인간의 감독, 검증 또는 개입을 필요로 하도록 설계된 시스템.
  ​
* 인프라스트럭처 코드화(IaC): 수동 프로세스 대신 코드를 통해 인프라를 관리하고 프로비저닝하며, 보안 스캔과 일관된 배포를 가능하게 함.
  ​
* 탈옥: 주로 대형 언어 모델에서 안전 장치를 우회하여 금지된 콘텐츠를 생성하는 데 사용되는 기법.
  ​
* 최소 권한 원칙: 사용자 및 프로세스에 대해 필요한 최소한의 접근 권한만 부여하는 보안 원칙.
  ​
* LIME(국소 해석 가능 모델 불가지론적 설명): 모든 머신러닝 분류기의 예측을 해석 가능한 모델로 국소적으로 근사하여 설명하는 기술입니다.
  ​
* 멤버십 추론 공격: 특정 데이터 포인트가 머신러닝 모델 학습에 사용되었는지를 판별하는 것을 목표로 하는 공격.
  ​
* MITRE ATLAS: 인공지능 시스템을 위한 적대적 위협 지형도; AI 시스템에 대한 적대적 전술 및 기법의 지식 기반.
  ​
* 모델 카드 – 모델 카드는 AI 모델의 성능, 한계, 의도된 사용 및 윤리적 고려사항에 대한 표준화된 정보를 제공하여 투명성과 책임 있는 AI 개발을 촉진하는 문서입니다.
  ​
* 모델 추출: 공격자가 대상 모델에 반복적으로 쿼리를 하여 허가 없이 기능적으로 유사한 복제본을 만드는 공격.
  ​
* 모델 인버전: 모델 출력을 분석하여 학습 데이터를 재구성하려는 공격.
  ​
* 모델 수명 주기 관리 – AI 모델 수명 주기 관리는 AI 모델의 설계, 개발, 배포, 모니터링, 유지보수 및 최종 폐기에 이르는 모든 단계를 감독하여 모델이 효과적이고 목표에 부합하도록 보장하는 과정입니다.
  ​
* 모델 중독: 학습 과정 중에 모델에 취약점이나 백도어를 직접 도입하는 행위.
  ​
* 모델 도용/절취: 반복적인 쿼리를 통해 독점 모델의 복사본 또는 근사치를 추출하는 행위.
  ​
* 멀티 에이전트 시스템: 각각 잠재적으로 다른 역량과 목표를 가진 여러 상호작용하는 AI 에이전트로 구성된 시스템.
  ​
* OPA (오픈 정책 에이전트): 스택 전체에서 통합된 정책 시행을 가능하게 하는 오픈 소스 정책 엔진입니다.
  ​
* 개인정보 보호 머신러닝(PPML): 학습 데이터의 개인정보를 보호하면서 머신러닝 모델을 학습하고 배포하는 기술 및 방법.
  ​
* 프롬프트 인젝션: 악의적인 명령어가 입력에 삽입되어 모델의 의도된 동작을 무력화하는 공격.
  ​
* RAG (검색 강화 생성): 응답을 생성하기 전에 외부 지식 소스에서 관련 정보를 검색하여 대형 언어 모델을 향상시키는 기술입니다.
  ​
* 레드-티밍: AI 시스템의 취약점을 식별하기 위해 적대적 공격을 시뮬레이션하여 적극적으로 테스트하는 행위.
  ​
* SBOM(소프트웨어 자재 명세서): 소프트웨어 또는 AI 모델 구축에 사용되는 다양한 구성 요소의 세부 정보와 공급망 관계를 포함하는 공식 기록.
  ​
* SHAP (SHapley Additive exPlanations): 예측에 대한 각 특징의 기여도를 계산하여 모든 머신러닝 모델의 출력을 설명하는 게임 이론적 접근법.
  ​
* 공급망 공격: 제3자 라이브러리, 데이터셋 또는 사전 학습된 모델과 같이 보안 수준이 낮은 공급망 요소를 대상으로 하여 시스템을 침해하는 행위.
  ​
* 전이 학습: 한 작업을 위해 개발된 모델을 두 번째 작업의 모델 시작점으로 재사용하는 기법.
  ​
* 벡터 데이터베이스: 고차원 벡터(임베딩)를 저장하고 효율적인 유사도 검색을 수행하도록 설계된 특화된 데이터베이스.
  ​
* 취약점 스캐닝: AI 프레임워크 및 종속성을 포함한 소프트웨어 구성 요소에서 알려진 보안 취약점을 자동으로 식별하는 도구.
  ​
* 워터마킹: AI 생성 콘텐츠에 출처를 추적하거나 AI 생성 여부를 감지하기 위해 인지할 수 없는 표시를 삽입하는 기술.
  ​
* 제로데이 취약점: 개발자가 패치를 만들고 배포하기 전에 공격자가 악용할 수 있는 이전에 알려지지 않은 취약점.

