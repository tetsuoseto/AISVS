# 附录 A：术语表

>This 全面的术语表提供了在 AISVS 中使用的关键 AI、ML 和安全术语的定义，以确保清晰度和共同理解。

* 对抗样本：一种经过精心设计的输入，旨在使人工智能模型犯错，通常通过添加人眼几乎察觉不到的微小扰动来实现。
  ​
* 对抗鲁棒性——在人工智能领域，对抗鲁棒性指的是模型在保持性能的同时，抵御被故意设计的恶意输入所迷惑或操控的能力，这些输入旨在导致错误。
  ​
* 智能体 – AI代理是一类使用 AI 的软件系统，用于代表用户追求目标并完成任务。它们具备推理、规划和记忆能力，并具有一定程度的自治性，能够做出决策、学习和适应。
  ​
* 具代理性的 AI：能够在一定程度上自主运作以实现目标的 AI 系统，通常在没有直接人类干预的情况下做出决策并采取行动。
  ​
* 基于属性的访问控制（ABAC）：一种访问控制范式，其授权决策基于用户、资源、操作和环境的属性，在查询时进行评估。
  ​
* 后门攻击：一种数据投毒攻击类型，其中模型在遇到某些触发器时被训练以特定方式响应，而在其他情况下保持正常行为。
  ​
* 偏见：在人工智能模型输出中的系统性错误，可能在特定群体或特定情境下导致不公平或带有歧视性的结果。
  ​
* 偏见利用：一种利用 AI 模型已知偏见来操纵输出或结果的攻击技术。
  ​
* Cedar: 亚马逊的策略语言与引擎，用于在 AI 系统中实现基于属性的访问控制（ABAC）的细粒度权限。
  ​
* 思维链：一种通过在给出最终答案之前生成中间推理步骤来提高语言模型推理能力的技术。
  ​
* 断路器：在特定风险阈值超过时，自动暂停人工智能系统运行的机制。
  ​
* 数据泄露：通过 AI 模型的输出或行为对敏感信息的无意暴露。
  ​
* 数据投毒：故意篡改训练数据以破坏模型的完整性，通常用于安装后门或降低性能。
  ​
* 差分隐私 – 差分隐私是一种在数学上严格的框架，用于在发布数据集的统计信息时保护个体数据主体的隐私。它使数据持有者能够在分享群体的聚合模式的同时，限制关于特定个人泄露的信息。
  ​
* 嵌入：对数据（文本、图像等）的稠密向量表示，在高维空间中捕捉语义含义。
  ​
* 可解释性 – 人工智能中的可解释性是指一个人工智能系统能够提供人类可理解的理由来解释其决策和预测，并揭示其内部工作原理。
  ​
* 可解释的人工智能（XAI）：通过各种技术和框架，使 AI 系统能够为其决策和行为提供人类可理解的解释。
  ​
* 联邦学习：一种机器学习方法，在多台去中心化设备上对模型进行训练，这些设备各自持有本地数据样本，而不直接交换数据。
  ​
* 防护栏：为防止 AI 系统产生有害、带偏见或其他不良输出而实施的约束。
  ​
* 幻觉 – AI 幻觉指的是一种现象：AI 模型生成的信息并非基于其训练数据或客观现实，且可能是错误的或具有误导性的。
  ​
* Human-in-the-Loop (HITL): 设计成在关键决策点需要人类监督、验证或干预的系统。
  ​
* 基础设施即代码（IaC）：通过代码对基础设施进行管理和配置，而非手动流程，从而实现安全扫描和一致的部署。
  ​
* 越狱：用于绕过人工智能系统中的安全护栏的技术，尤其是在大型语言模型中，以生成被禁止的内容。
  ​
* 最小权限原则：指仅向用户和进程授予完成任务所需的最小访问权限的安全原则。
  ​
* LIME（Local Interpretable Model-agnostic Explanations）：一种通过在局部使用一个可解释的模型来近似任意机器学习分类器的预测并解释这些预测的技术。
  ​
* 成员推断攻击：一种旨在确定某一特定数据点是否被用于训练机器学习模型的攻击。
  ​
* MITRE ATLAS： 面向人工智能系统的对抗性威胁态势； 一个关于针对人工智能系统的对抗性战术和技术的知识库。
  ​
* 模型卡 – 模型卡是一份关于人工智能模型的性能、局限性、预期用途以及伦理考量的标准化信息的文档，旨在促进透明度和负责任的人工智能发展。
  ​
* 模型提取攻击：一种攻击方式，攻击者通过反复查询目标模型，在未获授权的情况下创建一个功能上相似的副本。
  ​
* 模型反演：一种试图通过分析模型输出来重建训练数据的攻击。
  ​
* 模型生命周期管理 – AI 模型生命周期管理 是对 AI 模型存在的各个阶段进行监管的过程，包括其设计、开发、部署、监控、维护，以及最终的退役，以确保它保持有效并与目标保持一致。
  ​
* 模型中毒攻击：在训练过程中直接将漏洞或后门引入模型。
  ​
* 模型窃取/盗用：通过反复查询提取专有模型的副本或近似模型。
  ​
* 多智能体系统：由多个相互作用的智能体组成的系统，每个智能体可能具有不同的能力和目标。
  ​
* OPA（Open Policy Agent）：一个开源的策略引擎，能够在整个技术栈中实现统一的策略执行。
  ​
* 隐私保护的机器学习（PPML）：在保护训练数据隐私的同时，用于训练和部署机器学习模型的技术与方法。
  ​
* 提示注入攻击：一种通过在输入中嵌入恶意指令来使模型偏离其预期行为的攻击。
  ​
* RAG（检索增强生成）: 一种通过在生成响应之前从外部知识源检索相关信息来提升大型语言模型的技术。
  ​
* 红队演练：通过模拟对抗性攻击来主动测试 AI 系统以识别漏洞的做法。
  ​
* SBOM（软件物料清单）：一种正式记录，包含用于构建软件或人工智能模型的各种组件的详细信息及其供应链关系。
  ​
* SHAP（SHapley Additive exPlanations）：一种博弈论方法，用于通过计算每个特征对预测结果的贡献来解释任意机器学习模型的输出。
  ​
* 供应链攻击：通过攻击供应链中安全性较低的环节来妥协系统，例如第三方库、数据集或预训练模型。
  ​
* 迁移学习：一种技术，在一个任务上开发的模型被重新用作另一个任务的起点。
  ​
* 向量数据库：一种专门设计用于存储高维向量（嵌入向量）并执行高效相似性搜索的数据库。
  ​
* 漏洞扫描：用于识别软件组件中已知安全漏洞的自动化工具，包括 AI 框架及其依赖项。
  ​
* 水印技术：在 AI 生成的内容中嵌入不可察觉的标记，以追踪其来源或检测是否由 AI 生成。
  ​
* 零日漏洞：在开发者创建并部署补丁之前，攻击者可以利用的此前未知的漏洞。

