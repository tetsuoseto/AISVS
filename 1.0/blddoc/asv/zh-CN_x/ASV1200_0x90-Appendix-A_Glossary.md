# 附录 A：术语表

>本综合词汇表提供了AISVS中使用的关键人工智能、机器学习和安全术语的定义，以确保清晰和共识。

* 对抗样本：一种故意设计的输入，旨在导致人工智能模型出错，通常通过添加人类难以察觉的微妙扰动。
  ​
* 对抗鲁棒性——AI中的对抗鲁棒性指的是模型保持其性能并抵抗那些故意设计用来引起错误的恶意输入所造成的欺骗或操控的能力。
  ​
* 智能体——智能体是使用人工智能代表用户追求目标和完成任务的软件系统。它们表现出推理、规划和记忆能力，并具备一定程度的自主性，能够做出决策、学习和适应。
  ​
* 具代理性的人工智能：能够以一定程度的自主性运行以实现目标的人工智能系统，通常在没有直接人工干预的情况下做出决策和采取行动。
  ​
* 基于属性的访问控制（ABAC）：一种访问控制范式，其授权决策基于用户、资源、操作和环境的属性，并在查询时进行评估。
  ​
* 后门攻击：一种数据投毒攻击，模型被训练成对某些触发器作出特定响应，而在其他情况下表现正常。
  ​
* 偏差：AI模型输出中的系统性错误，可能导致某些群体或特定情境下的不公平或歧视性结果。
  ​
* 偏见利用：一种利用人工智能模型中已知偏见来操纵输出或结果的攻击技术。
  ​
* Cedar：亚马逊用于实现 AI 系统基于属性的访问控制（ABAC）的细粒度权限策略语言和引擎。
  ​
* 思维链：一种通过在生成最终答案之前产生中间推理步骤来提升语言模型推理能力的技术。
  ​
* 断路器：当超过特定风险阈值时，自动停止人工智能系统操作的机制。
  ​
* 数据泄露：通过AI模型输出或行为无意中暴露敏感信息。
  ​
* 数据投毒：故意破坏训练数据以损害模型完整性的行为，通常目的是植入后门或降低性能。
  ​
* 差分隐私——差分隐私是一种数学上严格的框架，用于在保护个体数据主体隐私的同时发布关于数据集的统计信息。它使数据持有者能够共享群体的汇总模式，同时限制泄露关于特定个体的信息。
  ​
* 嵌入：数据（文本、图像等）的密集向量表示，能够在高维空间中捕捉语义意义。
  ​
* 可解释性——人工智能中的可解释性是指AI系统能够为其决策和预测提供人类可理解的原因，从而揭示其内部工作原理的能力。
  ​
* 可解释人工智能（XAI）：通过各种技术和框架设计的人工智能系统，旨在为其决策和行为提供人类可理解的解释。
  ​
* 联邦学习：一种机器学习方法，模型在多个持有本地数据样本的去中心化设备上进行训练，而无需交换数据本身。
  ​
* 护栏：为防止人工智能系统产生有害的、带有偏见的或其他不良输出而实施的约束。
  ​
* 幻觉——人工智能幻觉是指AI模型生成不正确或误导性信息的现象，这些信息既不基于其训练数据，也不符合事实现实。
  ​
* 人类参与循环（HITL）：设计成在关键决策点需要人类监督、验证或干预的系统。
  ​
* 基础设施即代码（IaC）：通过代码管理和配置基础设施，取代手动流程，实现安全扫描和一致的部署。
  ​
* 越狱：用于规避人工智能系统，特别是大型语言模型中的安全防护措施，以生成被禁止内容的技术。
  ​
* 最小特权：一种安全原则，仅授予用户和进程所需的最低访问权限。
  ​
* LIME（局部可解释模型无关解释）：一种通过用可解释模型在局部近似来解释任何机器学习分类器预测的技术。
  ​
* 成员推断攻击：一种旨在确定特定数据点是否被用于训练机器学习模型的攻击。
  ​
* MITRE ATLAS：针对人工智能系统的对抗性威胁格局；一个关于针对AI系统的对抗策略和技术的知识库。
  ​
* 模型卡——模型卡是一种文件，提供关于人工智能模型的性能、局限性、预期用途以及伦理考虑的标准化信息，以促进透明度和负责任的人工智能开发。
  ​
* 模型提取：一种攻击方式，攻击者反复查询目标模型，以未经授权的方式创建一个功能相似的副本。
  ​
* 模型反演：一种通过分析模型输出尝试重建训练数据的攻击。
  ​
* 模型生命周期管理 – AI模型生命周期管理是监督AI模型存在的所有阶段的过程，包括其设计、开发、部署、监控、维护以及最终退役，以确保模型保持有效并符合目标。
  ​
* 模型中毒：在训练过程中直接向模型引入漏洞或后门。
  ​
* 模型窃取/盗用：通过反复查询提取专有模型的副本或近似版本。
  ​
* 多智能体系统：由多个相互作用的人工智能代理组成的系统，每个代理可能具有不同的能力和目标。
  ​
* OPA（开放策略代理）：一个开源的策略引擎，能够实现跨堆栈的统一策略执行。
  ​
* 隐私保护机器学习（PPML）：在保护训练数据隐私的同时训练和部署机器学习模型的技术和方法。
  ​
* 提示注入：一种攻击手段，通过在输入中嵌入恶意指令来覆盖模型的预期行为。
  ​
* RAG（检索增强生成）：一种通过在生成回复之前从外部知识源检索相关信息来增强大型语言模型的技术。
  ​
* 红队演练：通过模拟对抗性攻击来主动测试人工智能系统，以识别其漏洞的做法。
  ​
* SBOM（软件物料清单）：一份正式记录，包含用于构建软件或人工智能模型的各种组件的详细信息及其供应链关系。
  ​
* SHAP（Shapley加法解释）：一种博弈论方法，通过计算每个特征对预测结果的贡献来解释任何机器学习模型的输出。
  ​
* 供应链攻击：通过针对供应链中安全性较低的环节，如第三方库、数据集或预训练模型，来破坏系统。
  ​
* 迁移学习：一种技术，其中为一个任务开发的模型被重新用作第二个任务模型的起点。
  ​
* 向量数据库：一种专门设计用于存储高维向量（嵌入）并执行高效相似度搜索的数据库。
  ​
* 漏洞扫描：自动化工具，用于识别软件组件中的已知安全漏洞，包括人工智能框架和依赖项。
  ​
* 水印技术：在 AI 生成的内容中嵌入不可察觉的标记，以追踪其来源或检测 AI 生成。
  ​
* 零日漏洞：一种先前未知的漏洞，攻击者可以在开发人员创建和部署补丁之前利用该漏洞。

