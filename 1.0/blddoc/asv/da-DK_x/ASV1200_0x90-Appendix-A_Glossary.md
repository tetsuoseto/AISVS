# Appendiks A: Ordliste

Denne omfattende ordliste giver definitioner på nøglebegreber inden for AI, ML og sikkerhed, som anvendes gennem hele AISVS for at sikre klarhed og fælles forståelse.

* Adversarial eksempel: En input bevidst udformet til at få en AI-model til at begå en fejl, ofte ved at tilføje subtile forstyrrelser, som er umærkelige for mennesker.
  ​
* Adversarial Robustness – Adversarial robusthed i AI henviser til en models evne til at opretholde sin ydeevne og modstå at blive narret eller manipuleret af bevidst konstruerede, skadelige input designet til at forårsage fejl.
  ​
* Agent – AI-agenter er software-systemer, der bruger AI til at forfølge mål og udføre opgaver på vegne af brugerne. De udviser ræsonnering, planlægning og hukommelse og har et vist niveau af autonomi til at træffe beslutninger, lære og tilpasse sig.
  ​
* Agentisk AI: AI-systemer, der kan operere med en vis grad af autonomi for at opnå mål, ofte ved at træffe beslutninger og udføre handlinger uden direkte menneskelig indgriben.
  ​
* Attribueret adgangskontrol (ABAC): Et adgangskontrolparadigme, hvor autorisationsbeslutninger baseres på attributter for brugeren, ressourcen, handlingen og miljøet, vurderet på forespørgselstidspunktet.
  ​
* Bagdørsangreb: En type dataforureningsangreb, hvor modellen trænes til at reagere på en bestemt måde på visse triggere, mens den ellers opfører sig normalt.
  ​
* Bias: Systematiske fejl i AI-modeludgange, der kan føre til uretfærdige eller diskriminerende resultater for visse grupper eller i specifikke kontekster.
  ​
* Biasudnyttelse: En angrebsteknik, der udnytter kendte bias i AI-modeller til at manipulere output eller resultater.
  ​
* Cedar: Amazons politik-sprog og motor til detaljerede tilladelser, der bruges til implementering af ABAC for AI-systemer.
  ​
* Chain of Thought: En teknik til at forbedre ræsonnement i sprogmodeller ved at generere mellemliggende ræsonnementstrin, før der produceres et endeligt svar.
  ​
* Kredsløbsafbrydere: Mekanismer, der automatisk stopper AI-systemets drift, når specifikke risikogrænser overskrides.
  ​
* Data Lækage: Utilsigtet eksponering af følsomme oplysninger gennem AI-models output eller adfærd.
  ​
* Dataforgiftning: Den bevidste forvanskning af træningsdata for at kompromittere modellens integritet, ofte for at installere bagdøre eller forringe ydelsen.
  ​
* Differential Privacy – Differential privacy er en matematisk stringent ramme for udgivelse af statistisk information om datasæt samtidig med beskyttelse af privatlivet for individuelle datasubjekter. Det gør det muligt for en dataholder at dele samlede mønstre for gruppen, samtidig med at information, der afsløres om specifikke individer, begrænses.
  ​
* Embedninger: Tætte vektorrepræsentationer af data (tekst, billeder osv.), der fanger semantisk mening i et højdimensionelt rum.
  ​
* Forklarbarhed – Forklarbarhed i AI er evnen hos et AI-system til at give menneskeligt forståelige grunde for dets beslutninger og forudsigelser, hvilket tilbyder indsigt i dets interne funktioner.
  ​
* Forklarbar AI (XAI): AI-systemer designet til at give menneskeligt forståelige forklaringer på deres beslutninger og adfærd gennem forskellige teknikker og rammeværk.
  ​
* Federeret læring: En maskinlæringsmetode, hvor modeller trænes på tværs af flere decentraliserede enheder, der indeholder lokale datasamples, uden at udveksle selve dataene.
  ​
* Guardrails: Begrænsninger implementeret for at forhindre AI-systemer i at producere skadelige, forudindtagede eller på anden måde uønskede output.
  ​
* Hallucination – En AI-hallucination refererer til et fænomen, hvor en AI-model genererer forkert eller vildledende information, som ikke er baseret på dens træningsdata eller faktuelle virkelighed.
  ​
* Human-in-the-Loop (HITL): Systemer designet til at kræve menneskelig overvågning, verifikation eller indgriben ved afgørende beslutningspunkter.
  ​
* Infrastructure as Code (IaC): Administration og levering af infrastruktur gennem kode i stedet for manuelle processer, hvilket muliggør sikkerhedsscanning og konsistente udrulninger.
  ​
* Jailbreak: Teknikker brugt til at omgå sikkerhedsværn i AI-systemer, især i store sprogmodeller, for at producere forbudt indhold.
  ​
* Mindste privilegium: Sikkerhedsprincippet om kun at give de nødvendige minimumsadgangsrettigheder til brugere og processer.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): En teknik til at forklare forudsigelser fra enhver maskinlæringsklassifikator ved at tilnærme den lokalt med en fortolkbar model.
  ​
* Membership Inference Attack: Et angreb, der har til formål at afgøre, om et specifikt datapunkt blev brugt til at træne en maskinlæringsmodel.
  ​
* MITRE ATLAS: Adversarial Threat Landscape for Artificial-Intelligence Systems; en vidensbase over modstanderes taktikker og teknikker mod AI-systemer.
  ​
* Modelkort – Et modelkort er et dokument, der giver standardiseret information om en AI-modells ydeevne, begrænsninger, tilsigtede anvendelser og etiske overvejelser for at fremme gennemsigtighed og ansvarlig AI-udvikling.
  ​
* Modeludtrækning: Et angreb, hvor en modstander gentagne gange forespørger en målmodel for at skabe en funktionelt lignende kopi uden autorisation.
  ​
* Modelinversion: Et angreb, der forsøger at rekonstruere træningsdata ved at analysere modeludgange.
  ​
* Model Lifecycle Management – AI Model Lifecycle Management er processen med at overvåge alle faser af en AI-models eksistens, inklusive dens design, udvikling, implementering, overvågning, vedligeholdelse og endelige udfasning, for at sikre, at den forbliver effektiv og i overensstemmelse med målsætningerne.
  ​
* Modelforgiftning: Indførelse af sårbarheder eller bagdøre direkte i en model under træningsprocessen.
  ​
* Modeltyveri/tyveri: Udtrækning af en kopi eller tilnærmelse af en proprietær model gennem gentagne forespørgsler.
  ​
* Multi-agent System: Et system sammensat af flere interagerende AI-agenter, hver med potentielt forskellige kapabiliteter og mål.
  ​
* OPA (Open Policy Agent): En open-source policy motor, der muliggør en ensartet håndhævelse af politikker på tværs af hele systemet.
  ​
* Privatlivsbeskyttende Maskinlæring (PPML): Teknikker og metoder til at træne og implementere ML-modeller, samtidig med at træningsdataenes privatliv beskyttes.
  ​
* Prompt Injection: Et angreb, hvor ondsindede instruktioner bliver indlejret i input for at tilsidesætte en models tilsigtede adfærd.
  ​
* RAG (Retrieval-Augmented Generation): En teknik, der forbedrer store sprogmodeller ved at hente relevant information fra eksterne videnskilder, før der genereres et svar.
  ​
* Red-Teaming: Praksis med aktivt at teste AI-systemer ved at simulere fjendtlige angreb for at identificere sårbarheder.
  ​
* SBOM (Software Bill of Materials): En formel registrering, der indeholder detaljer og leverandørkædeforhold for forskellige komponenter, der anvendes til at bygge software eller AI-modeller.
  ​
* SHAP (SHapley Additive exPlanations): En spilteoretisk tilgang til at forklare output fra enhver maskinlæringsmodel ved at beregne bidraget fra hver funktion til forudsigelsen.
  ​
* Supply Chain Attack: Kompromittering af et system ved at målrette mindre sikre elementer i dets forsyningskæde, såsom tredjepartsbiblioteker, datasæt eller fortrænede modeller.
  ​
* Transfer Learning: En teknik hvor en model udviklet til en opgave genbruges som udgangspunkt for en model til en anden opgave.
  ​
* Vektordatabase: En specialiseret database designet til at gemme højdimensionelle vektorer (indlejringer) og udføre effektive søgninger efter lighed.
  ​
* Sårbarhedsscanning: Automatiserede værktøjer, der identificerer kendte sikkerhedssårbarheder i softwarekomponenter, herunder AI-rammeværk og afhængigheder.
  ​
* Vandmærkning: Teknikker til at indlejre uigenkendelige markører i AI-genereret indhold for at spore dets oprindelse eller opdage AI-generering.
  ​
* Zero-Day-sårbarhed: En tidligere ukendt sårbarhed, som angribere kan udnytte, før udviklere skaber og implementerer en rettelse.

