# Apêndice A: Glossário

>This glossário abrangente fornece definições de termos-chave de IA, ML e segurança usados ao longo do AISVS para garantir clareza e entendimento comum.

* Exemplo adversarial: Uma entrada cuidadosamente elaborada para fazer com que um modelo de IA cometa um erro, muitas vezes por meio de perturbações sutis imperceptíveis aos humanos.
  ​
* Robustez adversarial – A robustez adversarial na IA refere-se à capacidade de um modelo de manter seu desempenho e resistir a ser enganado ou manipulado por entradas intencionalmente elaboradas e maliciosas, projetadas para causar erros.
  ​
* Agente – Agentes de IA são sistemas de software que usam IA para perseguir objetivos e realizar tarefas em nome dos usuários. Eles demonstram raciocínio, planejamento e memória e têm um nível de autonomia para tomar decisões, aprender e adaptar-se.
  ​
* IA agentiva: sistemas de IA que podem operar com certo grau de autonomia para alcançar objetivos, muitas vezes tomando decisões e agindo sem intervenção humana direta.
  ​
* Controle de Acesso Baseado em Atributos (ABAC): Um paradigma de controle de acesso no qual as decisões de autorização são baseadas em atributos do usuário, do recurso, da ação e do ambiente, avaliados no momento da consulta.
  ​
* Ataque de porta dos fundos: Um tipo de ataque de envenenamento de dados no qual o modelo é treinado para responder de uma forma específica a certos gatilhos, comportando-se de forma normal caso contrário.
  ​
* Viés: Erros sistemáticos nas saídas de modelos de IA que podem levar a resultados injustos ou discriminatórios para certos grupos ou em contextos específicos.
  ​
* Exploração de vieses: Uma técnica de ataque que aproveita vieses conhecidos em modelos de IA para manipular saídas ou resultados.
  ​
* Cedar: a linguagem de políticas e o motor da Amazon para permissões granulares usadas na implementação de ABAC para sistemas de IA.
  ​
* Cadeia de raciocínio: uma técnica para melhorar o raciocínio em modelos de linguagem, gerando etapas intermediárias de raciocínio antes de produzir uma resposta final.
  ​
* Disjuntores: Mecanismos que interrompem automaticamente as operações do sistema de IA quando limites de risco específicos são excedidos.
  ​
* Vazamento de Dados: Exposição não intencional de informações sensíveis por meio das saídas ou do comportamento de modelos de IA.
  ​
* Envenenamento de dados: A corrupção deliberada de dados de treinamento para comprometer a integridade do modelo, muitas vezes para instalar portas dos fundos ou degradar o desempenho.
  ​
* Privacidade Diferencial – A privacidade diferencial é um arcabouço matematicamente rigoroso para divulgar informações estatísticas sobre conjuntos de dados, mantendo a privacidade dos titulares de dados. Isso permite que o detentor dos dados compartilhe padrões agregados do grupo, ao mesmo tempo em que limita as informações que são divulgadas sobre indivíduos específicos.
  ​
* Embeddings: Representações vetoriais densas de dados (texto, imagens, etc.) que capturam o significado semântico em um espaço de alta dimensão.
  ​
* Explicabilidade – A explicabilidade na IA é a capacidade de um sistema de IA de fornecer razões compreensíveis para suas decisões e previsões, oferecendo insights sobre seu funcionamento interno.
  ​
* IA explicável (XAI): sistemas de IA projetados para fornecer explicações compreensíveis por humanos para suas decisões e comportamentos, por meio de várias técnicas e estruturas.
  ​
* Aprendizado Federado: uma abordagem de aprendizado de máquina na qual modelos são treinados em vários dispositivos descentralizados que possuem amostras de dados locais, sem compartilhar os dados em si.
  ​
* Barreiras de proteção: Restrições implementadas para impedir que sistemas de IA produzam saídas prejudiciais, tendenciosas ou de outra forma indesejáveis.
  ​
* Alucinação – Uma alucinação de IA refere-se a um fenômeno no qual um modelo de IA gera informações incorretas ou enganosas que não se baseiam em seus dados de treinamento nem na realidade dos fatos.
  ​
* Humano-no-Loop (HITL): Sistemas projetados para exigir supervisão humana, verificação ou intervenção em pontos cruciais de decisão.
  ​
* Infraestrutura como Código (IaC): Gerenciar e provisionar infraestrutura por meio de código, em vez de processos manuais, permitindo o escaneamento de segurança e implantações consistentes.
  ​
* Jailbreak: Técnicas usadas para contornar as barreiras de segurança em sistemas de IA, particularmente em grandes modelos de linguagem, para produzir conteúdo proibido.
  ​
* Princípio do menor privilégio: o princípio de segurança que consiste em conceder apenas os direitos de acesso mínimos necessários para usuários e processos.
  ​
* LIME (Explicações Locais Interpretáveis e Independentes do Modelo): uma técnica para explicar as previsões de qualquer classificador de aprendizado de máquina, aproximando-o localmente com um modelo interpretável.
  ​
* Ataque de inferência de pertencimento: um ataque cuja finalidade é determinar se um ponto de dados específico foi utilizado para treinar um modelo de aprendizado de máquina.
  ​
* MITRE ATLAS: Panorama de Ameaças Adversariais para Sistemas de Inteligência Artificial; uma base de conhecimento de táticas e técnicas adversariais contra sistemas de inteligência artificial.
  ​
* Cartão de Modelo – É um documento que fornece informações padronizadas sobre o desempenho de um modelo de IA, suas limitações, usos pretendidos e considerações éticas, para promover transparência e o desenvolvimento responsável de IA.
  ​
* Extração de Modelo: um atacante faz consultas repetidas a um modelo-alvo para criar uma cópia funcionalmente semelhante sem autorização.
  ​
* Inversão de Modelo: Um ataque que tenta reconstruir os dados de treinamento analisando as saídas do modelo.
  ​
* Gestão do Ciclo de Vida de Modelos – Gestão do Ciclo de Vida de Modelos de IA é o processo de supervisionar todas as fases da existência de um modelo de IA, incluindo seu design, desenvolvimento, implantação, monitoramento, manutenção e aposentadoria eventual, para garantir que ele permaneça eficaz e alinhado com os objetivos.
  ​
* Envenenamento de Modelo: Introduzir vulnerabilidades ou portas dos fundos diretamente em um modelo durante o processo de treinamento.
  ​
* Roubo/Extração de Modelo: Extrair uma cópia ou aproximação de um modelo proprietário por meio de consultas repetidas.
  ​
* Sistema multiagente: Um sistema composto por múltiplos agentes de IA que interagem entre si, cada um com capacidades e objetivos potencialmente diferentes.
  ​
* OPA (Open Policy Agent): um motor de políticas de código aberto que permite a execução unificada de políticas em toda a pilha de tecnologia.
  ​
* Aprendizado de Máquina com Privacidade Preservada (PPML): Técnicas e métodos para treinar e implantar modelos de Aprendizado de Máquina, protegendo a privacidade dos dados de treinamento.
  ​
* Injeção de Prompt: Um ataque em que instruções maliciosas são inseridas nas entradas para contornar o comportamento pretendido do modelo.
  ​
* RAG (Geração Aumentada por Recuperação): Uma técnica que aprimora modelos de linguagem de grande escala ao recuperar informações relevantes de fontes externas de conhecimento antes de gerar uma resposta.
  ​
* Red-Teaming: a prática de testar ativamente sistemas de IA simulando ataques adversários para identificar vulnerabilidades.
  ​
* SBOM (Lista de Materiais de Software): Um registro formal contendo os detalhes e as relações da cadeia de suprimentos de vários componentes usados na construção de software ou modelos de IA.
  ​
* SHAP (SHapley Additive exPlanations): Uma abordagem teórica de jogos para explicar a saída de qualquer modelo de aprendizado de máquina, calculando a contribuição de cada característica para a previsão.
  ​
* Ataque à Cadeia de Suprimentos: Comprometer um sistema ao mirar elementos menos-seguros em sua cadeia de suprimentos, como bibliotecas de terceiros, conjuntos de dados ou modelos pré-treinados.
  ​
* Aprendizagem por Transferência: Uma técnica na qual um modelo desenvolvido para uma tarefa é reutilizado como ponto de partida para um modelo em uma segunda tarefa.
  ​
* Banco de Dados Vetoriais: Um banco de dados especializado projetado para armazenar vetores de alta dimensão (embeddings) e realizar buscas de similaridade eficientes.
  ​
* Escaneamento de Vulnerabilidades: ferramentas automatizadas que identificam vulnerabilidades de segurança conhecidas em componentes de software, incluindo frameworks de IA e dependências.
  ​
* Marca d'água: Técnicas para incorporar marcadores imperceptíveis em conteúdo gerado por IA para rastrear sua origem ou detectar a geração por IA.
  ​
* Vulnerabilidade zero-day: uma vulnerabilidade previamente desconhecida que os atacantes podem explorar antes que os desenvolvedores criem e lancem um patch.

