# پیوست الف: واژه‌نامه

این واژه‌نامه جامع، تعاریف اصطلاحات کلیدی هوش مصنوعی، یادگیری ماشین، و امنیت را که در سراسر AISVS به کار رفته‌اند ارائه می‌دهد تا وضوح و درک مشترک را تضمین کند.

* مثال خصمانه: ورودی که عمداً طراحی شده تا مدل هوش مصنوعی را به اشتباه انداخته و معمولاً با افزودن اختلالات ظریف که برای انسان‌ها غیرقابل تشخیص هستند، انجام می‌شود.
  ​
* استحکام مقابله‌ای – استحکام مقابله‌ای در هوش مصنوعی به توانایی یک مدل در حفظ عملکرد خود و مقاومت در برابر فریب دادن یا دستکاری با ورودی‌های عمدی و مخرب طراحی شده برای ایجاد خطا اشاره دارد.
  ​
* عامل – عوامل هوش مصنوعی سیستم‌های نرم‌افزاری هستند که از هوش مصنوعی برای دنبال کردن اهداف و انجام وظایف به نمایندگی از کاربران استفاده می‌کنند. این عوامل دارای توانایی استدلال، برنامه‌ریزی و حافظه هستند و سطحی از خودمختاری برای تصمیم‌گیری، یادگیری و تطبیق دارند.
  ​
* هوش مصنوعی عاملی: سیستم‌های هوش مصنوعی که می‌توانند با درجه‌ای از خودمختاری عمل کنند تا به اهداف دست یابند، که اغلب تصمیم‌گیری می‌کنند و بدون مداخله مستقیم انسان اقدامات انجام می‌دهند.
  ​
* کنترل دسترسی مبتنی بر ویژگی (ABAC): یک پارادایم کنترل دسترسی که در آن تصمیمات اجازه دسترسی بر اساس ویژگی‌های کاربر، منبع، عمل و محیط، در زمان پرس‌وجو ارزیابی می‌شوند.
  ​
* حمله درب پشتی: نوعی از حملات مسموم‌سازی داده که در آن مدل به گونه‌ای آموزش داده می‌شود که به محرک‌های خاص به طور مشخص پاسخ دهد در حالی که در سایر مواقع به طور عادی رفتار می‌کند.
  ​
* جانبداری: خطاهای سیستماتیک در خروجی مدل هوش مصنوعی که می‌توانند به نتایج ناعادلانه یا تبعیض‌آمیز برای گروه‌های خاص یا در زمینه‌های خاص منجر شوند.
  ​
* بهره‌برداری از سوگیری: یک تکنیک حمله که از سوگیری‌های شناخته‌شده در مدل‌های هوش مصنوعی برای دستکاری خروجی‌ها یا نتایج استفاده می‌کند.
  ​
* سدار: زبان سیاست و موتور آمازون برای مجوزهای دقیق که در پیاده‌سازی ABAC برای سیستم‌های هوش مصنوعی استفاده می‌شود.
  ​
* زنجیره تفکر: تکنیکی برای بهبود استدلال در مدل‌های زبان با تولید مراحل استدلال میانی قبل از ارائه پاسخ نهایی.
  ​
* مدار شکن‌ها: مکانیزم‌هایی که به طور خودکار عملیات سیستم هوش مصنوعی را زمانی که آستانه‌های خطر خاصی فراتر می‌رود، متوقف می‌کنند.
  ​
* نشتی داده: افشای ناخواسته اطلاعات حساس از طریق خروجی‌ها یا رفتار مدل هوش مصنوعی.
  ​
* آلودگی داده: فساد عمدی داده‌های آموزشی برای به خطر انداختن صحت مدل، معمولاً برای نصب درهای پشتی یا کاهش عملکرد.
  ​
* حریم خصوصی تفاضلی – حریم خصوصی تفاضلی چارچوبی ریاضیاتی دقیق برای انتشار اطلاعات آماری درباره مجموعه داده‌هاست در حالی که حریم خصوصی افراد حاضر در داده‌ها را حفظ می‌کند. این روش به دارنده داده اجازه می‌دهد الگوهای کلی گروه را به اشتراک بگذارد در حالی که اطلاعات فاش‌شده درباره افراد خاص را محدود می‌کند.
  ​
* تعبیه‌ها: نمایش‌های برداری متراکم از داده‌ها (متن، تصاویر و غیره) که معنی معنایی را در فضای با ابعاد بالا ضبط می‌کنند.
  ​
* قابل تفسیر بودن – قابل تفسیر بودن در هوش مصنوعی به توانایی یک سیستم هوش مصنوعی برای ارائه دلایل قابل فهم توسط انسان برای تصمیمات و پیش‌بینی‌های خود گفته می‌شود، که بینشی نسبت به عملکرد داخلی آن ارائه می‌دهد.
  ​
* هوش مصنوعی قابل توضیح (XAI): سیستم‌های هوش مصنوعی که به گونه‌ای طراحی شده‌اند تا از طریق تکنیک‌ها و چارچوب‌های مختلف، توضیحات قابل فهم برای انسان درباره تصمیمات و رفتارهای خود ارائه دهند.
  ​
* یادگیری فدراسیون شده: رویکردی در یادگیری ماشین که در آن مدل‌ها به‌صورت توزیع‌شده و بر روی چندین دستگاه غیرمتمرکز که نمونه‌های داده محلی را نگه می‌دارند، آموزش داده می‌شوند، بدون اینکه داده‌ها به خودی خود تبادل شوند.
  ​
* گاردریل‌ها: محدودیت‌هایی که برای جلوگیری از تولید خروجی‌های مضر، مغرضانه یا به نحوی ناخواسته توسط سیستم‌های هوش مصنوعی اعمال می‌شوند.
  ​
* توهم – توهم در هوش مصنوعی به مفهوم پدیده‌ای اشاره دارد که در آن یک مدل هوش مصنوعی اطلاعات نادرست یا گمراه‌کننده‌ای تولید می‌کند که بر اساس داده‌های آموزشی یا واقعیت‌های عینی نیست.
  ​
* انسان-در-حلقه (HITL): سیستم‌هایی که به گونه‌ای طراحی شده‌اند که نیازمند نظارت، تایید، یا دخالت انسان در نقاط بحرانی تصمیم‌گیری باشند.
  ​
* زیرساخت به عنوان کد (IaC): مدیریت و تأمین زیرساخت از طریق کد به جای فرآیندهای دستی، که امکان اسکن امنیتی و استقرارهای یکنواخت را فراهم می‌کند.
  ​
* Jailbreak: تکنیک‌هایی که برای دور زدن محدودیت‌های ایمنی در سیستم‌های هوش مصنوعی، به ویژه در مدل‌های زبان بزرگ، جهت تولید محتوای ممنوعه استفاده می‌شوند.
  ​
* کمترین امتیاز: اصل امنیتی اعطای حداقل دسترسی‌های لازم به کاربران و فرآیندها.
  ​
* LIME (توضیحات محلی مدل‌ناشناس قابل تفسیر): یک تکنیک برای توضیح پیش‌بینی‌های هر طبقه‌بندی‌کننده یادگیری ماشین است که با تقریب دادن محلی آن با یک مدل قابل تفسیر انجام می‌شود.
  ​
* حمله استنتاج عضویت: حمله‌ای که هدف آن تعیین این است که آیا یک نقطه داده خاص برای آموزش یک مدل یادگیری ماشین استفاده شده است یا نه.
  ​
* MITRE ATLAS: چشم‌انداز تهدیدات خصمانه برای سیستم‌های هوش مصنوعی؛ یک پایگاه دانش از تاکتیک‌ها و تکنیک‌های خصمانه علیه سیستم‌های هوش مصنوعی.
  ​
* کارت مدل – کارت مدل یک سند است که اطلاعات استاندارد شده درباره عملکرد یک مدل هوش مصنوعی، محدودیت‌ها، کاربردهای مورد نظر و ملاحظات اخلاقی آن را فراهم می‌کند تا شفافیت و توسعه مسئولانه هوش مصنوعی را ترویج دهد.
  ​
* استخراج مدل: حمله‌ای که در آن یک مهاجم به طور مکرر از مدل هدف پرسش می‌کند تا یک کپی عملکردی مشابه بدون اجازه ایجاد کند.
  ​
* وارونگی مدل: حمله‌ای که تلاش می‌کند با تحلیل خروجی‌های مدل، داده‌های آموزشی را بازسازی کند.
  ​
* مدیریت چرخه عمر مدل – مدیریت چرخه عمر مدل هوش مصنوعی فرآیند نظارت بر تمام مراحل وجود یک مدل هوش مصنوعی است که شامل طراحی، توسعه، استقرار، پایش، نگهداری و در نهایت بازنشستگی آن می‌شود تا اطمینان حاصل شود که مدل همچنان مؤثر و هماهنگ با اهداف باقی می‌ماند.
  ​
* سم‌پاشی مدل: وارد کردن آسیب‌پذیری‌ها یا درهای پشتی به طور مستقیم در یک مدل در طول فرآیند آموزش.
  ​
* دزدی/سرقت مدل: استخراج یک نسخه یا تقریب از یک مدل اختصاصی از طریق پرسش‌های مکرر.
  ​
* سیستم چندعامله: سیستمی متشکل از چندین عامل هوش مصنوعی که با یکدیگر تعامل دارند و هر کدام ممکن است قابلیت‌ها و اهداف متفاوتی داشته باشند.
  ​
* OPA (Open Policy Agent): یک موتور سیاست متن‌باز است که اجرای یکنواخت سیاست‌ها را در سراسر پشته امکان‌پذیر می‌کند.
  ​
* یادگیری ماشین حفظ حریم خصوصی (PPML): تکنیک‌ها و روش‌هایی برای آموزش و استقرار مدل‌های یادگیری ماشین در حالی که حریم خصوصی داده‌های آموزشی حفظ می‌شود.
  ​
* تزریق پرامپت: حمله‌ای که در آن دستورات مخرب در ورودی‌ها جاسازی می‌شوند تا رفتار مورد نظر مدل را نادیده بگیرند.
  ​
* RAG (تولید تقویت‌شده با بازیابی): تکنیکی که مدل‌های زبان بزرگ را با بازیابی اطلاعات مرتبط از منابع دانش خارجی قبل از تولید پاسخ تقویت می‌کند.
  ​
* تیم قرمز: عملیاتی است که در آن به طور فعال سیستم‌های هوش مصنوعی با شبیه‌سازی حملات خصمانه تست می‌شوند تا آسیب‌پذیری‌ها شناسایی شوند.
  ​
* SBOM (فهرست مواد نرم‌افزار): یک رکورد رسمی شامل جزئیات و روابط زنجیره تامین اجزای مختلف استفاده شده در ساخت نرم‌افزار یا مدل‌های هوش مصنوعی.
  ​
* SHAP (توضیحات جمع‌پذیر شاپلی): یک رویکرد نظریه بازی‌ها برای تبیین خروجی هر مدل یادگیری ماشین از طریق محاسبه سهم هر ویژگی در پیش‌بینی.
  ​
* حمله زنجیره تامین: به خطر انداختن یک سیستم با هدف قرار دادن عناصر کم‌امنیت‌تر در زنجیره تأمین آن، مانند کتابخانه‌های شخص ثالث، مجموعه داده‌ها، یا مدل‌های از پیش آموزش‌دیده شده.
  ​
* یادگیری انتقالی: تکنیکی که در آن یک مدل توسعه یافته برای یک وظیفه به عنوان نقطه شروع برای مدلی در وظیفه دوم دوباره مورد استفاده قرار می‌گیرد.
  ​
* پایگاه داده وکتور: یک پایگاه داده تخصصی طراحی شده برای ذخیره بردارهای با بعد بالا (امبدینگ‌ها) و انجام جستجوهای مشابهت بهینه.
  ​
* اسکن آسیب‌پذیری: ابزارهای خودکاری که آسیب‌پذیری‌های امنیتی شناخته شده در اجزای نرم‌افزاری، از جمله چارچوب‌های هوش مصنوعی و وابستگی‌ها را شناسایی می‌کنند.
  ​
* واترمارکینگ: تکنیک‌هایی برای قرار دادن نشانگرهای غیرقابل تشخیص در محتوای تولید شده توسط هوش مصنوعی به منظور ردیابی منبع آن یا شناسایی تولید توسط هوش مصنوعی.
  ​
* آسیب‌پذیری روز صفر: یک آسیب‌پذیری ناشناخته قبلی که مهاجمان می‌توانند قبل از اینکه توسعه‌دهندگان وصله‌ای ایجاد و مستقر کنند، از آن بهره‌برداری کنند.

