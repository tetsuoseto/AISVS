# ضمیمه A: واژه‌نامه

این واژه‌نامه جامع تعریف‌های اصطلاحات کلیدی هوش مصنوعی، یادگیری ماشینی و امنیت را که در سراسر AISVS استفاده شده‌اند فراهم می‌کند تا وضوح و درک مشترک تضمین شود.

* مثال مخرب: ورودی‌ای که به طور عمدی طراحی شده است تا یک مدل هوش مصنوعی را به اشتباه اندازد، معمولاً با افزودن تغییرات ظریف که برای انسان‌ها قابل درک نیستند.
  ​
* استحکام مقابل حملات خصمانه – استحکام مقابل حملات خصمانه در هوش مصنوعی به توانایی یک مدل برای حفظ عملکرد خود و مقاوم بودن در برابر فریب دادن یا دستکاری توسط ورودی‌های عمدی و مخرب طراحی شده برای ایجاد خطا اشاره دارد.
  ​
* عامل – عامل‌های هوش مصنوعی سیستم‌های نرم‌افزاری هستند که از هوش مصنوعی برای دنبال کردن اهداف و انجام وظایف به نمایندگی از کاربران استفاده می‌کنند. آنها دارای قابلیت‌های استدلال، برنامه‌ریزی و حافظه هستند و سطحی از استقلال دارند تا تصمیم گیری کنند، یاد بگیرند و سازگار شوند.
  ​
* هوش مصنوعی عاملی: سیستم‌های هوش مصنوعی که قادر به عمل کردن با درجه‌ای از خودمختاری برای دستیابی به اهداف هستند و اغلب بدون دخالت مستقیم انسان تصمیم‌گیری و اقدام می‌کنند.
  ​
* کنترل دسترسی مبتنی بر صفات (ABAC): یک الگوی کنترل دسترسی که در آن تصمیمات اعطای دسترسی بر اساس صفات کاربر، منبع، عمل و محیط، که در زمان پرس و جو ارزیابی می‌شوند، اتخاذ می‌گردد.
  ​
* حمله درب پشتی: نوعی حمله مسموم‌سازی داده که در آن مدل طوری آموزش داده می‌شود که به صورت خاص به محرک‌های خاص پاسخ دهد در حالی که در غیر این صورت به طور عادی رفتار می‌کند.
  ​
* سوگیری: خطاهای سیستماتیک در خروجی‌های مدل‌های هوش مصنوعی که می‌تواند به نتایج ناعادلانه یا تبعیض‌آمیز برای گروه‌های خاص یا در زمینه‌های مشخص منجر شود.
  ​
* بهره‌برداری از تبعیض: یک تکنیک حمله که از تعصبات شناخته‌شده در مدل‌های هوش مصنوعی برای دستکاری خروجی‌ها یا نتایج استفاده می‌کند.
  ​
* سدار: زبان سیاست و موتور آمازون برای مجوزهای دقیق که در پیاده‌سازی کنترل دسترسی مبتنی بر ویژگی (ABAC) برای سیستم‌های هوش مصنوعی استفاده می‌شود.
  ​
* زنجیره تفکر: تکنیکی برای بهبود استدلال در مدل‌های زبانی با تولید مراحل میانی استدلال قبل از ارائه پاسخ نهایی.
  ​
* قطع‌کننده‌های مدار: مکانیزم‌هایی که به‌طور خودکار عملیات سیستم هوش مصنوعی را زمانی که آستانه‌های ریسک خاصی عبور می‌کنند، متوقف می‌کنند.
  ​
* نشت داده: افشای ناخواسته اطلاعات حساس از طریق خروجی‌ها یا رفتار مدل هوش مصنوعی.
  ​
* آلوده‌سازی داده‌ها: خرابکاری عمدی در داده‌های آموزشی برای به خطر انداختن صحت مدل، که اغلب به منظور نصب درهای پشتی یا کاهش عملکرد انجام می‌شود.
  ​
* حریم خصوصی تفاضلی – حریم خصوصی تفاضلی یک چارچوب دقیق ریاضی برای انتشار اطلاعات آماری درباره مجموعه داده‌ها است که در عین حال از حفظ حریم خصوصی افراد داده شده محافظت می‌کند. این چارچوب به دارنده داده اجازه می‌دهد الگوهای تجمعی گروه را به اشتراک بگذارد در حالی که اطلاعات فاش شده درباره افراد خاص را محدود می‌کند.
  ​
* بردارهای جاسازی‌شده: نمایش‌های برداری متراکم داده‌ها (متن، تصاویر و غیره) که معنای معنایی را در یک فضای با ابعاد بالا به‌دست می‌دهند.
  ​
* قابلیت توضیح‌پذیری – قابلیت توضیح‌پذیری در هوش مصنوعی به توانایی یک سیستم هوش مصنوعی برای ارائه دلایل قابل فهم برای انسان درباره تصمیمات و پیش‌بینی‌های خود گفته می‌شود که بینشی درباره عملکرد داخلی آن فراهم می‌کند.
  ​
* هوش مصنوعی قابل توضیح (XAI): سیستم‌های هوش مصنوعی طراحی شده برای ارائه توضیحات قابل فهم برای انسان درباره تصمیمات و رفتارهای خود از طریق تکنیک‌ها و چارچوب‌های مختلف.
  ​
* یادگیری فدراسیون: رویکردی در یادگیری ماشین که در آن مدل‌ها به صورت توزیع‌شده در چندین دستگاه غیرمتمرکز که نمونه‌های داده محلی را نگهداری می‌کنند، آموزش داده می‌شوند، بدون اینکه داده‌ها مستقیماً تبادل شوند.
  ​
* نرده‌های حفاظتی: محدودیت‌هایی که برای جلوگیری از تولید خروجی‌های مضر، جانبدارانه یا به‌گونه‌ای نامطلوب توسط سیستم‌های هوش مصنوعی اعمال می‌شوند.
  ​
* توهم – توهم در هوش مصنوعی به پدیده‌ای اشاره دارد که در آن یک مدل هوش مصنوعی اطلاعات نادرست یا گمراه‌کننده‌ای تولید می‌کند که بر اساس داده‌های آموزشی یا واقعیت‌های حقیقی نیست.
  ​
* انسان در حلقه (HITL): سیستم‌هایی که به گونه‌ای طراحی شده‌اند که نیازمند نظارت، تأیید یا مداخله انسانی در نقاط تصمیم‌گیری حیاتی هستند.
  ​
* زیرساخت به عنوان کد (IaC): مدیریت و فراهم‌آوری زیرساخت‌ها از طریق کد به جای فرآیندهای دستی، که امکان اسکن امنیتی و استقرارهای سازگار را فراهم می‌کند.
  ​
* Jailbreak: تکنیک‌هایی که برای دور زدن محافظ‌های ایمنی در سیستم‌های هوش مصنوعی، به‌ویژه در مدل‌های زبان بزرگ، استفاده می‌شوند تا محتوای ممنوعه تولید کنند.
  ​
* حداقل دسترسی: اصل امنیتی اعطای تنها حداقل حقوق دسترسی لازم برای کاربران و فرایندها.
  ​
* LIME (توضیحات قابل تفسیر محلی و مستقل از مدل): روشی برای توضیح پیش‌بینی‌های هر طبقه‌بند یادگیری ماشین با تقریب زدن آن به‌صورت محلی با یک مدل قابل تفسیر.
  ​
* حمله استنباط عضویت: حمله‌ای که هدف آن تعیین این است که آیا یک داده خاص برای آموزش مدل یادگیری ماشین استفاده شده است یا خیر.
  ​
* MITRE ATLAS: چشم‌انداز تهدیدهای خصمانه برای سیستم‌های هوش مصنوعی؛ یک پایگاه دانش از تاکتیک‌ها و تکنیک‌های خصمانه علیه سیستم‌های هوش مصنوعی.
  ​
* کارت مدل – کارت مدل یک سند است که اطلاعات استاندارد شده‌ای درباره عملکرد، محدودیت‌ها، کاربردهای مورد نظر و ملاحظات اخلاقی یک مدل هوش مصنوعی ارائه می‌دهد تا شفافیت و توسعه مسئولانه هوش مصنوعی را ترویج کند.
  ​
* استخراج مدل: حمله‌ای که در آن دشمن به طور مکرر از یک مدل هدف پرس‌وجو می‌کند تا یک نسخه عملکردی مشابه بدون مجوز ایجاد کند.
  ​
* وارون‌سازی مدل: حمله‌ای که تلاش می‌کند با تحلیل خروجی‌های مدل، داده‌های آموزشی را بازسازی کند.
  ​
* مدیریت چرخه عمر مدل – مدیریت چرخه عمر مدل هوش مصنوعی فرآیند نظارت بر تمام مراحل وجود یک مدل هوش مصنوعی است، از جمله طراحی، توسعه، استقرار، مانیتورینگ، نگهداری و در نهایت بازنشستگی آن، تا اطمینان حاصل شود که مدل موثر باقی می‌ماند و با اهداف هماهنگ است.
  ​
* مسمومیت مدل: وارد کردن آسیب‌پذیری‌ها یا درهای پشتی مستقیماً به مدل در حین فرایند آموزش.
  ​
* سرقت/ربودن مدل: استخراج نسخه‌ای یا تقریب از یک مدل اختصاصی از طریق پرس‌وجوهای مکرر.
  ​
* سیستم چندعامله: سیستمی متشکل از چندین عامل هوش مصنوعی تعاملی است که هر کدام ممکن است قابلیت‌ها و اهداف متفاوتی داشته باشند.
  ​
* OPA (Open Policy Agent): یک موتور سیاست متن‌باز است که اجرای یکنواخت سیاست‌ها را در تمام لایه‌های سیستم امکان‌پذیر می‌سازد.
  ​
* یادگیری ماشین حفظ حریم خصوصی (PPML): تکنیک‌ها و روش‌هایی برای آموزش و استقرار مدل‌های یادگیری ماشین در حالی که حریم خصوصی داده‌های آموزشی حفظ می‌شود.
  ​
* تزریق فرمان: حمله‌ای که در آن دستورات مخرب در ورودی‌ها جاسازی می‌شوند تا رفتار مورد نظر مدل را نادیده بگیرند.
  ​
* RAG (تولید تقویت‌شده با واکشی): یک تکنیک که مدل‌های بزرگ زبان را با واکشی اطلاعات مرتبط از منابع دانش خارجی قبل از تولید پاسخ، ارتقا می‌دهد.
  ​
* رد-تیمینگ: عملی است که در آن سیستم‌های هوش مصنوعی به‌صورت فعال با شبیه‌سازی حملات مخرب آزمایش می‌شوند تا آسیب‌پذیری‌ها شناسایی شوند.
  ​
* SBOM (فهرست مواد نرم‌افزاری): یک سند رسمی که شامل جزئیات و روابط زنجیره تأمین اجزای مختلف استفاده‌شده در ساخت نرم‌افزار یا مدل‌های هوش مصنوعی است.
  ​
* SHAP (توضیحات افزایشی شاپلی): رویکردی مبتنی بر نظریه بازی‌ها برای توضیح خروجی هر مدل یادگیری ماشین با محاسبه سهم هر ویژگی در پیش‌بینی.
  ​
* حمله زنجیره تامین: نفوذ به سیستم با هدف قرار دادن عناصر کم‌امن‌تر در زنجیره تامین آن، مانند کتابخانه‌های شخص ثالث، مجموعه داده‌ها یا مدل‌های از قبل آموزش‌دیده شده.
  ​
* یادگیری انتقالی: تکنیکی که در آن یک مدل توسعه یافته برای یک وظیفه به عنوان نقطه شروع برای مدل در وظیفه دوم مورد استفاده مجدد قرار می‌گیرد.
  ​
* پایگاه داده برداری: یک پایگاه داده تخصصی طراحی شده برای ذخیره بردارهای با ابعاد بالا (تعبیه‌ها) و انجام جستجوهای شباهت کارآمد.
  ​
* اسکن آسیب‌پذیری: ابزارهای خودکار که آسیب‌پذیری‌های امنیتی شناخته‌شده در اجزای نرم‌افزاری، از جمله چارچوب‌ها و وابستگی‌های هوش مصنوعی را شناسایی می‌کنند.
  ​
* واترمارکینگ: تکنیک‌هایی برای جاسازی علائم نامحسوس در محتوای تولید شده توسط هوش مصنوعی به منظور ردیابی منبع آن یا شناسایی تولید توسط هوش مصنوعی.
  ​
* آسیب‌پذیری صفر روز: یک آسیب‌پذیری ناشناخته قبلی که مهاجمان می‌توانند قبل از اینکه توسعه‌دهندگان پچ ایجاد و اجرا کنند، از آن سوءاستفاده کنند.

