# ضمیمه A: فرهنگ واژگان

این فرهنگ جامع تعاریف اصطلاحات کلیدی هوش مصنوعی، یادگیری ماشین و امنیت را که در سراسر AISVS استفاده شده‌اند، به منظور اطمینان از وضوح و درک مشترک ارائه می‌دهد.

* مثال خصمانه: ورودیی که به طور عمدی طراحی شده است تا مدل هوش مصنوعی را به اشتباه اندازد، اغلب با افزودن تغییرات ظریف که برای انسان‌ها قابل درک نیستند.
  ​
* استحکام مقابله‌ای – استحکام مقابله‌ای در هوش مصنوعی به توانایی یک مدل برای حفظ عملکرد خود و مقاومت در برابر فریب خوردن یا دستکاری توسط ورودی‌های عمدی و مخرب طراحی شده برای ایجاد خطا اشاره دارد.
  ​
* عامل – عامل‌های هوش مصنوعی سیستم‌های نرم‌افزاری هستند که از هوش مصنوعی برای دنبال کردن اهداف و انجام وظایف به نمایندگی از کاربران استفاده می‌کنند. آن‌ها توانایی استدلال، برنامه‌ریزی و حافظه را نشان می‌دهند و دارای سطحی از خودمختاری برای اتخاذ تصمیم، یادگیری و سازگاری هستند.
  ​
* هوش مصنوعی عاملیت‌دار: سیستم‌های هوش مصنوعی که می‌توانند با درجه‌ای از خودمختاری عمل کنند تا اهدافی را محقق سازند، که اغلب تصمیم‌گیری و اقدام بدون دخالت مستقیم انسان را شامل می‌شوند.
  ​
* کنترل دسترسی مبتنی بر ویژگی (ABAC): یک الگوی کنترل دسترسی که تصمیمات مجوزدهی بر اساس ویژگی‌های کاربر، منابع، عمل و محیط، که در زمان پرس‌وجو ارزیابی می‌شوند، اتخاذ می‌شود.
  ​
* حمله درب‌پشتی: نوعی حمله مسموم‌سازی داده که در آن مدل طوری آموزش داده می‌شود که به طور خاص به محرک‌های مشخصی پاسخ دهد در حالی که در سایر موارد رفتار عادی دارد.
  ​
* سوگیری: خطاهای سیستماتیکی در خروجی‌های مدل هوش مصنوعی که می‌تواند منجر به نتایج ناعادلانه یا تبعیض‌آمیز برای گروه‌های خاص یا در زمینه‌های مشخص شود.
  ​
* استثمار تعصب: یک تکنیک حمله که از تعصبات شناخته شده در مدل‌های هوش مصنوعی برای دستکاری خروجی‌ها یا نتایج استفاده می‌کند.
  ​
* Cedar: زبان سیاست آمازون و موتور آن برای مجوزهای دقیق که در پیاده‌سازی ABAC برای سیستم‌های هوش مصنوعی استفاده می‌شود.
  ​
* زنجیره تفکر: تکنیکی برای بهبود استدلال در مدل‌های زبانی با تولید گام‌های میانی استدلال قبل از ارائه پاسخ نهایی.
  ​
* قطع‌کننده‌های مدار: مکانیزم‌هایی که به‌طور خودکار عملیات سیستم هوش مصنوعی را زمانی که آستانه‌های خطر مشخصی فراتر رود، متوقف می‌کنند.
  ​
* نشت داده: افشای ناخواسته اطلاعات حساس از طریق خروجی‌ها یا رفتار مدل هوش مصنوعی.
  ​
* سرریز داده: فساد عمدی داده‌های آموزشی به منظور به خطر انداختن یکپارچگی مدل، که اغلب برای نصب درهای پشتی یا کاهش کارایی انجام می‌شود.
  ​
* حریم خصوصی تفاضلی – حریم خصوصی تفاضلی چارچوبی ریاضیاتی دقیق برای ارائه اطلاعات آماری درباره داده‌ها است در حالی که حریم خصوصی افراد داده‌شده را محافظت می‌کند. این امکان را به دارنده داده می‌دهد تا الگوهای تجمیعی گروه را به اشتراک بگذارد و در عین حال اطلاعات منتشر شده درباره افراد خاص را محدود کند.
  ​
* تعبیه‌ها: نمایش‌های برداری متراکم از داده‌ها (متن، تصاویر و غیره) که معنای مفهومی را در یک فضای با ابعاد بالا در بر می‌گیرند.
  ​
* قابلیت توضیح‌پذیری – قابلیت توضیح‌پذیری در هوش مصنوعی به معنای توانایی یک سیستم هوش مصنوعی در ارائه دلایل قابل فهم برای انسان‌ها درباره تصمیمات و پیش‌بینی‌های خود است، که بینش‌هایی در مورد عملکرد داخلی آن ارائه می‌دهد.
  ​
* هوش مصنوعی قابل توضیح (XAI): سیستم‌های هوش مصنوعی طراحی شده برای ارائه توضیحات قابل فهم برای انسان درباره تصمیمات و رفتارهای خود از طریق تکنیک‌ها و چارچوب‌های مختلف.
  ​
* یادگیری مشارکتی: رویکردی در یادگیری ماشین که در آن مدل‌ها بر روی چندین دستگاه غیرمتمرکز که نمونه‌های داده محلی را نگه می‌دارند، آموزش داده می‌شوند، بدون اینکه داده‌ها به خودی خود تبادل شوند.
  ​
* گاردریل‌ها: محدودیت‌هایی که برای جلوگیری از تولید خروجی‌های مضر، دارای تعصب یا به هر شکل نامطلوب توسط سیستم‌های هوش مصنوعی اعمال می‌شوند.
  ​
* هذیان – هذیان هوش مصنوعی به پدیده‌ای اشاره دارد که در آن یک مدل هوش مصنوعی اطلاعات نادرست یا گمراه‌کننده‌ای تولید می‌کند که بر اساس داده‌های آموزشی یا واقعیت‌های عینی نیست.
  ​
* انسان در حلقه (HITL): سیستم‌هایی که طراحی شده‌اند تا نظارت، تایید، یا مداخله انسانی در نقاط تصمیم‌گیری حیاتی را نیاز داشته باشند.
  ​
* زیرساخت به‌عنوان کد (IaC): مدیریت و ارائه زیرساخت از طریق کد به جای فرآیندهای دستی، که امکان اسکن امنیتی و استقرارهای یکنواخت را فراهم می‌کند.
  ​
* Jailbreak: تکنیک‌هایی که برای دور زدن حفاظ‌های ایمنی در سیستم‌های هوش مصنوعی، به‌ویژه در مدل‌های زبان بزرگ، برای تولید محتوای ممنوعه استفاده می‌شوند.
  ​
* کمترین سطح دسترسی: اصل امنیتی که تنها حداقل حقوق دسترسی لازم را برای کاربران و فرآیندها اعطا می‌کند.
  ​
* LIME (توضیحات قابل تفسیر محلی مستقل از مدل): روشی برای توضیح پیش‌بینی‌های هر دسته‌بندی‌کننده یادگیری ماشین از طریق تقریب محلی آن با یک مدل قابل تفسیر.
  ​
* حمله استنتاج عضویت: حمله‌ای که هدف آن تعیین این است که آیا یک نقطه داده خاص برای آموزش یک مدل یادگیری ماشین استفاده شده است یا خیر.
  ​
* MITRE ATLAS: چشم‌انداز تهدیدات مهاجمان برای سیستم‌های هوش مصنوعی؛ یک پایگاه دانش از تاکتیک‌ها و تکنیک‌های مهاجمان علیه سیستم‌های هوش مصنوعی.
  ​
* کارت مدل – کارت مدل یک سند است که اطلاعات استاندارد شده‌ای در مورد عملکرد، محدودیت‌ها، کاربردهای مورد نظر و ملاحظات اخلاقی یک مدل هوش مصنوعی ارائه می‌دهد تا شفافیت و توسعه مسئولانه هوش مصنوعی را ترویج کند.
  ​
* استخراج مدل: حمله‌ای که در آن یک مهاجم به طور مکرر مدل هدف را پرس‌وجو می‌کند تا نسخه‌ای با عملکرد مشابه بدون مجوز ایجاد کند.
  ​
* وارون‌سازی مدل: حمله‌ای که با تحلیل خروجی‌های مدل سعی در بازسازی داده‌های آموزشی دارد.
  ​
* مدیریت چرخه عمر مدل – مدیریت چرخه عمر مدل هوش مصنوعی فرآیند نظارت بر تمام مراحل وجود یک مدل هوش مصنوعی است، از جمله طراحی، توسعه، استقرار، پایش، نگهداری و نهایتاً بازنشستگی آن، به منظور اطمینان از اینکه مدل موثر باقی می‌ماند و با اهداف همسو است.
  ​
* آلوده‌سازی مدل: وارد کردن آسیب‌پذیری‌ها یا درهای پشتی به‌طور مستقیم در مدل در فرآیند آموزش.
  ​
* دزدیدن/سرقت مدل: استخراج نسخه یا تقریب یک مدل اختصاصی از طریق پرسش‌های تکراری.
  ​
* سیستم چندعاملی: سیستمی متشکل از چند عامل هوش مصنوعی که هر یک ممکن است دارای قابلیت‌ها و اهداف متفاوتی باشند.
  ​
* OPA (عامل سیاست‌باز): یک موتور سیاست متن‌باز است که امکان اجرای یکپارچه سیاست‌ها را در سراسر پشته فراهم می‌کند.
  ​
* یادگیری ماشین حفظ حریم خصوصی (PPML): تکنیک‌ها و روش‌هایی برای آموزش و پیاده‌سازی مدل‌های یادگیری ماشین در حالی که حریم خصوصی داده‌های آموزشی حفظ می‌شود.
  ​
* تزریق فرمان: حمله‌ای که در آن دستورهای مخرب در ورودی‌ها جاسازی می‌شوند تا رفتار مورد نظر مدل را بازنویسی کنند.
  ​
* RAG (تولید افزوده‌شده با بازیابی): تکنیکی که مدل‌های زبان بزرگ را با بازیابی اطلاعات مرتبط از منابع دانش خارجی قبل از تولید پاسخ، بهبود می‌بخشد.
  ​
* تیم قرمز: عملی است که در آن سیستم‌های هوش مصنوعی به طور فعال با شبیه‌سازی حملات دشمنانه آزمایش می‌شوند تا آسیب‌پذیری‌ها شناسایی شوند.
  ​
* SBOM (فهرست مواد نرم‌افزاری): یک رکورد رسمی حاوی جزئیات و روابط زنجیره تأمین اجزای مختلف مورد استفاده در ساخت نرم‌افزار یا مدل‌های هوش مصنوعی است.
  ​
* SHAP (توضیحات جمعی شاپلی): یک رویکرد نظریه بازی‌ها برای توضیح خروجی هر مدل یادگیری ماشین با محاسبه سهم هر ویژگی در پیش‌بینی.
  ​
* حمله زنجیره تامین: به خطر انداختن یک سیستم با هدف قرار دادن عناصر کم‌امنیت‌تر در زنجیره تامین آن، مانند کتابخانه‌های شخص ثالث، مجموعه‌های داده یا مدل‌های پیش‌آموزش‌دیده.
  ​
* یادگیری انتقالی: تکنیکی که در آن مدل توسعه یافته برای یک کار به عنوان نقطه شروع برای مدلی در کار دوم استفاده مجدد می‌شود.
  ​
* پایگاه داده برداری: یک پایگاه داده تخصصی طراحی شده برای ذخیره بردارهای با ابعاد بالا (تعبیه‌ها) و انجام جستجوهای کارآمد بر اساس شباهت.
  ​
* اسکن آسیب‌پذیری: ابزارهای خودکاری که آسیب‌پذیری‌های امنیتی شناخته‌شده در اجزای نرم‌افزاری، از جمله چارچوب‌ها و وابستگی‌های هوش مصنوعی را شناسایی می‌کنند.
  ​
* واترمارکینگ: تکنیک‌هایی برای جاسازی نشانگرهای غیرقابل تشخیص در محتوای تولید شده توسط هوش مصنوعی به منظور ردیابی منبع آن یا تشخیص تولید توسط هوش مصنوعی.
  ​
* آسیب‌پذیری صفر روزه: یک آسیب‌پذیری ناشناخته قبلی است که مهاجمان می‌توانند قبل از اینکه توسعه‌دهندگان وصله‌ای ایجاد و منتشر کنند، از آن سوءاستفاده کنند.

