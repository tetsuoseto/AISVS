# پیوست الف: واژه‌نامه

این فرهنگ لغت جامع تعاریف اصطلاحات کلیدی هوش مصنوعی، یادگیری ماشین و امنیت را که در سراسر AISVS استفاده شده‌اند، برای اطمینان از وضوح و درک مشترک فراهم می‌کند.

* نمونه خصمانه: ورودی‌ای که عمدتاً برای ایجاد اشتباه در مدل هوش مصنوعی طراحی شده است، معمولاً با افزودن تغییرات ظریف و غیرقابل تشخیص برای انسان‌ها.
  ​
* مقاومت در برابر دشمنی – مقاومت در برابر دشمنی در هوش مصنوعی به توانایی یک مدل برای حفظ عملکرد خود و مقاومت در برابر فریب خوردن یا دستکاری شدن توسط ورودی‌های عمدی و مخرب اشاره دارد که به منظور ایجاد خطا طراحی شده‌اند.
  ​
* عامل – عامل‌های هوش مصنوعی سیستم‌های نرم‌افزاری هستند که از هوش مصنوعی برای دنبال کردن اهداف و انجام وظایف به نمایندگی از کاربران استفاده می‌کنند. آنها توانایی استدلال، برنامه‌ریزی و حافظه دارند و دارای سطحی از خودمختاری برای تصمیم‌گیری، یادگیری و تطبیق هستند.
  ​
* هوش مصنوعی عامل‌محور: سیستم‌های هوش مصنوعی که می‌توانند با درجه‌ای از خودمختاری عمل کنند تا اهدافی را به دست آورند، اغلب با تصمیم‌گیری و انجام اقدامات بدون دخالت مستقیم انسان.
  ​
* کنترل دسترسی مبتنی بر خصوصیات (ABAC): یک الگوی کنترل دسترسی که در آن تصمیم‌های مجوزدهی بر اساس خصوصیات کاربر، منبع، عمل و محیط گرفته می‌شود و این تصمیم‌ها در زمان پرس‌وجو ارزیابی می‌شوند.
  ​
* حمله در پشتی: نوعی حمله مسمومیت داده است که در آن مدل آموزش دیده تا به طور خاص به محرک‌های خاص واکنش نشان دهد در حالی که در شرایط دیگر به طور عادی رفتار می‌کند.
  ​
* تعصب: خطاهای سیستماتیک در خروجی‌های مدل‌های هوش مصنوعی که می‌تواند منجر به نتایج ناعادلانه یا تبعیض‌آمیز برای گروه‌های خاص یا در زمینه‌های مشخص شود.
  ​
* سوء استفاده از تعصب: یک تکنیک حمله است که با بهره‌گیری از تعصبات شناخته شده در مدل‌های هوش مصنوعی، خروجی‌ها یا نتایج را دستکاری می‌کند.
  ​
* Cedar: زبان سیاست و موتور آمازون برای مجوزهای دقیق که در پیاده‌سازی ABAC برای سیستم‌های هوش مصنوعی استفاده می‌شود.
  ​
* زنجیره تفکر: تکنیکی برای بهبود استدلال در مدل‌های زبانی از طریق تولید مراحل استدلال واسطه قبل از ارائه پاسخ نهایی.
  ​
* قطع‌کننده‌های مدار: مکانیزم‌هایی که به‌طور خودکار عملیات سیستم هوش مصنوعی را زمانی متوقف می‌کنند که آستانه‌های ریسک خاصی überschritten شوند.
  ​
* خدمات استنتاج محرمانه: یک سرویس استنتاج که مدل‌های هوش مصنوعی را داخل محیط اجرای مورد اعتماد (TEE) یا مکانیزم محاسبات محرمانه معادل اجرا می‌کند، به‌طوری‌که وزن‌های مدل و داده‌های استنتاج رمزنگاری شده، مهر و موم شده و در برابر دسترسی غیرمجاز یا دستکاری محافظت می‌شوند.
  ​
* بار کاری محرمانه: یک بار کاری هوش مصنوعی (مثلاً آموزش، استنتاج، پیش‌پردازش) که در داخل یک محیط اجرای مورد اعتماد (TEE) با جداسازی سخت‌افزاری تحمیلی، رمزنگاری حافظه، و اعتبارسنجی از راه دور اجرا می‌شود تا کد، داده‌ها و مدل‌ها را از دسترسی میزبان یا هم‌مستأجر محافظت کند.
  ​
* نشت داده: افشای ناخواسته اطلاعات حساس از طریق خروجی‌ها یا رفتار مدل هوش مصنوعی.
  ​
* سم‌پاشی داده: فساد عمدی داده‌های آموزشی برای به خطر انداختن یکپارچگی مدل، که اغلب برای نصب درهای پشتی یا کاهش عملکرد انجام می‌شود.
  ​
* حریم خصوصی تفاضلی – حریم خصوصی تفاضلی چارچوبی دقیق و ریاضیاتی برای انتشار اطلاعات آماری درباره مجموعه داده‌ها است در حالی که حریم خصوصی افراد را حفظ می‌کند. این روش به دارنده داده اجازه می‌دهد تا الگوهای جمعی گروه را به اشتراک بگذارد در حالی که اطلاعاتی که درباره افراد خاص فاش می‌شود را محدود می‌کند.
  ​
* بردارهای جاسازی شده: نمایش‌های برداری چگال داده‌ها (متن، تصاویر و غیره) که معنای معنایی را در فضای چندبعدی بالا ثبت می‌کنند.
  ​
* قابلیت توضیح‌پذیری – قابلیت توضیح‌پذیری در هوش مصنوعی به توانایی یک سیستم هوش مصنوعی در ارائه دلایل قابل درک برای انسان‌ها درباره تصمیمات و پیش‌بینی‌هایش اطلاق می‌شود که بینشی درباره عملکرد داخلی آن ارائه می‌دهد.
  ​
* هوش مصنوعی توضیح‌پذیر (XAI): سیستم‌های هوش مصنوعی که به منظور ارائه توضیحات قابل درک برای انسان‌ها درباره تصمیمات و رفتارهای خود از طریق تکنیک‌ها و چارچوب‌های مختلف طراحی شده‌اند.
  ​
* یادگیری فدرال: روشی در یادگیری ماشین که در آن مدل‌ها بر روی چندین دستگاه غیرمتمرکز که نمونه‌های داده محلی را نگه می‌دارند، آموزش داده می‌شوند، بدون اینکه خود داده‌ها تبادل شوند.
  ​
* فرمول‌بندی: دستورالعمل یا روشی که برای تولید یک اثر یا مجموعه داده استفاده می‌شود، مانند ابرپارامترها، پیکربندی آموزش، مراحل پیش‌پردازش، یا اسکریپت‌های ساخت.
  ​
* گاردریل‌ها: محدودیت‌هایی که برای جلوگیری از تولید خروجی‌های مضر، تعصب‌آمیز یا به‌طور کلی نامطلوب توسط سیستم‌های هوش مصنوعی اعمال می‌شوند.
  ​
* توهم – توهم در هوش مصنوعی به پدیده‌ای اشاره دارد که در آن یک مدل هوش مصنوعی اطلاعات نادرست یا گمراه‌کننده تولید می‌کند که بر اساس داده‌های آموزشی یا واقعیت‌های واقعی نمی‌باشد.
  ​
* انسان در حلقه (HITL): سیستم‌هایی که طوری طراحی شده‌اند که نیازمند نظارت، تایید، یا دخالت انسان در نقاط تصمیم‌گیری حیاتی هستند.
  ​
* زیرساخت به عنوان کد (IaC): مدیریت و تأمین زیرساخت از طریق کد به جای فرآیندهای دستی، که امکان اسکن امنیتی و استقرارهای سازگار را فراهم می‌کند.
  ​
* بریک‌کردن: تکنیک‌هایی که برای دور زدن محدودیت‌های ایمنی در سیستم‌های هوش مصنوعی، به‌ویژه در مدل‌های زبانی بزرگ، به‌کار می‌روند تا محتوای ممنوعه تولید شود.
  ​
* کمترین امتیاز: اصل امنیتی اعطای تنها حداقل حقوق دسترسی لازم برای کاربران و فرایندها.
  ​
* LIME (توضیحات مدل‌تفسیرپذیر مستقل از مدل محلی): تکنیکی برای توضیح پیش‌بینی‌های هر طبقه‌بند یادگیری ماشین با تقریب زدن آن به صورت محلی با یک مدل قابل تفسیر است.
  ​
* MCP (پروتکل زمینه مدل): پروتکلی که به مدل‌ها و عامل‌های هوش مصنوعی امکان می‌دهد با تبادل درخواست‌ها و پاسخ‌های ساختار یافته و نوع‌دار از طریق یک انتقال تعریف شده، به ابزارها، منابع داده و منابع خارجی دسترسی پیدا کنند.
  ​
* حمله استنتاج عضویت: حمله‌ای که هدف آن تعیین این است که آیا یک نقطه داده خاص برای آموزش یک مدل یادگیری ماشین استفاده شده است یا خیر.
  ​
* MITRE ATLAS: چشم‌انداز تهدیدات متخاصم برای سیستم‌های هوش‌مصنوعی؛ یک پایگاه دانش از تاکتیک‌ها و تکنیک‌های متخاصم علیه سیستم‌های هوش‌مصنوعی.
  ​
* کارت مدل – کارت مدل سندی است که اطلاعات استاندارد شده‌ای درباره عملکرد مدل هوش مصنوعی، محدودیت‌ها، کاربردهای مورد نظر و ملاحظات اخلاقی آن ارائه می‌دهد تا شفافیت و توسعه مسئولانه هوش مصنوعی را ترویج کند.
  ​
* استخراج مدل: حمله‌ای که در آن یک مهاجم به طور مکرر از مدل هدف پرس‌وجو می‌کند تا یک نسخه عملکردی مشابه بدون مجوز ایجاد کند.
  ​
* برگردان مدل: حمله‌ای که سعی می‌کند داده‌های آموزشی را با تحلیل خروجی‌های مدل بازسازی کند.
  ​
* مدیریت چرخه عمر مدل – مدیریت چرخه عمر مدل هوش مصنوعی فرآیند نظارت بر تمام مراحل وجود یک مدل هوش مصنوعی است، شامل طراحی، توسعه، استقرار، پایش، نگهداری و نهایتاً بازنشستگی آن، به منظور اطمینان از موثر بودن و هماهنگی با اهداف.
  ​
* آلوده‌سازی مدل: معرفی آسیب‌پذیری‌ها یا درهای پشتی به طور مستقیم در مدل طی فرآیند آموزش.
  ​
* سرقت/دزدیدن مدل: استخراج یک نسخه یا تقریب از یک مدل اختصاصی از طریق پرس‌وجوهای مکرر.
  ​
* سیستم چندعامله: سیستمی متشکل از چندین عامل هوش مصنوعی تعاملی است که هرکدام ممکن است قابلیت‌ها و اهداف متفاوتی داشته باشند.
  ​
* OPA (Open Policy Agent): یک موتور سیاست‌گذاری متن‌باز که امکان اجرای یکنواخت سیاست‌ها در سراسر پشته را فراهم می‌کند.
  ​
* یادگیری ماشین حفظ حریم خصوصی (PPML): تکنیک‌ها و روش‌هایی برای آموزش و استقرار مدل‌های یادگیری ماشین در حالی که حفظ حریم خصوصی داده‌های آموزشی تضمین می‌شود.
  ​
* تزریق پرامپت: حمله‌ای که در آن دستورات مخرب در ورودی‌ها تعبیه می‌شوند تا رفتار مورد نظر مدل را نادیده بگیرند.
  ​
* RAG (تولید تقویت‌شده با بازیابی): یک تکنیک است که مدل‌های زبان بزرگ را با بازیابی اطلاعات مرتبط از منابع دانش خارجی قبل از تولید پاسخ بهبود می‌بخشد.
  ​
* رد-تیمینگ: عملیاتی که در آن سیستم‌های هوش مصنوعی با شبیه‌سازی حملات خصمانه به صورت فعال آزمایش می‌شوند تا نقاط ضعف آن‌ها شناسایی گردد.
  ​
* SBOM (صورت‌ وضعیت مواد نرم‌افزاری): یک سند رسمی حاوی جزئیات و روابط زنجیره تامین اجزای مختلف استفاده شده در ساخت نرم‌افزار یا مدل‌های هوش مصنوعی.
  ​
* SHAP (توضیحات جمع‌شونده شپلِی): رویکردی نظریه بازی‌ها برای توضیح خروجی هر مدل یادگیری ماشین با محاسبه سهم هر ویژگی در پیش‌بینی.
  ​
* احراز هویت قوی: احراز هویتی که در برابر سرقت مدارک و بازپخش مقاومت می‌کند و حداقل دو عامل (دانش، مالکیت، ویژگی ذاتی) و مکانیزم‌های مقاوم در برابر فیشینگ مانند FIDO2/WebAuthn، احراز هویت خدمات مبتنی بر گواهی‌نامه، یا توکن‌های کوتاه‌مدت را الزامی می‌سازد.
  ​
* حمله زنجیره تأمین: نفوذ به یک سیستم با هدف قرار دادن عناصر کمتر امن در زنجیره تأمین آن، مانند کتابخانه‌های شخص ثالث، مجموعه داده‌ها یا مدل‌های پیش‌آموزش‌دیده.
  ​
* یادگیری انتقالی: تکنیکی که در آن یک مدل توسعه یافته برای یک وظیفه به‌عنوان نقطه شروع برای مدل در وظیفه دوم مجدداً استفاده می‌شود.
  ​
* پایگاه داده برداری: پایگاه داده‌ای تخصصی طراحی شده برای ذخیره بردارهای با ابعاد بالا (تعبیه‌ها) و انجام جستجوهای مشابهت به طور کارآمد.
  ​
* اسکن آسیب‌پذیری: ابزارهای خودکار که آسیب‌پذیری‌های امنیتی شناخته‌شده در مؤلفه‌های نرم‌افزاری، از جمله چارچوب‌ها و وابستگی‌های هوش مصنوعی را شناسایی می‌کنند.
  ​
* واترمارکینگ: تکنیک‌هایی برای جاسازی نشانگرهای غیرقابل تشخیص در محتوای تولید شده توسط هوش مصنوعی به منظور ردیابی منبع آن یا شناسایی تولید توسط هوش مصنوعی.
  ​
* آسیب‌پذیری صفر روز: آسیب‌پذیری‌ای که قبلاً ناشناخته بوده و مهاجمان می‌توانند قبل از اینکه توسعه‌دهندگان یک وصله امنیتی ایجاد و منتشر کنند، از آن سوءاستفاده کنند.

