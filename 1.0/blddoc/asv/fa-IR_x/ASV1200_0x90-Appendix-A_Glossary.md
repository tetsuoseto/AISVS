# ضمیمه الف: واژه‌نامه

این واژه‌نامه جامع، تعاریف اصطلاحات کلیدی هوش مصنوعی، یادگیری ماشین و امنیت را که در سراسر AISVS به کار رفته‌اند، برای اطمینان از وضوح و درک مشترک ارائه می‌دهد.

* مثال معارض: ورودی‌ای که به طور عمدی ساخته شده است تا باعث شود یک مدل هوش مصنوعی اشتباه کند، اغلب با افزودن تغییرات ظریف که برای انسان‌ها قابل تشخیص نیستند.
  ​
* مقاومت در برابر حملات مخرب – مقاومت در برابر حملات مخرب در هوش مصنوعی به توانایی یک مدل اشاره دارد که عملکرد خود را حفظ کند و در برابر ورودی‌های عمدتاً طراحی‌شده و مخرب که هدفشان ایجاد اشتباه است، فریب خوردن یا دستکاری شدن مقاومت کند.
  ​
* عامل – عوامل هوش مصنوعی سیستم‌های نرم‌افزاری هستند که از هوش مصنوعی برای دنبال کردن اهداف و انجام وظایف به نمایندگی از کاربران استفاده می‌کنند. آن‌ها توانایی استدلال، برنامه‌ریزی و حافظه را دارند و دارای درجه‌ای از خودمختاری برای تصمیم‌گیری، یادگیری و سازگاری هستند.
  ​
* هوش مصنوعی عاملیت‌دار: سیستم‌های هوش مصنوعی که می‌توانند با درجه‌ای از خودمختاری برای دستیابی به اهداف عمل کنند و اغلب بدون دخالت مستقیم انسان تصمیم‌گیری و اقدام می‌کنند.
  ​
* کنترل دسترسی مبتنی بر ویژگی (ABAC): یک پارادایم کنترل دسترسی که در آن تصمیمات مجوزدهی بر اساس ویژگی‌های کاربر، منبع، عمل و محیط، در زمان پرس‌وجو ارزیابی می‌شود.
  ​
* حمله درب‌پشتی: نوعی حمله مسموم‌سازی داده است که در آن مدل طوری آموزش داده می‌شود که به محرک‌های خاص به صورت مشخصی پاسخ دهد در حالی که در سایر موارد به طور عادی رفتار می‌کند.
  ​
* تعصب: خطاهای سیستماتیک در خروجی‌های مدل هوش مصنوعی که می‌توانند به نتایج ناعادلانه یا تبعیض‌آمیز برای گروه‌های خاص یا در زمینه‌های خاص منجر شوند.
  ​
* استفاده از سوگیری: یک تکنیک حمله که از سوگیری‌های شناخته شده در مدل‌های هوش مصنوعی بهره می‌برد تا خروجی‌ها یا نتایج را دستکاری کند.
  ​
* سدار: زبان و موتور سیاست‌های آمازون برای مجوزهای دقیق که در اجرای مدل کنترل دسترسی مبتنی بر ویژگی‌ها (ABAC) برای سیستم‌های هوش مصنوعی استفاده می‌شود.
  ​
* زنجیره تفکر: یک تکنیک برای بهبود استدلال در مدل‌های زبانی از طریق تولید گام‌های میانی استدلال قبل از ارائه پاسخ نهایی‌.
  ​
* قطع‌کننده‌های مدار: مکانیزم‌هایی که به‌صورت خودکار عملیات سیستم هوش مصنوعی را زمانی که آستانه‌های خطر خاصی عبور کنند، متوقف می‌کنند.
  ​
* نشت داده: افشای ناخواسته اطلاعات حساس از طریق خروجی‌ها یا رفتار مدل هوش مصنوعی.
  ​
* آلوده‌سازی داده: فساد عمدی داده‌های آموزشی به منظور به خطر انداختن یکپارچگی مدل، اغلب برای نصب درهای پشتی یا کاهش عملکرد.
  ​
* حریم خصوصی تفاضلی – حریم خصوصی تفاضلی یک چارچوب ریاضی دقیق برای ارائه اطلاعات آماری درباره مجموعه داده‌ها است که در عین حال از حریم خصوصی افراد داده شده محافظت می‌کند. این امکان را برای دارنده داده فراهم می‌کند تا الگوهای کلی گروه را به اشتراک بگذارد و در عین حال اطلاعات فاش شده درباره افراد خاص را محدود کند.
  ​
* تعبیه‌ها: نمایش‌های برداری چگال از داده‌ها (متن، تصاویر و غیره) که معنی معنایی را در یک فضای چندبعدی بالا به‌دست می‌آورند.
  ​
* قابل توضیح بودن – قابل توضیح بودن در هوش مصنوعی به توانایی یک سیستم هوش مصنوعی در ارائه دلایل قابل فهم برای انسان‌ها درباره تصمیمات و پیش‌بینی‌هایش اشاره دارد که بینش‌هایی در مورد عملکرد داخلی آن ارائه می‌دهد.
  ​
* هوش مصنوعی قابل توضیح (XAI): سیستم‌های هوش مصنوعی طراحی شده برای ارائه توضیحات قابل فهم توسط انسان درباره تصمیمات و رفتارهای خود از طریق روش‌ها و چارچوب‌های مختلف.
  ​
* یادگیری فدرال: روشی در یادگیری ماشین که در آن مدل‌ها در چندین دستگاه غیرمتمرکز با داده‌های محلی آموزش داده می‌شوند، بدون اینکه خود داده‌ها تبادل شوند.
  ​
* گاردریل‌ها: محدودیت‌هایی که برای جلوگیری از تولید خروجی‌های مضر، جانبدارانه یا به هر نحو نامطلوب توسط سامانه‌های هوش مصنوعی پیاده‌سازی شده‌اند.
  ​
* هالوژنیشن – هالوژنیشن در هوش مصنوعی به پدیده‌ای اطلاق می‌شود که در آن مدل هوش مصنوعی اطلاعات نادرست یا گمراه‌کننده‌ای تولید می‌کند که بر اساس داده‌های آموزشی یا واقعیت‌های عینی نیست.
  ​
* انسان در حلقه (HITL): سیستم‌هایی که به گونه‌ای طراحی شده‌اند که نیاز به نظارت، تأیید یا مداخله انسانی در نقاط تصمیم‌گیری حیاتی دارند.
  ​
* زیرساخت به عنوان کد (IaC): مدیریت و فراهم‌سازی زیرساخت از طریق کد به جای فرآیندهای دستی، که امکان اسکن امنیتی و استقرارهای یکنواخت را فراهم می‌کند.
  ​
* شکنجه: تکنیک‌هایی که برای دور زدن موانع ایمنی در سیستم‌های هوش مصنوعی، به ویژه در مدل‌های زبانی بزرگ، به منظور تولید محتوای ممنوعه استفاده می‌شوند.
  ​
* حداقل امتیاز: اصل امنیتی اعطای کمترین حقوق دسترسی لازم برای کاربران و فرآیندها.
  ​
* LIME (توضیحات محلی قابل تفسیر و مستقل از مدل): تکنیکی برای توضیح پیش‌بینی‌های هر طبقه‌بند یادگیری ماشین با تقریب زدن محلی آن با یک مدل قابل تفسیر.
  ​
* حمله استنتاج عضویت: حمله‌ای که هدف آن تعیین این است که آیا یک داده خاص برای آموزش مدل یادگیری ماشین استفاده شده است یا خیر.
  ​
* MITRE ATLAS: چشم‌انداز تهدیدهای خصمانه برای سیستم‌های هوش مصنوعی؛ یک پایگاه دانش از تاکتیک‌ها و تکنیک‌های خصمانه علیه سیستم‌های هوش مصنوعی.
  ​
* کارت مدل – کارت مدل سندی است که اطلاعات استاندارد شده‌ای درباره عملکرد، محدودیت‌ها، کاربردهای مورد نظر و ملاحظات اخلاقی یک مدل هوش مصنوعی ارائه می‌دهد تا شفافیت و توسعه مسئولانه هوش مصنوعی را ترویج کند.
  ​
* استخراج مدل: حمله‌ای که در آن یک مهاجم به طور مکرر مدل هدف را پرس‌وجو می‌کند تا یک نسخه مشابه عملکردی بدون مجوز ایجاد کند.
  ​
* وارونه‌سازی مدل: حمله‌ای که سعی دارد داده‌های آموزشی را با تحلیل خروجی‌های مدل بازسازی کند.
  ​
* مدیریت چرخه عمر مدل – مدیریت چرخه عمر مدل هوش مصنوعی فرآیند نظارت بر تمامی مراحل وجود یک مدل هوش مصنوعی است، شامل طراحی، توسعه، استقرار، نظارت، نگهداری و در نهایت بازنشستگی آن، تا تضمین شود که مدل مؤثر باقی می‌ماند و با اهداف هماهنگ است.
  ​
* تسمم مدل: وارد کردن آسیب‌پذیری‌ها یا درهای پشتی به‌طور مستقیم در مدل در طی فرآیند آموزش.
  ​
* سرقت/کپی‌برداری مدل: استخراج یک نسخه یا تقریب از یک مدل مالکیتی از طریق سوالات مکرر.
  ​
* سیستم چندعامله: سیستمی متشکل از چندین عامل هوش مصنوعی که هر کدام ممکن است قابلیت‌ها و اهداف متفاوتی داشته باشند و با هم تعامل دارند.
  ​
* OPA (Open Policy Agent): یک موتور سیاست متن‌باز است که امکان اجرای یکپارچه سیاست‌ها را در سراسر لایه‌ها فراهم می‌کند.
  ​
* یادگیری ماشین حفظ حریم خصوصی (PPML): تکنیک‌ها و روش‌هایی برای آموزش و اجرای مدل‌های یادگیری ماشین در حالی که حریم خصوصی داده‌های آموزشی حفظ می‌شود.
  ​
* تزریق فرمان: حمله‌ای که در آن دستورهای مخرب در ورودی‌ها جاسازی شده‌اند تا رفتار مورد نظر مدل را نادیده بگیرند.
  ​
* RAG (تولید افزوده‌شده با بازیابی): روشی که مدل‌های زبان بزرگ را با بازیابی اطلاعات مرتبط از منابع دانش خارجی قبل از تولید پاسخ، بهبود می‌بخشد.
  ​
* رد-تیمینگ: عملیاتی که شامل تست فعال سیستم‌های هوش مصنوعی با شبیه‌سازی حملات خصمانه برای شناسایی آسیب‌پذیری‌ها است.
  ​
* SBOM (فهرست مواد نرم‌افزار): یک رکورد رسمی شامل جزئیات و روابط زنجیره تأمین اجزای مختلف مورد استفاده در ساخت نرم‌افزار یا مدل‌های هوش مصنوعی.
  ​
* SHAP (توضیحات جمع‌شونده شاپلی): رویکردی مبتنی بر نظریه بازی‌ها برای توضیح خروجی هر مدل یادگیری ماشین با محاسبه سهم هر ویژگی در پیش‌بینی.
  ​
* حمله زنجیره تامین: نفوذ به یک سیستم از طریق هدف قرار دادن عناصر کمتر امن در زنجیره تامین آن، مانند کتابخانه‌های شخص ثالث، مجموعه داده‌ها، یا مدل‌های از پیش آموزش‌دیده شده.
  ​
* یادگیری انتقالی: تکنیکی که در آن یک مدل توسعه یافته برای یک وظیفه به عنوان نقطه شروع برای مدل در یک وظیفه دوم مجدداً استفاده می‌شود.
  ​
* پایگاه داده برداری: یک پایگاه داده تخصصی طراحی شده برای ذخیره بردارهای با ابعاد بالا (تعبیه‌ها) و انجام جستجوهای شباهت به‌صورت کارآمد.
  ​
* اسکن آسیب‌پذیری: ابزارهای خودکاری که آسیب‌پذیری‌های امنیتی شناخته شده در اجزای نرم‌افزاری، از جمله چارچوب‌ها و وابستگی‌های هوش مصنوعی را شناسایی می‌کنند.
  ​
* واترمارکینگ: تکنیک‌هایی برای جاسازی نشانگرهای غیرقابل تشخیص در محتوای تولید شده توسط هوش مصنوعی به منظور ردیابی منبع آن یا شناسایی تولید توسط هوش مصنوعی.
  ​
* آسیب‌پذیری صفر روز: یک آسیب‌پذیری ناشناخته که مهاجمان می‌توانند قبل از اینکه توسعه‌دهندگان وصله‌ای برای آن ایجاد و اعمال کنند، از آن سوءاستفاده کنند.

