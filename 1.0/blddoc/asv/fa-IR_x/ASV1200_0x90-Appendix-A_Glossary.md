# ضمیمه A: واژه‌نامه

>This واژه‌نامه جامع تعاریف اصطلاحات کلیدی هوش مصنوعی (AI)، یادگیری ماشین (ML)، و امنیت که در AISVS به کار می‌رود را برای اطمینان از شفافیت و درک مشترک ارائه می‌دهد.

* نمونه خصمانه: ورودی که به‌طور عمدی طراحی شده است تا باعث شود مدل هوش مصنوعی اشتباه کند، اغلب با افزودن تغییرات ناچیز نامحسوس برای انسان‌ها.
  ​
* مقاومت در برابر حملات مخرب – مقاومت مدل در برابر ورودی‌های مخرب که عمداً ساخته شده‌اند تا فریب داده شوند یا دستکاری شوند و منجر به خطا گردند، به معنای توانایی حفظ عملکرد مدل در هوش مصنوعی است.
  ​
* عامل – عامل‌های هوش مصنوعی سامانه‌های نرم‌افزاری هستند که از هوش مصنوعی برای دستیابی به اهداف و انجام وظایف به نمایندگی از کاربران استفاده می‌کنند. آنها دارای استدلال، برنامه‌ریزی و حافظه هستند و سطحی از خودمختاری برای تصمیم‌گیری، یادگیری و سازگار شدن دارند.
  ​
* هوش مصنوعی عامل‌محور: سامانه‌های هوش مصنوعی که می‌توانند با درجه‌ای از خودمختاری برای رسیدن به اهداف عمل کنند و اغلب تصمیم‌گیری می‌کنند و اقداماتی بدون مداخله مستقیم انسان انجام می‌دهند.
  ​
* کنترل دسترسی مبتنی بر ویژگی‌ها (ABAC): الگویی برای کنترل دسترسی که تصمیمات مجوز بر اساس ویژگی‌های کاربر، منبع، اقدام و محیط گرفته می‌شود و در زمان استعلام ارزیابی می‌گردد.
  ​
* حملهٔ درب پشتی: نوعی حمله سم‌پاشی داده‌ها است که در آن مدل به محرک‌های معین به روشی خاص پاسخ می‌دهد، در حالی که در سایر مواقع به‌طور عادی رفتار می‌کند.
  ​
* سوگیری: خطاهای سیستماتیک در خروجی‌های مدل‌های هوش مصنوعی که می‌توانند به نتایج غیرعادلانه یا تبعیض‌آمیز برای گروه‌های خاص یا در زمینه‌های خاص منجر شوند.
  ​
* استفاده از سوگیری: یک تکنیک حمله که از سوگیری‌های شناخته‌شده در مدل‌های هوش مصنوعی برای دستکاری خروجی‌ها یا نتایج استفاده می‌کند.
  ​
* Cedar: زبان سیاستی آمازون و موتور برای مجوزهای با جزئیات دقیق که در پیاده‌سازی ABAC برای سیستم‌های هوش مصنوعی استفاده می‌شود.
  ​
* زنجیرهٔ تفکر: روشی برای بهبود استدلال در مدل‌های زبانی با تولید گام‌های استدلال میانی پیش از ارائه پاسخ نهایی.
  ​
* قطع‌کننده‌های مدار: سازوکارهایی که هنگامی که آستانه‌های خطر مشخصی فراتر می‌روند، عملیات سامانهٔ هوش مصنوعی را به‌طور خودکار متوقف می‌کنند.
  ​
* افشای داده‌ها: افشای ناخواسته اطلاعات حساس از طریق خروجی‌های مدل‌های هوش مصنوعی یا رفتار آن‌ها.
  ​
* آلودگی داده‌های آموزشی: خرابکاری عمدی در داده‌های آموزشی به منظور به خطر انداختن یکپارچگی مدل، اغلب برای نصب درهای پشتی یا کاهش کارایی.
  ​
* حریم خصوصی تفاضلی – حریم خصوصی تفاضلی یک چارچوب ریاضیاتی دقیق برای انتشار اطلاعات آماری درباره مجموعه‌های داده است در حالی که از حریم خصوصی افراد محافظت می‌کند. این چارچوب به دارنده داده امکان می‌دهد الگوهای جمعی گروه را به اشتراک بگذارد و در عین حال اطلاعاتی که درباره افراد خاص افشا می‌شود محدود می‌کند.
  ​
* Embeddingها: نمایش‌های بردیِ فشرده از داده‌ها (متن، تصاویر و غیره) که معنایِ مفهومیِ داده‌ها را در فضایِ با ابعادِ بالا ثبت می‌کنند.
  ​
* توضیح‌پذیری – توضیح‌پذیری در هوش مصنوعی، توانایی یک سیستم هوش مصنوعی است برای ارائه دلایل قابل فهم برای تصمیمات و پیش‌بینی‌های آن و ارائه بینشی نسبت به سازوکارهای داخلی آن.
  ​
* هوش مصنوعی قابل توضیح (XAI): سامانه‌های هوش مصنوعی که برای ارائه توضیحات قابل فهم درباره تصمیمات و رفتارهایشان از طریق تکنیک‌ها و چارچوب‌های مختلف طراحی شده‌اند.
  ​
* یادگیری فدراسیون توزیع‌شده: رویکردی در یادگیری ماشین است که مدل‌ها در میان چندین دستگاه غیرمتمرکز که داده‌های نمونه‌های محلی را در خود نگه می‌دارند، بدون تبادل داده‌ها آموزش داده می‌شوند.
  ​
* ریل‌های حفاظتی: محدودیت‌هایی که برای جلوگیری از تولید خروجی‌های مضر، جانبدارانه یا به هر صورت ناخواسته توسط سامانه‌های هوش مصنوعی اعمال می‌شوند.
  ​
* توهم – یک توهم در هوش مصنوعی به پدیده‌ای اشاره دارد که طی آن یک مدل هوش مصنوعی اطلاعات نادرست یا گمراه‌کننده تولید می‌کند که مبتنی بر داده‌های آموزشی خود یا واقعیت عینی نیست.
  ​
* انسان در حلقه (HITL): سیستم‌هایی که برای نیاز به نظارت، تأیید یا مداخله انسانی در نقاط تصمیم‌گیری بحرانی طراحی شده‌اند.
  ​
* زیرساخت به عنوان کد (IaC): مدیریت و راه‌اندازی زیرساخت از طریق کد به‌جای فرایندهای دستی، امکان اسکن امنیتی و استقرارهای منسجم را فراهم می‌کند.
  ​
* جیل‌بریک: تکنیک‌هایی که برای دور زدن محدودیت‌های ایمنی در سیستم‌های هوش مصنوعی، به‌ویژه در مدل‌های زبانی بزرگ، برای تولید محتوای ممنوعه به کار می‌روند.
  ​
* اصل حداقل دسترسی: اصل امنیتی که تنها حقوق دسترسی لازم و کافی را به کاربران و فرآیندها اعطا می‌کند.
  ​
* LIME (توضیحات محلی تفسیرپذیر مستقل از مدل): روشی برای توضیح پیش‌بینی‌های هر طبقه‌بند یادگیری ماشین با تقریب زدن آن به‌طور محلی با یک مدل تفسیرپذیر است.
  ​
* حمله استنتاج عضویت: حمله‌ای است که هدف آن تعیین اینکه آیا یک نمونه داده خاص برای آموزش یک مدل یادگیری ماشین استفاده شده است.
  ​
* MITRE ATLAS: چشم‌انداز تهدیدهای خصمانه برای سیستم‌های هوش مصنوعی؛ یک پایگاه دانش از تاکتیک‌ها و تکنیک‌های خصمانه علیه سیستم‌های هوش مصنوعی.
  ​
* کارت مدل – سندی است که اطلاعات استانداردشده درباره عملکرد مدل هوش مصنوعی، محدودیت‌های آن، کاربردهای مدنظر و ملاحظات اخلاقی را ارائه می‌دهد تا شفافیت و توسعه مسئولانه هوش مصنوعی را ترویج کند.
  ​
* استخراج مدل: حمله‌ای که در آن یک مهاجم به طور مکرر از مدل هدف پرسش می‌کند تا نسخه‌ای از نظر کارکردی مشابه بدون مجوز ایجاد کند.
  ​
* معکوس‌سازی مدل: حمله‌ای که با تحلیل خروجی‌های مدل سعی در بازسازی داده‌های آموزشی دارد.
  ​
* مدیریت چرخه عمر مدل – مدیریت چرخه عمر مدل‌های هوش مصنوعی فرایندی است که نظارت بر تمامی مراحل وجود یک مدل هوش مصنوعی را در بر می‌گیرد، از طراحی آن، توسعه، استقرار، پایش، نگهداری و در نهایت بازنشستگی آن، تا اطمینان حاصل شود که مدل مؤثر باقی می‌ماند و با اهداف مطابقت دارد.
  ​
* سمی‌سازی مدل: افزودن آسیب‌پذیری‌ها یا درهای پشتی به طور مستقیم به مدل در طول فرایند آموزش.
  ​
* سرقت مدل: از طریق پرسش‌های مکرر، استخراج نسخه کپی یا تقریبی از یک مدل اختصاصی.
  ​
* سامانه چندعامله: سامانه‌ای که از چند عامل هوش مصنوعی در حال تعامل با یکدیگر تشکیل شده است و هر یک از آن‌ها ممکن است دارای قابلیت‌ها و اهداف متفاوتی باشند.
  ​
* OPA (Open Policy Agent): یک موتور سیاست‌گذاری متن‌باز است که امکان اجرای سیاست‌های یکپارچه را در سراسر پشته فراهم می‌کند.
  ​
* یادگیری ماشین با حفاظت از حریم خصوصی (PPML): تکنیک‌ها و روش‌هایی برای آموزش و استقرار مدل‌های یادگیری ماشین در حالی که حریم خصوصی داده‌های آموزشی حفظ می‌شود.
  ​
* تزریق پرامپت: حمله‌ای است که دستورالعمل‌های مخرب در ورودی‌ها جاسازی می‌شوند تا رفتار مدنظر مدل را نقض کند.
  ​
* RAG (تولید با بازیابی): روشی است که مدل‌های زبانی بزرگ را با بازیابی اطلاعات مرتبط از منابع دانش خارجی پیش از تولید پاسخ بهبود می‌بخشد.
  ​
* Red-Teaming: رویکردی است که به‌طور فعال سیستم‌های هوش مصنوعی را با شبیه‌سازی حملات خصمانه آزمایش می‌کند تا آسیب‌پذیری‌ها را شناسایی کند.
  ​
* SBOM (فهرست مواد نرم‌افزاری): یک پرونده رسمی حاوی جزئیات و روابط زنجیره تامین اجزای مختلف مورد استفاده در ساخت نرم‌افزار یا مدل‌های هوش مصنوعی.
  ​
* SHAP (SHapley Additive exPlanations): رویکرد مبتنی بر نظریه بازی برای توضیح خروجی هر مدل یادگیری ماشین با محاسبه سهم هر ویژگی در پیش‌بینی.
  ​
* حمله به زنجیره تأمین: دستکاری یک سیستم از طریق هدف قرار دادن عناصر کم‌امنیت‌تر در زنجیره تأمین آن، مانند کتابخانه‌های شخص ثالث، مجموعه‌های داده، یا مدل‌های از پیش آموزش‌دیده.
  ​
* یادگیری انتقالی: روشی است که در آن یک مدل توسعه یافته برای یک کار به‌عنوان نقطۀ آغاز برای مدلی در کار دوم دوباره استفاده می‌شود.
  ​
* پایگاه داده بردی: یک پایگاه داده تخصصی که برای ذخیره بردهای با ابعاد بالا (بردهای تعبیه‌ای) طراحی شده و جستجوهای تشابه کارآمد انجام می‌دهد.
  ​
* اسکن آسیب‌پذیری: ابزارهای خودکار که آسیب‌پذیری‌های امنیتی شناخته‌شده را در مؤلفه‌های نرم‌افزاری شناسایی می‌کنند، از جمله فریم‌ورک‌های هوش مصنوعی و وابستگی‌ها.
  ​
* آب‌نشان‌گذاری: روش‌هایی برای درج نشان‌های نامحسوس در محتوای تولیدشده توسط هوش مصنوعی تا منشاء آن را پیگیری کنند یا تولید با هوش مصنوعی را تشخیص دهند.
  ​
* آسیب‌پذیری روز صفر: یک آسیب‌پذیری ناشناخته که مهاجمان می‌توانند پیش از ایجاد و پیاده‌سازی یک پچ توسط توسعه‌دهندگان از آن بهره‌برداری کنند.

