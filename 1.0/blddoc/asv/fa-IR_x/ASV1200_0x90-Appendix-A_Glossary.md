# پیوست الف: فرهنگ اصطلاحات

این فرهنگ لغت جامع، تعاریف اصطلاحات کلیدی هوش مصنوعی، یادگیری ماشین و امنیت را که در سراسر AISVS استفاده شده‌اند، برای اطمینان از وضوح و درک مشترک فراهم می‌کند.

* مثال خصمانه: یک ورودی که به طور عمدی ساخته شده است تا مدل هوش مصنوعی را به اشتباه بیندازد، اغلب با افزودن تغییرات ظریف که برای انسان‌ها قابل تشخیص نیستند.
  ​
* استحکام مقابله‌ای – استحکام مقابله‌ای در هوش مصنوعی به توانایی یک مدل برای حفظ عملکرد خود و مقاومت در برابر فریب خوردن یا دستکاری شدن توسط ورودی‌های عمداً ساخته شده و مخرب اشاره دارد که برای ایجاد خطا طراحی شده‌اند.
  ​
* عامل – عوامل هوش مصنوعی سیستم‌های نرم‌افزاری هستند که از هوش مصنوعی برای دنبال کردن اهداف و انجام وظایف به نمایندگی از کاربران استفاده می‌کنند. آنها دارای توانایی استدلال، برنامه‌ریزی و حافظه هستند و سطحی از خودمختاری برای اتخاذ تصمیمات، یادگیری و سازگاری دارند.
  ​
* هوش مصنوعی عامل‌مند: سیستم‌های هوش مصنوعی که قادر به عملکرد با درجه‌ای از خودمختاری برای دستیابی به اهداف هستند و اغلب بدون مداخله مستقیم انسان تصمیم‌گیری و اقدام می‌کنند.
  ​
* کنترل دسترسی مبتنی بر ویژگی (ABAC): یک الگوی کنترل دسترسی است که تصمیمات مجوزدهی بر اساس ویژگی‌های کاربر، منبع، عمل و محیط گرفته می‌شود و در زمان پرس‌وجو ارزیابی می‌گردد.
  ​
* حمله پشتی: نوعی حمله به منظور آلوده‌سازی داده‌ها است که در آن مدل طوری آموزش داده می‌شود که به محرک‌های خاص به شکلی ویژه پاسخ دهد در حالی که در غیر این صورت رفتار عادی دارد.
  ​
* سوگیری: خطاهای سیستماتیک در خروجی‌های مدل‌های هوش مصنوعی که می‌توانند منجر به نتایج ناعادلانه یا تبعیض‌آمیز برای گروه‌های خاص یا در زمینه‌های مشخص شوند.
  ​
* بهره‌برداری از سوگیری: یک تکنیک حمله است که از سوگیری‌های شناخته شده در مدل‌های هوش مصنوعی برای دستکاری نتایج یا خروجی‌ها استفاده می‌کند.
  ​
* سیدار: زبان سیاست و موتور آمازون برای مجوزهای دقیق که در پیاده‌سازی ABAC برای سیستم‌های هوش مصنوعی استفاده می‌شود.
  ​
* زنجیره تفکر: تکنیکی برای بهبود استدلال در مدل‌های زبانی با تولید مراحل میانی استدلال قبل از ارائه پاسخ نهایی.
  ​
* قطع‌کننده‌های مدار: مکانیزم‌هایی که به‌طور خودکار عملیات سیستم هوش مصنوعی را زمانی که آستانه‌های خاصی از ریسک فراتر رود، متوقف می‌کنند.
  ​
* نشت داده: افشای ناخواسته اطلاعات حساس از طریق خروجی‌ها یا رفتار مدل هوش مصنوعی.
  ​
* آلوده‌سازی داده‌ها: فساد عمدی داده‌های آموزشی برای به خطر انداختن یکپارچگی مدل، که اغلب برای نصب درهای پشتی یا کاهش عملکرد انجام می‌شود.
  ​
* حریم خصوصی تفاضلی – حریم خصوصی تفاضلی چارچوبی ریاضی و دقیق برای ارائه اطلاعات آماری درباره مجموعه داده‌ها است که در عین حال حفظ حریم خصوصی افراد تشکیل‌دهنده داده‌ها را تضمین می‌کند. این روش به دارنده داده اجازه می‌دهد تا الگوهای کلی گروه را به اشتراک بگذارد، در حالی که اطلاعات مربوط به افراد خاص را محدود می‌کند.
  ​
* بردارهای جاسازی‌شده: نمایش‌های برداری متراکم از داده‌ها (متن، تصاویر و غیره) که معنای معنایی را در یک فضای ابعاد بالا ضبط می‌کنند.
  ​
* قابل توضیح بودن – قابل توضیح بودن در هوش مصنوعی به توانایی یک سیستم هوش مصنوعی در ارائه دلایل قابل درک برای انسان‌ها درباره تصمیمات و پیش‌بینی‌هایش گفته می‌شود که بینشی درباره عملکرد داخلی آن فراهم می‌کند.
  ​
* هوش مصنوعی قابل توضیح (XAI): سیستم‌های هوش مصنوعی طراحی شده برای ارائه توضیحات قابل فهم برای انسان درباره تصمیمات و رفتارهای خود از طریق تکنیک‌ها و چارچوب‌های مختلف.
  ​
* یادگیری فدرال: رویکردی در یادگیری ماشین که در آن مدل‌ها در چندین دستگاه غیرمتمرکز با نگهداری نمونه‌های داده محلی آموزش داده می‌شوند، بدون اینکه داده‌ها به صورت مستقیم تبادل شوند.
  ​
* خط قرمزها: محدودیت‌هایی که برای جلوگیری از تولید خروجی‌های مضر، جانبدارانه یا به‌طور کلی ناخواسته توسط سیستم‌های هوش مصنوعی اعمال می‌شوند.
  ​
* توهم – توهم در هوش مصنوعی به پدیده‌ای اشاره دارد که در آن یک مدل هوش مصنوعی اطلاعات نادرست یا گمراه‌کننده‌ای تولید می‌کند که بر اساس داده‌های آموزشی یا واقعیت‌های عینی نیست.
  ​
* انسان در حلقه (HITL): سیستم‌هایی که به گونه‌ای طراحی شده‌اند که به نظارت، تأیید یا دخالت انسانی در نقاط تصمیم‌گیری حیاتی نیاز دارند.
  ​
* زیرساخت به‌عنوان کد (IaC): مدیریت و تهیه زیرساخت از طریق کد به جای فرآیندهای دستی، که امکان اسکن امنیتی و استقرارهای سازگار را فراهم می‌کند.
  ​
* جیل‌بریک: تکنیک‌هایی که برای دور زدن محدودیت‌های امنیتی در سیستم‌های هوش مصنوعی، به‌ویژه در مدل‌های زبانی بزرگ، به‌منظور تولید محتوای ممنوعه استفاده می‌شوند.
  ​
* کمترین امتیاز: اصل امنیتی اعطای تنها حداقل دسترسی‌های لازم برای کاربران و فرایندها.
  ​
* LIME (توضیحات محلی قابل تفسیر و مدل-مستقل): تکنیکی برای توضیح پیش‌بینی‌های هر دسته‌بند یادگیری ماشین با تقریب زدن محلی آن با یک مدل قابل تفسیر.
  ​
* حمله استنتاج عضویت: حمله‌ای که هدف آن تعیین این است که آیا یک نقطه داده خاص برای آموزش یک مدل یادگیری ماشین مورد استفاده قرار گرفته است یا خیر.
  ​
* MITRE ATLAS: چشم‌انداز تهدیدات جعلی برای سیستم‌های هوش مصنوعی؛ یک پایگاه دانش از تاکتیک‌ها و تکنیک‌های ضد سیستم‌های هوش مصنوعی.
  ​
* کارت مدل – کارت مدل یک سند است که اطلاعات استاندارد شده‌ای درباره عملکرد مدل هوش مصنوعی، محدودیت‌ها، کاربردهای مورد نظر و ملاحظات اخلاقی ارائه می‌دهد تا شفافیت و توسعه مسئولانه هوش مصنوعی را ترویج کند.
  ​
* استخراج مدل: حمله‌ای که در آن یک مهاجم به طور مکرر مدل هدف را پرس‌وجو می‌کند تا نسخه‌ای عملکردی مشابه بدون مجوز ایجاد کند.
  ​
* معکوس‌سازی مدل: حمله‌ای که تلاش می‌کند داده‌های آموزشی را با تحلیل خروجی‌های مدل بازسازی کند.
  ​
* مدیریت چرخه عمر مدل – مدیریت چرخه عمر مدل هوش مصنوعی فرآیندی است که بر تمام مراحل وجود یک مدل هوش مصنوعی نظارت دارد، از جمله طراحی، توسعه، استقرار، پایش، نگهداری و نهایتاً بازنشستگی آن، تا اطمینان حاصل شود که مدل مؤثر باقی می‌ماند و با اهداف همراستا است.
  ​
* سمی‌سازی مدل: وارد کردن آسیب‌پذیری‌ها یا درهای پشتی به طور مستقیم به یک مدل در طول فرآیند آموزش.
  ​
* دزدیدن/قاچاق مدل: استخراج یک نسخه یا تقریب از یک مدل اختصاصی از طریق پرس‌وجوهای مکرر.
  ​
* سیستم چندعامله: سیستمی متشکل از چندین عامل هوش مصنوعی تعاملی که هر کدام ممکن است قابلیت‌ها و اهداف متفاوتی داشته باشند.
  ​
* OPA (عامل سیاست باز): یک موتور سیاست متن‌باز است که امکان اعمال سیاست‌های یکپارچه را در سراسر پشته فراهم می‌کند.
  ​
* یادگیری ماشین حفظ حریم خصوصی (PPML): تکنیک‌ها و روش‌هایی برای آموزش و استقرار مدل‌های یادگیری ماشین در حالی که حریم خصوصی داده‌های آموزشی حفظ می‌شود.
  ​
* تزریق پرامپت: نوعی حمله که در آن دستورات مخرب در ورودی‌ها تعبیه می‌شوند تا رفتار مورد نظر مدل را نادیده بگیرند.
  ​
* RAG (تولید افزایش یافته با بازیابی): تکنیکی است که مدل‌های زبان بزرگ را با بازیابی اطلاعات مرتبط از منابع دانش خارجی قبل از تولید پاسخ، بهبود می‌بخشد.
  ​
* رد تئیمینگ: عملیاتی است که شامل آزمایش فعال سیستم‌های هوش مصنوعی با شبیه‌سازی حملات خصمانه برای شناسایی نقاط ضعف می‌باشد.
  ​
* SBOM (فهرست مواد نرم‌افزار): یک رکورد رسمی حاوی جزئیات و روابط زنجیره تأمین اجزای مختلف استفاده‌شده در ساخت نرم‌افزار یا مدل‌های هوش مصنوعی.
  ​
* SHAP (توضیحات افزایشی شاپلِی): رویکردی مبتنی بر نظریه بازی‌ها برای شرح خروجی هر مدل یادگیری ماشین با محاسبه سهم هر ویژگی در پیش‌بینی.
  ​
* حمله زنجیره تامین: به خطر انداختن یک سیستم با هدف قرار دادن عناصر کم‌امنیت‌تر در زنجیره تامین آن، مانند کتابخانه‌های شخص ثالث، مجموعه داده‌ها یا مدل‌های از پیش آموزش‌دیده شده.
  ​
* یادگیری انتقالی: تکنیکی که در آن مدلی که برای یک وظیفه توسعه یافته است، به عنوان نقطه شروع برای مدلی در وظیفه دوم مجدداً استفاده می‌شود.
  ​
* پایگاه داده برداری: یک پایگاه داده تخصصی که برای ذخیره بردارهای با ابعاد بالا (نمایه‌ها) و انجام جستجوهای شباهت با کارایی بالا طراحی شده است.
  ​
* اسکن آسیب‌پذیری: ابزارهای خودکاری که آسیب‌پذیری‌های امنیتی شناخته‌شده در اجزای نرم‌افزاری، از جمله چارچوب‌ها و وابستگی‌های هوش مصنوعی را شناسایی می‌کنند.
  ​
* واترمارکینگ: تکنیک‌هایی برای جاسازی نشانگرهای غیرقابل تشخیص در محتوای تولید شده توسط هوش مصنوعی به منظور ردیابی منبع آن یا تشخیص تولید توسط هوش مصنوعی.
  ​
* آسیب‌پذیری صفر روز: آسیب‌پذیری‌ای که قبلاً ناشناخته بوده و مهاجمان می‌توانند قبل از اینکه توسعه‌دهندگان وصله‌ای ایجاد و منتشر کنند، از آن سوءاستفاده کنند.

