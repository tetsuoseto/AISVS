# 10 مقاومت در برابر حملات مخرب و دفاع از حریم خصوصی

## هدف کنترل

اطمینان حاصل کنید که مدل‌های هوش مصنوعی هنگام مواجهه با حملات دور زدن، استنتاج، استخراج یا مسموم‌سازی، قابل اطمینان، محافظت‌کننده از حریم خصوصی و مقاوم در برابر سوء استفاده باقی بمانند.

---

## 10.1 تطابق و ایمنی مدل

در برابر خروجی‌های مضر یا نقض‌کننده‌ی سیاست‌ها محافظت کنید.

|   #    | توضیحات                                                                                                                                         | سطح | نقش |
| :----: | ----------------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.1.1 | اطمینان حاصل کنید که یک مجموعه تست هماهنگی (پرامپت‌های تیم قرمز، پروب‌های فرار، محتوای ممنوعه) تحت کنترل نسخه است و در هر نسخه مدل اجرا می‌شود. |  1  | D/V |
| 10.1.2 | تأیید کنید که محافظ‌های رد کردن و اتمام ایمن اجرا شده‌اند.                                                                                      |  1  |  D  |
| 10.1.3 | اطمینان حاصل کنید که یک ارزیاب خودکار نرخ محتوای مضر را اندازه‌گیری می‌کند و بازگشت‌های منفی فراتر از آستانه تعیین‌شده را علامت‌گذاری می‌کند.   |  2  | D/V |
| 10.1.4 | اطمینان حاصل کنید که آموزش ضد دورزدن سیستم مستند و قابل بازتولید است.                                                                           |  2  |  D  |
| 10.1.5 | اطمینان حاصل کنید که اثبات‌های رسمی رعایت سیاست یا پایش معتبر، حوزه‌های حیاتی را پوشش می‌دهند.                                                  |  3  |  V  |

---

## 10.2 مقاوم‌سازی در برابر نمونه‌های مخرب

افزایش مقاومت در برابر ورودی‌های دستکاری شده. آموزش مقاومتی قوی و امتیازدهی بنچمارک در حال حاضر بهترین روش‌های مورد استفاده هستند.

|   #    | توضیحات                                                                                                             | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.2.1 | بررسی کنید که مخازن پروژه شامل تنظیمات آموزش تقابلی با دانه‌های قابل بازتولید باشند.                                |  1  |  D  |
| 10.2.2 | اطمینان حاصل کنید که تشخیص مثال‌های متخاصم در خطوط تولید باعث ایجاد هشدارهای مسدودکننده می‌شود.                     |  2  | D/V |
| 10.2.4 | تأیید کنید که اثبات‌های مقاومت تاییدشده یا گواهی‌های بازه‌ای حداقل شامل مهم‌ترین کلاس‌های بحرانی باشند.             |  3  |  V  |
| 10.2.5 | اطمینان حاصل کنید که تست‌های رگرسیون از حملات تطبیقی استفاده می‌کنند تا عدم کاهش قابل سنجش در مقاومت را تأیید کنند. |  3  |  V  |

---

## 10.3 کاهش استنباط عضویت

محدود کردن امکان تصمیم‌گیری درباره اینکه آیا یک رکورد در داده‌های آموزشی بوده است یا خیر. حفظ حریم خصوصی تفاضلی و ماسک کردن نمره اطمینان همچنان مؤثرترین روش‌های دفاعی شناخته شده هستند.

|   #    | توضیحات                                                                                                                 | سطح | نقش |
| :----: | ----------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.3.1 | تأیید کنید که تنظیم منظم‌سازی آنتروپی به ازای هر پرسش یا مقیاس‌بندی دما باعث کاهش پیش‌بینی‌های بیش از حد مطمئن می‌شود.  |  1  |  D  |
| 10.3.2 | اطمینان حاصل کنید که آموزش از بهینه‌سازی گرادیان خصوصی متفاوت با کران ε برای مجموعه داده‌های حساس استفاده می‌کند.       |  2  |  D  |
| 10.3.3 | تأیید کنید که شبیه‌سازی‌های حمله (مدل سایه یا جعبه سیاه) میزان AUC حمله ≤ 0.60 را روی داده‌های نگه‌داشته‌شده نشان دهند. |  2  |  V  |

---

## 10.4 مقاومت در برابر وارونگی مدل

جلوگیری از بازسازی ویژگی‌های خصوصی. بررسی‌های اخیر بر کوتاه‌سازی خروجی و تضمین‌های DP به‌عنوان دفاع‌های عملی تأکید دارند.

|   #    | توضیحات                                                                                                                                  | سطح | نقش |
| :----: | ---------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.4.1 | اطمینان حاصل کنید که ویژگی‌های حساس هرگز به صورت مستقیم خروجی داده نشوند؛ در صورت لزوم، از دسته‌بندی‌ها یا تبدیلات یک‌طرفه استفاده کنید. |  1  |  D  |
| 10.4.2 | تأیید کنید که محدودیت‌های نرخ پرس‌وجو، پرس‌وجوهای تطبیقی تکراری از همان اصل را محدود می‌کنند.                                            |  1  | D/V |
| 10.4.3 | اطمینان حاصل کنید که مدل با نویز حفظ حریم خصوصی آموزش داده شده است.                                                                      |  2  |  D  |

---

## 10.5 دفاع در برابر استخراج مدل

کپی‌برداری غیرمجاز را شناسایی و جلوگیری کنید. استفاده از نشان‌گذاری آبی و تحلیل الگوی پرس‌وجو توصیه می‌شود.

|   #    | توضیحات                                                                                                                                                 | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.5.1 | تأیید کنید که دروازه‌های استنتاج محدودیت‌های نرخ کلی و محدودیت‌های نرخ به ازای هر کلید API را که متناسب با آستانه حفظ مدل تنظیم شده‌اند، اعمال می‌کنند. |  1  |  D  |
| 10.5.2 | تأیید کنید که آمارهای آنتروپی پرس‌وجو و چندگانگی ورودی، یک آشکارساز استخراج خودکار را تغذیه می‌کنند.                                                    |  2  | D/V |
| 10.5.3 | تأیید کنید که واترمارک‌های شکننده یا احتمالاتی می‌توانند با p < 0.01 در ≤ 1 000 پرس‌و‌جو علیه یک نسخه مشکوک اثبات شوند.                                 |  2  |  V  |
| 10.5.4 | اطمینان حاصل کنید که کلیدهای واترمارک و مجموعه‌های تریگر در یک ماژول امنیت سخت‌افزاری ذخیره شده و سالانه چرخش می‌یابند.                                 |  3  |  D  |
| 10.5.5 | اطمینان حاصل کنید که رویدادهای هشدار استخراج شامل کوئری‌های مخرب بوده و با کتابچه‌های راهنمای پاسخ به حادثه یکپارچه شده‌اند.                            |  3  |  V  |

---

## 10.6 تشخیص داده‌های آلوده در زمان استنتاج

شناسایی و خنثی‌سازی ورودی‌های دارای درب‌پشتی یا مسموم شده.

|   #    | توضیحات                                                                                                                                   | سطح | نقش |
| :----: | ----------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.6.1 | اطمینان حاصل کنید که ورودی‌ها قبل از استنتاج مدل از طریق یک آشکارساز ناهنجاری (مانند STRIP، امتیازدهی ثبات) عبور می‌کنند.                 |  1  |  D  |
| 10.6.2 | اطمینان حاصل کنید که آستانه‌های آشکارساز بر روی مجموعه‌های اعتبارسنجی تمیز/مسموم تنظیم شده‌اند تا کمتر از ۵٪ خطاهای مثبت کاذب به دست آید. |  1  |  V  |
| 10.6.3 | بررسی کنید که ورودی‌هایی که به‌عنوان آلوده علامت‌گذاری شده‌اند، موجب فعال شدن جریان‌های کاری مسدودسازی نرم و بازبینی انسانی شوند.         |  2  |  D  |
| 10.6.4 | تأیید کنید که آشکارسازها تحت آزمایش استرس با حملات درب‌پشتی تطبیقی و بدون محرک قرار گرفته‌اند.                                            |  2  |  V  |
| 10.6.5 | اطمینان حاصل کنید که معیارهای اثربخشی تشخیص ثبت شده و به طور دوره‌ای با اطلاعات تهدید جدید بازبینی می‌شوند.                               |  3  |  D  |

---

## 10.7 انطباق پویا سیاست امنیتی

به‌روزرسانی‌های سیاست امنیتی در زمان واقعی بر اساس اطلاعات تهدید و تحلیل رفتاری.

|   #    | توضیحات                                                                                                                                      | سطح | نقش |
| :----: | -------------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.7.1 | اطمینان حاصل کنید که سیاست‌های امنیتی می‌توانند به‌طور پویا بدون راه‌اندازی مجدد عامل به‌روزرسانی شوند در حالی که صحت نسخه سیاست حفظ می‌شود. |  1  | D/V |
| 10.7.2 | اطمینان حاصل کنید که به‌روزرسانی‌های سیاست به‌صورت رمزنگاری‌شده توسط کارکنان امنیتی مجاز امضا شده و قبل از اجرا اعتبارسنجی می‌شوند.          |  2  | D/V |
| 10.7.3 | اطمینان حاصل کنید که تغییرات سیاست پویا با سوابق کامل حسابرسی شامل توجیه، زنجیره‌های تصویب و روش‌های بازگردانی ثبت می‌شوند.                  |  2  | D/V |
| 10.7.4 | تأیید کنید که مکانیزم‌های امنیت تطبیقی حساسیت شناسایی تهدید را بر اساس زمینه ریسک و الگوهای رفتاری تنظیم می‌کنند.                            |  3  | D/V |
| 10.7.5 | تأیید کنید که تصمیمات تطبیق سیاست قابل توضیح باشند و شامل شواهد و مدارک برای بازبینی تیم امنیتی باشند.                                       |  3  | D/V |

---

## 10.8 تحلیل امنیت مبتنی بر انعکاس

اعتبارسنجی امنیت از طریق خودبازتابی عامل و تحلیل فرامعلوماتی.

|   #    | توضیحات                                                                                                                        | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------------------ | :-: | :-: |
| 10.8.1 | تأیید کنید که مکانیزم‌های بازتاب عامل شامل ارزیابی خود متمرکز بر امنیت تصمیمات و اقدامات باشد.                                 |  1  | D/V |
| 10.8.2 | اطمینان حاصل کنید که خروجی‌های بازتابی برای جلوگیری از دستکاری مکانیزم‌های خودارزیابی توسط ورودی‌های مخرب، اعتبارسنجی می‌شوند. |  2  | D/V |
| 10.8.3 | تأیید کنید که تحلیل امنیت فراشناختی، سوگیری، دستکاری یا به خطر افتادن احتمالی در فرآیندهای استدلال عامل را شناسایی می‌کند.     |  2  | D/V |
| 10.8.4 | تأیید کنید که هشدارهای امنیتی مبتنی بر بازتاب، رصد پیشرفته و جریان‌های کاری احتمالی دخالت انسانی را فعال می‌کنند.              |  3  | D/V |
| 10.8.5 | تأیید کنید که یادگیری مستمر از بازتاب‌های امنیتی، تشخیص تهدید را بهبود می‌بخشد بدون اینکه عملکرد مشروع را کاهش دهد.            |  3  | D/V |

---

## 10.9 امنیت تکامل و خود بهبودی

کنترل‌های امنیتی برای سیستم‌های عاملی که توانایی خودتغییری و تکامل دارند.

|   #    | توضیحات                                                                                                       | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.9.1 | تأیید کنید که قابلیت‌های خود-تغییریابی محدود به نواحی امن مشخص شده با مرزهای تأیید رسمی باشند.                |  1  | D/V |
| 10.9.2 | اطمینان حاصل کنید که پیشنهادهای توسعه قبل از اجرا تحت ارزیابی تأثیر امنیتی قرار گیرند.                        |  2  | D/V |
| 10.9.3 | اطمینان حاصل کنید که مکانیزم‌های خودبهبودی شامل قابلیت بازگردانی همراه با بررسی صحت هستند.                    |  2  | D/V |
| 10.9.4 | تأیید کنید که امنیت متا-یادگیری از دستکاری مخرب الگوریتم‌های بهبود جلوگیری می‌کند.                            |  3  | D/V |
| 10.9.5 | اطمینان حاصل کنید که خودبه‌سازی بازگشتی توسط محدودیت‌های رسمی ایمنی با اثبات‌های ریاضی همگرایی محدود شده است. |  3  | D/V |

---

### مراجع

* [MITRE ATLAS adversary tactics for ML](https://atlas.mitre.org/)
* [NIST AI Risk Management Framework 1.0, 2023](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
* [OWASP Top 10 for LLM Applications, 2025](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
* [Adversarial Training: A Survey — Zhao et al., 2024](https://arxiv.org/abs/2410.15042)
* [RobustBench adversarial-robustness benchmark](https://robustbench.github.io/)
* [Membership-Inference & Model-Inversion Risk Survey, 2025](https://www.sciencedirect.com/science/article/abs/pii/S0950705125003867)
* [PURIFIER: Confidence-Score Defense against MI Attacks — AAAI 2023](https://ojs.aaai.org/index.php/AAAI/article/view/26289)
* [Model-Inversion Attacks & Defenses Survey — AI Review, 2025](https://link.springer.com/article/10.1007/s10462-025-11248-0)
* [Comprehensive Defense Framework Against Model Extraction — IEEE TDSC 2024](https://doi.org/10.1109/TDSC.2023.3261327)
* [Fragile Model Watermarking Survey — 2025](https://www.sciencedirect.com/science/article/abs/pii/S0165168425002026)
* [Data Poisoning in Deep Learning: A Survey — Zhao et al., 2025](https://arxiv.org/abs/2503.22759)
* [BDetCLIP: Multimodal Prompting Backdoor Detection — Niu et al., 2024](https://arxiv.org/abs/2405.15269)

