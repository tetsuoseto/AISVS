# 10 مقاومت در برابر حملات مخرب و حفاظت از حریم خصوصی

## هدف کنترل

اطمینان حاصل کنید که مدل‌های هوش مصنوعی در مواجهه با حمله‌های فرار از تشخیص، استنتاج، استخراج یا آلوده‌سازی داده، همچنان قابل اعتماد باشند، حریم خصوصی را حفظ کنند و در برابر سوءاستفاده مقاوم بمانند.

---

## 10.1 همسویی مدل و ایمنی

از خروجی‌های مضر یا خروجی‌های نقض‌کننده سیاست جلوگیری کنید.

|   #    | شرح                                                                                                                                                          | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | :-: | :-: |
| 10.1.1 | تأیید کنید که مجموعه‌ای از آزمون‌های هم‌راستایی (پرومپت‌های تیم قرمز، آزمایش‌های جیلبریک، محتوای ممنوع) دارای کنترل نسخه است و در هر انتشار مدل اجرا می‌شود. |  1  | D/V |
| 10.1.2 | اطمینان حاصل کنید که ریل‌های حفاظتی برای امتناع و تکمیل ایمن اعمال می‌شوند.                                                                                  |  1  |  D  |
| 10.1.3 | تأیید کنید که یک ارزیاب خودکار نرخ محتوای مضر را اندازه‌گیری کند و افت‌های عملکردی را که فراتر از آستانه‌ای مشخص هستند، علامت‌گذاری کند.                     |  2  | D/V |
| 10.1.4 | تایید کنید که آموزش مقابله با جیل‌بریک مستند و قابل بازتولید است.                                                                                            |  2  |  D  |
| 10.1.5 | اطمینان حاصل کنید که اثبات‌های رسمی انطباق با سیاست یا پایش گواهی‌شده دامنه‌های حیاتی را پوشش می‌دهند.                                                       |  3  |  V  |

---

## 10.2 سخت‌سازی-نمونه‌های حمله‌ای

افزایش مقاومت در برابر ورودی‌های دستکاری‌شده. آموزش مقاوم در برابر حملات adversarial و امتیازدهی مبتنی بر بنچمارک، بهترین شیوه‌های کنونی هستند.

|   #    | شرح                                                                                                                    | سطح | نقش |
| :----: | ---------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.2.1 | بررسی کنید که آیا مخازن پروژه شامل پیکربندی‌های آموزش خصمانه با بذرهای تکرارپذیر هستند.                                |  1  |  D  |
| 10.2.2 | تأیید کنید که تشخیص نمونه‌های حمله‌ای در خط لوله‌های تولید، هشدارهای انسدادی ایجاد می‌کند.                             |  2  | D/V |
| 10.2.4 | بررسی کنید که اثبات‌های مقاومت گواهی‌شده یا گواهی‌های محدودۀ بازه‌ای حداقل کلاس‌های بحرانیِ برتر را پوشش می‌دهند.      |  3  |  V  |
| 10.2.5 | اطمینان حاصل کنید که تست‌های رگرسیون از حملات تطبیقی استفاده می‌کنند تا هیچ کاهش قابل اندازه‌گیری در مقاومت تأیید شود. |  3  |  V  |

---

## 10.3 کاهش عضویت-استنتاج

توانایی تشخیص اینکه آیا یک رکورد در داده‌های آموزشی وجود داشته است را محدود کنید. خصوصی‌سازی تفاضلی و ماسک‌گذاری نمره اعتماد همچنان مؤثرترین دفاع‌های شناخته‌شده هستند.

|   #    | شرح                                                                                                                      | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------------ | :-: | :-: |
| 10.3.1 | بررسی کنید که تنظیم آنتروپی برای هر کوئری یا مقیاس‌گذاری با دما، پیش‌بینی‌های بیش‌ازحد مطمئن را کاهش می‌دهد.             |  1  |  D  |
| 10.3.2 | تأیید کنید که فرایند آموزش از بهینه‌سازی با حریم خصوصی تفاضلی با محدودیت ε برای داده‌های حساس استفاده می‌کند.            |  2  |  D  |
| 10.3.3 | تأیید کنید که شبیه‌سازی‌های حمله (مدل سایه‌ای یا جعبه سیاه) AUC حمله را بر روی داده‌های نگهداری‌شده ≤ 0.60 نشان می‌دهند. |  2  |  V  |

---

## 10.4 مقاومت-در برابر معکوس‌سازی مدل

جلوگیری از بازسازی ویژگی‌های خصوصی. بررسی‌های اخیر بر قطع خروجی و ضمانت‌های حریم خصوصی تفاضلی به‌عنوان دفاع‌های عملی تأکید می‌کنند.

|   #    | شرح                                                                                                                                | سطح | نقش |
| :----: | ---------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.4.1 | اطمینان حاصل کنید که ویژگی‌های حساس هرگز به طور مستقیم خروجی داده نشوند؛ در صورت نیاز، از باکت‌ها یا تبدیلات یک‌طرفه استفاده کنید. |  1  |  D  |
| 10.4.2 | بررسی کنید که محدودیت‌های نرخ کوئری، کوئری‌های تطبیقی مکرر را از همان کاربر معتبر کند.                                             |  1  | D/V |
| 10.4.3 | تأیید کنید که مدل با نویز حفظ حریم خصوصی آموزش دیده است.                                                                           |  2  |  D  |

---

## 10.5 مدل-استخراج دفاع

تشخیص و جلوگیری از کپی‌برداری غیرمجاز. واترمارک‌گذاری و تحلیل الگوهای پرس‌وجو توصیه می‌شود.

|   #    | شرح                                                                                                                                                    | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------------------------ | :-: | :-: |
| 10.5.1 | تأیید کنید که درگاه‌های استنتاج محدودیت‌های نرخ جهانی و محدودیت‌های نرخ برای هر کلید API را اعمال می‌کنند که با آستانهٔ حافظه‌پذیری مدل تنظیم شده‌اند. |  1  |  D  |
| 10.5.2 | اطمینان حاصل کنید که آمارهای query-entropy و input-plurality یک آشکارساز استخراج خودکار را تغذیه می‌کنند.                                              |  2  | D/V |
| 10.5.3 | تصدیق کنید که واترمارک‌های شکننده یا احتمالی می‌توانند با p < 0.01 در ≤ 1 000 کوئری در برابر کلون مشکوک ثابت شوند.                                     |  2  |  V  |
| 10.5.4 | اطمینان حاصل کنید که کلیدهای واترمارک و مجموعه‌های محرک در یک ماژول امنیتی سخت‌افزاری نگهداری می‌شوند و هر ساله چرخانده می‌شوند.                       |  3  |  D  |
| 10.5.5 | بررسی کنید که رویدادهای هشدار استخراج شامل کوئری‌های مخرب هستند و با کتاب‌های راهنمای پاسخ به حوادث یکپارچه شده‌اند.                                   |  3  |  V  |

---

## 10.6 تشخیص داده‌های مسموم در زمان استنتاج

ورودی‌های دارای در پشتی یا آلوده را شناسایی و خنثی کنید.

|   #    | شرح                                                                                                                                  | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------ | :-: | :-: |
| 10.6.1 | تأیید کنید که ورودی‌ها قبل از استنتاج مدل از طریق یک تشخیص‌دهندهٔ ناهنجاری عبور می‌کنند (مثلاً STRIP، ارزیابی سازگاری).              |  1  |  D  |
| 10.6.2 | تایید کنید که آستانه‌های تشخیصگر روی مجموعه‌های اعتبارسنجی سالم و آلوده تنظیم شده‌اند تا نرخ مثبت‌های کاذب را به کمتر از 5% برسانند. |  1  |  V  |
| 10.6.3 | تأیید کنید که ورودی‌های مسموم علامت‌گذاری‌شده باعث فعال‌شدن مسدودسازی نرم و گردش‌های کار بازبینی انسانی شوند.                        |  2  |  D  |
| 10.6.4 | تأیید کنید که تشخیص‌دهنده‌ها با حملات درب پشتی سازگار و بدون محرک تحت آزمایش فشار قرار می‌گیرند.                                     |  2  |  V  |
| 10.6.5 | اطمینان حاصل کنید که معیارهای اثربخشی تشخیص ثبت می‌شوند و به طور دوره‌ای با اطلاعات تهدیدی تازه دوباره ارزیابی می‌شوند.              |  3  |  D  |

---

## 10.7 تطبیق سیاست امنیتی پویا

به‌روزرسانی‌های سیاست امنیتی در زمان واقعی بر اساس اطلاعات تهدید و تحلیل رفتاری.

|   #    | شرح                                                                                                                                          | سطح | نقش |
| :----: | -------------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.7.1 | تأیید کنید که سیاست‌های امنیتی می‌توانند به‌طور پویا بدون نیاز به راه‌اندازی مجدد عامل به‌روزرسانی شوند، در عین حفظ یکپارچگی نسخه‌های سیاست. |  1  | D/V |
| 10.7.2 | اطمینان حاصل کنید که به‌روزرسانی‌های سیاست با امضای دیجیتال از سوی کارکنان امنیتی مجاز امضا می‌شوند و پیش از اجرا اعتبارسنجی می‌شوند.        |  2  | D/V |
| 10.7.3 | اطمینان حاصل کنید که تغییرات پویا در سیاست‌ها با سوابق حسابرسی کامل ثبت می‌شوند، از جمله توجیه، زنجیره‌های تصویب، و رویه‌های بازگردانی.      |  2  | D/V |
| 10.7.4 | تأیید کنید که مکانیزم‌های امنیتی تطبیقی حساسیت تشخیص تهدید را بر اساس زمینه ریسک و الگوهای رفتاری تنظیم می‌کنند.                             |  3  | D/V |
| 10.7.5 | تأیید کنید که تصمیم‌های تطبیق سیاست قابل توضیح هستند و برای بازبینی تیم امنیت، مسیرهای شواهدی را ارائه دهید.                                 |  3  | D/V |

---

## 10.8 تحلیل امنیتی مبتنی بر بازتاب

اعتبارسنجی امنیتی از طریق خودبازنگری عامل و تحلیل فراشناختی.

|   #    | شرح                                                                                                                     | سطح | نقش |
| :----: | ----------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.8.1 | تأیید کنید که مکانیزم‌های بازتاب عامل شامل خودارزیابی امنیت‌محور تصمیم‌ها و اقدامات هستند.                              |  1  | D/V |
| 10.8.2 | بررسی کنید که خروجی‌های بازتاب اعتبارسنجی شوند تا از دستکاری مکانیسم‌های خودارزیابی توسط ورودی‌های مخرب جلوگیری شود.    |  2  | D/V |
| 10.8.3 | تأیید کنید که تحلیل امنیتی متا-شناختی سوگیری احتمالی، دستکاری یا نقض امنیت در فرایندهای استدلال عامل را شناسایی می‌کند. |  2  | D/V |
| 10.8.4 | تأیید کنید که هشدارهای امنیتی مبتنی بر بازتاب، نظارت تقویت‌شده و گردش‌های کاری مداخله انسانی احتمالی را تحریک می‌کنند.  |  3  | D/V |
| 10.8.5 | بررسی کنید که یادگیری مداوم از بازتاب‌های امنیتی تشخیص تهدید را بهبود می‌دهد بدون تضعیف عملکرد مشروع.                   |  3  | D/V |

---

## 10.9 تکامل و امنیت خود-بهبود

کنترل‌های امنیتی برای سیستم‌های مبتنی بر عامل که قادر به خودتغییری و تکامل هستند.

|   #    | شرح                                                                                                                    | سطح | نقش |
| :----: | ---------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.9.1 | بررسی کنید که قابلیت‌های خودتغییری تنها در ناحیه‌های امن تعیین‌شده محدود شده‌اند و دارای مرزهای اعتبارسنجی رسمی هستند. |  1  | D/V |
| 10.9.2 | اطمینان حاصل کنید که پیشنهادهای تکاملی پیش از پیاده‌سازی از ارزیابی تأثیر امنیتی عبور می‌کنند.                         |  2  | D/V |
| 10.9.3 | تأیید کنید که مکانیسم‌های بهبود خودکار شامل قابلیت‌های بازگردانی با اعتبارسنجی یکپارچگی هستند.                         |  2  | D/V |
| 10.9.4 | اطمینان حاصل کنید که امنیت یادگیری متا از دستکاری خصمانه الگوریتم‌های بهبود جلوگیری می‌کند.                            |  3  | D/V |
| 10.9.5 | تأیید کنید که بهبود بازگشتیِ خود با محدودیت‌های ایمنی رسمی محدود شده باشد و با اثبات‌های ریاضیِ همگرایی همراه گردد.    |  3  | D/V |

---

### منابع

* [MITRE ATLAS adversary tactics for ML](https://atlas.mitre.org/)
* [NIST AI Risk Management Framework 1.0, 2023](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
* [OWASP Top 10 for LLM Applications, 2025](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
* [Adversarial Training: A Survey — Zhao et al., 2024](https://arxiv.org/abs/2410.15042)
* [RobustBench adversarial-robustness benchmark](https://robustbench.github.io/)
* [Membership-Inference & Model-Inversion Risk Survey, 2025](https://www.sciencedirect.com/science/article/abs/pii/S0950705125003867)
* [PURIFIER: Confidence-Score Defense against MI Attacks — AAAI 2023](https://ojs.aaai.org/index.php/AAAI/article/view/26289)
* [Model-Inversion Attacks & Defenses Survey — AI Review, 2025](https://link.springer.com/article/10.1007/s10462-025-11248-0)
* [Comprehensive Defense Framework Against Model Extraction — IEEE TDSC 2024](https://doi.org/10.1109/TDSC.2023.3261327)
* [Fragile Model Watermarking Survey — 2025](https://www.sciencedirect.com/science/article/abs/pii/S0165168425002026)
* [Data Poisoning in Deep Learning: A Survey — Zhao et al., 2025](https://arxiv.org/abs/2503.22759)
* [BDetCLIP: Multimodal Prompting Backdoor Detection — Niu et al., 2024](https://arxiv.org/abs/2405.15269)

