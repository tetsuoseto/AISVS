# 10 مقاومت مقابله‌ای و محافظت از حریم خصوصی

## هدف کنترل

اطمینان حاصل کنید که مدل‌های هوش مصنوعی در مواجهه با حملات فرار، استنباط، استخراج یا مسموم‌سازی، همچنان قابل اطمینان، حفظ‌کننده حریم خصوصی و مقاوم در برابر سوءاستفاده باقی بمانند.

---

## 10.1 همسویی مدل و ایمنی

از ایجاد خروجی‌های مضر یا مغایر با سیاست‌ها جلوگیری کنید.

|   #    | توضیحات                                                                                                                                          | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------------------ | :-: | :-: |
| 10.1.1 | اطمینان حاصل کنید که یک مجموعه آزمایشی هماهنگی (پرامپت‌های تیم سرخ، آزمایش‌های نفوذ، محتوای ممنوعه) کنترل نسخه شده و در هر نسخه مدل اجرا می‌شود. |  1  | D/V |
| 10.1.2 | تأیید کنید که موانع جلوگیری از رد و تکمیل ایمن اعمال شده‌اند.                                                                                    |  1  |  D  |
| 10.1.3 | تأیید کنید که یک ارزیاب خودکار نرخ محتوای مضر را اندازه‌گیری کرده و پسرفت‌ها را فراتر از آستانه تعیین شده علامت‌گذاری می‌کند.                    |  2  | D/V |
| 10.1.4 | اطمینان حاصل کنید که آموزش مقابله با دور زدن زندان مستند و قابل بازتولید باشد.                                                                   |  2  |  D  |
| 10.1.5 | تأیید کنید که اثبات‌های انطباق با سیاست رسمی یا نظارت گواهی‌شده حوزه‌های حیاتی را پوشش می‌دهند.                                                  |  3  |  V  |

---

## 10.2 سخت‌سازی نمونه‌های خصمانه

افزایش مقاومت در برابر ورودی‌های دستکاری شده. آموزش مقاوم در برابر حملات خصمانه و امتیازدهی بنچمارک بهترین روش‌های فعلی هستند.

|   #    | توضیحات                                                                                                               | سطح | نقش |
| :----: | --------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.2.1 | تأیید کنید که مخازن پروژه شامل تنظیمات آموزش خصمانه با بذرهای قابل بازتولید باشند.                                    |  1  |  D  |
| 10.2.2 | تأیید کنید که تشخیص نمونه‌های مخرب در خطوط تولید هشدارهای مسدودکننده را ایجاد می‌کند.                                 |  2  | D/V |
| 10.2.4 | تأیید کنید که اثبات‌های پایداری گواهی‌شده یا گواهی‌های بازه‌ای حداقل کلاس‌های بحرانی برتر را پوشش می‌دهند.            |  3  |  V  |
| 10.2.5 | تأیید کنید که آزمایش‌های رگرسیون از حملات تطبیقی برای اطمینان از عدم کاهش قابل اندازه‌گیری در مقاومت استفاده می‌کنند. |  3  |  V  |

---

## ۱۰.۳ کاهش استنباط عضویت

محدود کردن توانایی تصمیم‌گیری در مورد اینکه آیا یک رکورد در داده‌های آموزشی بوده است. حفظ حریم خصوصی تفاضلی و ماسک کردن امتیاز اطمینان همچنان مؤثرترین دفاع‌های شناخته‌شده هستند.

|   #    | توضیحات                                                                                                                                      | سطح | نقش |
| :----: | -------------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.3.1 | اطمینان حاصل کنید که تنظیم آنتروپی به ازای هر پرسش یا مقیاس‌دهی دما، پیش‌بینی‌های بیش از حد مطمئن را کاهش می‌دهد.                            |  1  |  D  |
| 10.3.2 | تأیید کنید که آموزش از بهینه‌سازی خصوصی متفاوت با کران ε برای داده‌های حساس استفاده می‌کند.                                                  |  2  |  D  |
| 10.3.3 | اطمینان حاصل کنید که شبیه‌سازی‌های حمله (مدل سایه یا جعبه سیاه) نشان می‌دهند که AUC حمله کمتر یا مساوی 0.60 بر روی داده‌های نگهداری شده است. |  2  |  V  |

---

## 10.4 مقاومت در برابر معکوس‌سازی مدل

جلوگیری از بازسازی ویژگی‌های خصوصی. نظرسنجی‌های اخیر تأکید می‌کنند که قطع خروجی و تضمین‌های DP به‌عنوان روش‌های دفاعی عملی محسوب می‌شوند.

|   #    | توضیحات                                                                                                                           | سطح | نقش |
| :----: | --------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.4.1 | اطمینان حاصل کنید که ویژگی‌های حساس هرگز به طور مستقیم خروجی داده نشوند؛ در صورت نیاز، از سطل‌ها یا تبدیلات یک‌طرفه استفاده کنید. |  1  |  D  |
| 10.4.2 | تأیید کنید که محدودیت‌های نرخ پرس و جو، پرس و جوهای تطبیقی مکرر از همان محور را محدود می‌کنند.                                    |  1  | D/V |
| 10.4.3 | بررسی کنید که مدل با نویز حفظ حریم خصوصی آموزش دیده است.                                                                          |  2  |  D  |

---

## 10.5 دفاع استخراج مدل

شناسایی و جلوگیری از کلونینگ غیرمجاز. پیشنهاد می‌شود از واترمارکینگ و تحلیل الگوی پرس‌وجو استفاده شود.

|   #    | توضیحات                                                                                                                                         | سطح | نقش |
| :----: | ----------------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.5.1 | بررسی کنید که دروازه‌های استنتاج محدودیت‌های نرخ جهانی و محدودیت‌های نرخ هر کلید API را که با آستانه حفظ حافظه مدل تنظیم شده‌اند، اجرا می‌کنند. |  1  |  D  |
| 10.5.2 | تأیید کنید که آمارهای انتروپی پرس‌وجو و کثرت ورودی به یک آشکارساز استخراج خودکار تغذیه می‌شوند.                                                 |  2  | D/V |
| 10.5.3 | تأیید کنید که واترمارک‌های شکننده یا احتمالاتی می‌توانند با p < 0.01 در تعداد ≤ 1 000 پرس‌و‌جو علیه یک کلون مشکوک ثابت شوند.                    |  2  |  V  |
| 10.5.4 | اطمینان حاصل کنید که کلیدهای واترمارک و مجموعه‌های محرک در ماژول امنیت سخت‌افزاری ذخیره شده و به صورت سالیانه چرخش می‌یابند.                    |  3  |  D  |
| 10.5.5 | اطمینان حاصل کنید که رویدادهای extraction-alert شامل کوئری‌های متخلف هستند و با کتابچه‌های پاسخ به حادثه یکپارچه شده‌اند.                       |  3  |  V  |

---

## 10.6 تشخیص داده‌های آلوده در زمان استنتاج

شناسایی و بی‌اثر کردن ورودی‌های دارای درهای پشتی یا آلوده شده.

|   #    | توضیحات                                                                                                                         | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.6.1 | اطمینان حاصل کنید که ورودی‌ها قبل از استنتاج مدل از طریق یک آشکارساز ناهنجاری (مثلاً STRIP، امتیازدهی ثبات) عبور کنند.          |  1  |  D  |
| 10.6.2 | اطمینان حاصل کنید که آستانه‌های آشکارساز بر روی مجموعه‌های اعتبارسنجی پاک/آلوده تنظیم شده‌اند تا کمتر از ۵٪ مثبت کاذب حاصل شود. |  1  |  V  |
| 10.6.3 | تأیید کنید که ورودی‌هایی که به‌عنوان مسموم علامت‌گذاری شده‌اند، باعث فعال شدن روندهای مسدودسازی نرم و بازبینی انسانی می‌شوند.   |  2  |  D  |
| 10.6.4 | تأیید کنید که شناسایی‌کننده‌ها با حملات پشت‌در بدون محرک و تطبیقی تحت فشار قرار گرفته‌اند.                                      |  2  |  V  |
| 10.6.5 | اطمینان حاصل کنید که معیارهای اثربخشی شناسایی ثبت می‌شوند و به‌طور دوره‌ای با اطلاعات تهدید جدید دوباره ارزیابی می‌گردند.       |  3  |  D  |

---

## 10.7 سازگاری پویا با سیاست امنیتی

به‌روزرسانی‌های سیاست امنیتی در زمان واقعی مبتنی بر اطلاعات تهدید و تحلیل رفتاری.

|   #    | توضیحات                                                                                                                                        | سطح | نقش |
| :----: | ---------------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.7.1 | تأیید کنید که سیاست‌های امنیتی به‌صورت پویا و بدون نیاز به راه‌اندازی مجدد عامل قابل به‌روزرسانی باشند در حالی که یکپارچگی نسخه سیاست حفظ شود. |  1  | D/V |
| 10.7.2 | اطمینان حاصل کنید که به‌روزرسانی‌های سیاست به‌صورت رمزنگاری شده توسط پرسنل امنیتی مجاز امضا شده و قبل از اعمال اعتبارسنجی شوند.                |  2  | D/V |
| 10.7.3 | اطمینان حاصل کنید که تغییرات سیاست‌های پویا با سوابق کامل حسابرسی شامل توجیه، زنجیره‌های تأیید و روش‌های بازگشت ثبت می‌شوند.                   |  2  | D/V |
| 10.7.4 | تأیید کنید که مکانیزم‌های امنیت تطبیقی حساسیت تشخیص تهدید را بر اساس زمینه ریسک و الگوهای رفتاری تنظیم می‌کنند.                                |  3  | D/V |
| 10.7.5 | اطمینان حاصل کنید که تصمیمات تطبیق سیاست قابل توضیح باشند و شامل مدارک مستندی برای بازبینی تیم امنیتی باشند.                                   |  3  | D/V |

---

## 10.8 تحلیل امنیت مبتنی بر بازتاب

اعتبارسنجی امنیت از طریق خودبازتابی عامل و تحلیل فراروشناختی.

|   #    | توضیحات                                                                                                                           | سطح | نقش |
| :----: | --------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.8.1 | اطمینان حاصل کنید که مکانیسم‌های بازتاب عامل شامل خودارزیابی متمرکز بر امنیت تصمیمات و اقدامات است.                               |  1  | D/V |
| 10.8.2 | اطمینان حاصل کنید که خروجی‌های بازتابی تأیید می‌شوند تا از دستکاری مکانیزم‌های خودارزیابی توسط ورودی‌های خصمانه جلوگیری شود.      |  2  | D/V |
| 10.8.3 | اطمینان حاصل کنید که تحلیل امنیت متاکاگنیتیو، سوگیری، دستکاری یا به خطر افتادن احتمالی در روندهای استدلال عامل را شناسایی می‌کند. |  2  | D/V |
| 10.8.4 | اطمینان حاصل کنید که هشدارهای امنیتی مبتنی بر بازتاب، نظارت پیشرفته و گردش‌کارهای احتمالی دخالت انسانی را فعال می‌کنند.           |  3  | D/V |
| 10.8.5 | تأیید کنید که یادگیری مداوم از بازتاب‌های امنیتی باعث بهبود شناسایی تهدیدات می‌شود بدون اینکه عملکرد مشروع را کاهش دهد.           |  3  | D/V |

---

## 10.9 امنیت تکامل و بهبود خودکار

کنترل‌های امنیتی برای سیستم‌های عاملی که قادر به خودتغییری و تکامل هستند.

|   #    | توضیحات                                                                                                             | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.9.1 | اطمینان حاصل کنید که قابلیت‌های خودتعمیر محدود به مناطق ایمن مشخص شده با مرزهای تأییدیه رسمی هستند.                 |  1  | D/V |
| 10.9.2 | اطمینان حاصل کنید که پیشنهادات تکاملی قبل از اجرا تحت ارزیابی تأثیر امنیتی قرار می‌گیرند.                           |  2  | D/V |
| 10.9.3 | تأیید کنید که مکانیزم‌های خودبهبودی شامل قابلیت بازگردانی با تأیید صحت باشند.                                       |  2  | D/V |
| 10.9.4 | اطمینان حاصل کنید که امنیت متا-یادگیری از دستکاری خصمانه الگوریتم‌های بهبود جلوگیری می‌کند.                         |  3  | D/V |
| 10.9.5 | تأیید کنید که بهبود خود بازگشتی تحت محدودیت‌های ایمنی رسمی قرار دارد و با اثبات‌های ریاضی همگرایی آن تضمین شده است. |  3  | D/V |

---

### مراجع

* [MITRE ATLAS adversary tactics for ML](https://atlas.mitre.org/)
* [NIST AI Risk Management Framework 1.0, 2023](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
* [OWASP Top 10 for LLM Applications, 2025](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
* [Adversarial Training: A Survey — Zhao et al., 2024](https://arxiv.org/abs/2410.15042)
* [RobustBench adversarial-robustness benchmark](https://robustbench.github.io/)
* [Membership-Inference & Model-Inversion Risk Survey, 2025](https://www.sciencedirect.com/science/article/abs/pii/S0950705125003867)
* [PURIFIER: Confidence-Score Defense against MI Attacks — AAAI 2023](https://ojs.aaai.org/index.php/AAAI/article/view/26289)
* [Model-Inversion Attacks & Defenses Survey — AI Review, 2025](https://link.springer.com/article/10.1007/s10462-025-11248-0)
* [Comprehensive Defense Framework Against Model Extraction — IEEE TDSC 2024](https://doi.org/10.1109/TDSC.2023.3261327)
* [Fragile Model Watermarking Survey — 2025](https://www.sciencedirect.com/science/article/abs/pii/S0165168425002026)
* [Data Poisoning in Deep Learning: A Survey — Zhao et al., 2025](https://arxiv.org/abs/2503.22759)
* [BDetCLIP: Multimodal Prompting Backdoor Detection — Niu et al., 2024](https://arxiv.org/abs/2405.15269)

