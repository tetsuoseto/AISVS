# 10 مقاومت در مقابل حملات متقابل و دفاع از حریم خصوصی

## هدف کنترل

اطمینان حاصل کنید که مدل‌های هوش مصنوعی هنگام مواجهه با حملات دور زدن، استنتاج، استخراج، یا مسموم‌سازی، قابل اعتماد، حفظ‌کننده حریم خصوصی و مقاوم در برابر سوءاستفاده باقی بمانند.

---

## ۱۰.۱ همسویی مدل و ایمنی

مراقب خروجی‌های مضر یا نقض‌کننده سیاست‌ها باشید.

|   #    | توضیحات                                                                                                                                                      | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | :-: | :-: |
| 10.1.1 | اطمینان حاصل کنید که یک مجموعه آزمون هم‌راستایی (دستورات تیم قرمز، آزمایش‌های فرار از محدودیت، محتوای ممنوعه) نسخه‌گذاری شده و در هر انتشار مدل اجرا می‌شود. |  1  | D/V |
| 10.1.2 | تأیید کنید که موانع جلوگیری از انکار و تکمیل ایمن اجرا شده باشند.                                                                                            |  1  |  D  |
| 10.1.3 | تأیید کنید که یک ارزیاب خودکار نرخ محتوای مضر را اندازه‌گیری کرده و پسرفت‌ها را فراتر از آستانه تعیین‌شده علامت‌گذاری می‌کند.                                |  2  | D/V |
| 10.1.4 | تأیید کنید که آموزش ضد شکستن محدودیت‌ها مستند و قابل تکرار است.                                                                                              |  2  |  D  |
| 10.1.5 | اطمینان حاصل کنید که اثبات‌های رسمی انطباق با سیاست یا نظارت گواهی‌شده، حوزه‌های حیاتی را پوشش می‌دهند.                                                      |  3  |  V  |

---

## 10.2 سخت‌سازی نمونه‌های خصمانه

افزایش مقاومت در برابر ورودی‌های دستکاری‌شده. آموزش مقاومتی پیشرفته و امتیازدهی معیارهای ارزیابی، بهترین روش فعلی هستند.

|   #    | توضیحات                                                                                                                         | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.2.1 | اطمینان حاصل کنید که مخازن پروژه شامل پیکربندی‌های آموزش خصمانه با بذرهای قابل تکرار هستند.                                     |  1  |  D  |
| 10.2.2 | اطمینان حاصل کنید که تشخیص نمونه‌های خصمانه در خطوط تولید هشدارهای مسدودکننده ایجاد می‌کند.                                     |  2  | D/V |
| 10.2.4 | اطمینان حاصل کنید که اثبات‌های پایداری تاییدشده یا گواهی‌های محدودبندی فاصله حداقل کلاس‌های بحرانی برتر را پوشش می‌دهند.        |  3  |  V  |
| 10.2.5 | اطمینان حاصل کنید که تست‌های رگرسیون از حملات تطبیقی استفاده می‌کنند تا عدم وجود کاهش قابل اندازه‌گیری در مقاومت را تایید کنند. |  3  |  V  |

---

## 10.3 کاهش استنباط عضویت

محدود کردن توانایی تصمیم‌گیری درباره اینکه آیا یک رکورد در داده‌های آموزشی بوده است. حفظ حریم خصوصی تفاضلی و پوشش‌دهی امتیاز اطمینان همچنان مؤثرترین دفاع‌های شناخته‌شده هستند.

|   #    | توضیحات                                                                                                                           | سطح | نقش |
| :----: | --------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.3.1 | تأیید کنید که منظم‌سازی آنتروپی به ازای هر پرسش یا مقیاس‌بندی دما، پیش‌بینی‌های بیش از حد مطمئن را کاهش می‌دهد.                   |  1  |  D  |
| 10.3.2 | تأیید کنید که آموزش از بهینه‌سازی تفاضلی-خصوصی با حد ε برای داده‌های حساس استفاده می‌کند.                                         |  2  |  D  |
| 10.3.3 | تأیید کنید که شبیه‌سازی‌های حمله (مدل سایه یا جعبه‌سیاه) نشان دهند که AUC حمله کمتر یا مساوی 0.60 روی داده‌های نگه‌داشته شده است. |  2  |  V  |

---

## ۱۰.۴ مقاومت در برابر برعکس‌سازی مدل

جلوگیری از بازسازی ویژگی‌های خصوصی. بررسی‌های اخیر تأکید می‌کنند که قطع خروجی و تضمین‌های DP به عنوان دفاع‌های عملی استفاده می‌شوند.

|   #    | توضیحات                                                                                                                              | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------------------------------------ | :-: | :-: |
| 10.4.1 | اطمینان حاصل کنید که ویژگی‌های حساس هرگز به طور مستقیم خروجی داده نمی‌شوند؛ در صورت نیاز، از سطل‌ها یا تبدیلات یک‌طرفه استفاده کنید. |  1  |  D  |
| 10.4.2 | تأیید کنید که محدودیت‌های نرخ پرس‌وجو، پرس‌وجوهای تطبیقی مکرر از همان اصل را محدود می‌کنند.                                          |  1  | D/V |
| 10.4.3 | اطمینان حاصل کنید که مدل با نویز حفظ حریم خصوصی آموزش داده شده است.                                                                  |  2  |  D  |

---

## 10.5 دفاع استخراج مدل

شناسایی و جلوگیری از کلون‌سازی غیرمجاز. استفاده از نشانه‌گذاری دیجیتال و تحلیل الگوهای پرس‌وجو پیشنهاد می‌شود.

|   #    | توضیحات                                                                                                                                                              | سطح | نقش |
| :----: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.5.1 | اطمینان حاصل کنید که دروازه‌های استنتاج محدودیت‌های نرخ کلی و محدودیت‌های نرخ مخصوص به هر کلید API را که متناسب با آستانه حفظ حافظه مدل تنظیم شده‌اند، اجرا می‌کنند. |  1  |  D  |
| 10.5.2 | تأیید کنید که آمارهای انتروپی پرس‌وجو و جمع‌گرایی ورودی، یک آشکارساز استخراج خودکار را تغذیه می‌کنند.                                                                |  2  | D/V |
| 10.5.3 | تأیید کنید که نشان‌های آبی شکننده یا احتمالاتی می‌توانند با p < 0.01 در ≤ 1 000 پرس‌وجو علیه یک کلون مشکوک اثبات شوند.                                               |  2  |  V  |
| 10.5.4 | اطمینان حاصل کنید که کلیدهای واترمارک و مجموعه‌های تریگر در یک ماژول امنیت سخت‌افزاری ذخیره شده و به صورت سالانه تغییر داده می‌شوند.                                 |  3  |  D  |
| 10.5.5 | تأیید کنید که رویدادهای استخراج-هشدار شامل پرس‌وجوهای متخلف بوده و با کتابچه‌های پاسخ به حادثه ادغام شده‌اند.                                                        |  3  |  V  |

---

## 10.6 تشخیص داده‌های آلوده در زمان استنتاج

شناسایی و خنثی‌سازی ورودی‌های دارای درب پشتی یا آلوده.

|   #    | توضیحات                                                                                                                                | سطح | نقش |
| :----: | -------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.6.1 | اطمینان حاصل کنید که ورودی‌ها قبل از استنتاج مدل از طریق یک آشکارساز ناهنجاری (مثلاً STRIP، امتیازدهی سازگاری) عبور کنند.              |  1  |  D  |
| 10.6.2 | اطمینان حاصل کنید که آستانه‌های آشکارساز بر روی مجموعه‌های اعتبارسنجی پاک/آلوده تنظیم شده‌اند تا کمتر از ۵٪ مثبت کاذب داشته باشند.     |  1  |  V  |
| 10.6.3 | اطمینان حاصل کنید که ورودی‌هایی که به عنوان مسموم علامت‌گذاری شده‌اند، باعث فعال‌شدن فرایندهای مسدودسازی نرم و بازبینی انسانی می‌شوند. |  2  |  D  |
| 10.6.4 | تأیید کنید که آشکارسازها با حملات در پشتی تطبیقی بدون ماشه تحت فشار قرار گرفته‌اند.                                                    |  2  |  V  |
| 10.6.5 | اطمینان حاصل کنید که معیارهای اثربخشی تشخیص ثبت می‌شوند و به‌طور دوره‌ای با اطلاعات تهدید تازه مجدداً ارزیابی می‌شوند.                 |  3  |  D  |

---

## 10.7 سازگارسازی پویای سیاست‌های امنیتی

به‌روزرسانی‌های سیاست امنیتی در زمان واقعی بر اساس اطلاعات تهدید و تحلیل رفتاری.

|   #    | توضیحات                                                                                                                                  | سطح | نقش |
| :----: | ---------------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.7.1 | اطمینان حاصل کنید که سیاست‌های امنیتی می‌توانند به صورت پویا بدون راه‌اندازی مجدد عامل به‌روز شوند در حالی که صحت نسخه سیاست حفظ می‌شود. |  1  | D/V |
| 10.7.2 | تأیید کنید که به‌روزرسانی‌های سیاست به‌صورت رمزنگاری‌شده توسط پرسنل امنیتی مجاز امضا شده و قبل از اعمال اعتبارسنجی می‌شوند.              |  2  | D/V |
| 10.7.3 | اطمینان حاصل کنید که تغییرات سیاست‌های پویا با داشتن مسیرهای کامل حسابرسی شامل توجیه، زنجیره‌های تایید و روش‌های بازگردانی، ثبت می‌شوند. |  2  | D/V |
| 10.7.4 | تأیید کنید که مکانیزم‌های امنیتی تطبیقی حساسیت تشخیص تهدید را بر اساس زمینه ریسک و الگوهای رفتاری تنظیم می‌کنند.                         |  3  | D/V |
| 10.7.5 | اطمینان حاصل کنید که تصمیمات تطبیق سیاست قابل توضیح باشند و شامل شواهد مستند برای بررسی تیم امنیتی باشند.                                |  3  | D/V |

---

## 10.8 تحلیل امنیت مبتنی بر بازتاب (Reflection-Based Security Analysis)

اعتبارسنجی امنیت از طریق خودبازتابی عامل و تحلیل فراتفکری.

|   #    | توضیحات                                                                                                                           | سطح | نقش |
| :----: | --------------------------------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.8.1 | اطمینان حاصل کنید که مکانیزم‌های بازتاب عامل شامل ارزیابی خود متمرکز بر امنیت از تصمیمات و اقدامات هستند.                         |  1  | D/V |
| 10.8.2 | اطمینان حاصل کنید که خروجی‌های بازتابی اعتبارسنجی شده‌اند تا از دستکاری مکانیزم‌های خودارزیابی توسط ورودی‌های خصمانه جلوگیری شود. |  2  | D/V |
| 10.8.3 | بررسی کنید که تحلیل امنیت فراشناختی، سوگیری احتمالی، دستکاری یا نفوذ در فرآیندهای استدلال عامل را شناسایی می‌کند.                 |  2  | D/V |
| 10.8.4 | تأیید کنید که هشدارهای امنیتی مبتنی بر انعکاس، پایش پیشرفته و جریان‌های کاری احتمالی مداخله انسانی را فعال می‌کنند.               |  3  | D/V |
| 10.8.5 | اطمینان حاصل کنید که یادگیری مداوم از بازتاب‌های امنیتی تشخیص تهدید را بهبود می‌بخشد بدون اینکه عملکردهای مشروع را تضعیف کند.     |  3  | D/V |

---

## 10.9 تکامل و امنیت خودبهبودی

کنترل‌های امنیتی برای سیستم‌های عاملی که قادر به خود-تغییر و تکامل هستند.

|   #    | توضیحات                                                                                                 | سطح | نقش |
| :----: | ------------------------------------------------------------------------------------------------------- | :-: | :-: |
| 10.9.1 | تأیید کنید که قابلیت‌های خود-تغییر محدود به مناطق ایمن تعیین‌شده با مرزهای تأیید رسمی باشد.             |  1  | D/V |
| 10.9.2 | اطمینان حاصل کنید که پیشنهادهای تکاملی پیش از اجرا تحت ارزیابی تأثیر امنیتی قرار می‌گیرند.              |  2  | D/V |
| 10.9.3 | تأیید کنید که مکانیسم‌های خودبهبودی شامل قابلیت‌های بازگشت به حالت قبلی همراه با تأیید صحت باشند.       |  2  | D/V |
| 10.9.4 | تأیید کنید که امنیت متا-یادگیری از دستکاری مخرب الگوریتم‌های بهبود جلوگیری می‌کند.                      |  3  | D/V |
| 10.9.5 | تأیید کنید که بهبود بازگشتی خودکار تحت محدودیت‌های ایمنی رسمی محدود شده است با اثبات‌های ریاضی همگرایی. |  3  | D/V |

---

### مراجع

* [MITRE ATLAS adversary tactics for ML](https://atlas.mitre.org/)
* [NIST AI Risk Management Framework 1.0, 2023](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
* [OWASP Top 10 for LLM Applications, 2025](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
* [Adversarial Training: A Survey — Zhao et al., 2024](https://arxiv.org/abs/2410.15042)
* [RobustBench adversarial-robustness benchmark](https://robustbench.github.io/)
* [Membership-Inference & Model-Inversion Risk Survey, 2025](https://www.sciencedirect.com/science/article/abs/pii/S0950705125003867)
* [PURIFIER: Confidence-Score Defense against MI Attacks — AAAI 2023](https://ojs.aaai.org/index.php/AAAI/article/view/26289)
* [Model-Inversion Attacks & Defenses Survey — AI Review, 2025](https://link.springer.com/article/10.1007/s10462-025-11248-0)
* [Comprehensive Defense Framework Against Model Extraction — IEEE TDSC 2024](https://doi.org/10.1109/TDSC.2023.3261327)
* [Fragile Model Watermarking Survey — 2025](https://www.sciencedirect.com/science/article/abs/pii/S0165168425002026)
* [Data Poisoning in Deep Learning: A Survey — Zhao et al., 2025](https://arxiv.org/abs/2503.22759)
* [BDetCLIP: Multimodal Prompting Backdoor Detection — Niu et al., 2024](https://arxiv.org/abs/2405.15269)

