# Anhang A: Glossar

Dieses umfassende Glossar bietet Definitionen der wichtigsten Begriffe aus den Bereichen KI, ML und Sicherheit, die im gesamten AISVS verwendet werden, um Klarheit und ein gemeinsames Verständnis zu gewährleisten.

* Adversarisches Beispiel: Eine Eingabe, die absichtlich erstellt wurde, um ein KI-Modell dazu zu bringen, einen Fehler zu machen, oft durch das Hinzufügen subtiler Störungen, die für Menschen nicht wahrnehmbar sind.
  ​
* Gegnerische Robustheit – Gegnerische Robustheit in der KI bezieht sich auf die Fähigkeit eines Modells, seine Leistung aufrechtzuerhalten und Widerstand gegen absichtlich gestaltete, bösartige Eingaben zu leisten, die darauf abzielen, Fehler zu verursachen oder das Modell zu manipulieren.
  ​
* Agent – KI-Agenten sind Softwaresysteme, die KI nutzen, um Ziele zu verfolgen und Aufgaben im Auftrag von Benutzern zu erledigen. Sie zeigen Fähigkeiten in Argumentation, Planung und Gedächtnis und verfügen über ein Maß an Autonomie, um Entscheidungen zu treffen, zu lernen und sich anzupassen.
  ​
* Agentische KI: KI-Systeme, die mit einem gewissen Grad an Autonomie arbeiten können, um Ziele zu erreichen, dabei oft Entscheidungen treffen und Handlungen ausführen, ohne direkte menschliche Eingriffe.
  ​
* Attributbasierte Zugriffskontrolle (ABAC): Ein Zugriffskontrollparadigma, bei dem Autorisierungsentscheidungen auf Attributen des Nutzers, der Ressource, der Aktion und der Umgebung basieren, die zur Abfragezeit bewertet werden.
  ​
* Backdoor-Angriff: Eine Art von Datenvergiftungsangriff, bei dem das Modell so trainiert wird, auf bestimmte Auslöser auf eine bestimmte Weise zu reagieren, während es sich ansonsten normal verhält.
  ​
* Bias: Systematische Fehler in den Ausgaben von KI-Modellen, die zu unfairen oder diskriminierenden Ergebnissen für bestimmte Gruppen oder in bestimmten Kontexten führen können.
  ​
* Bias-Ausnutzung: Eine Angriffstechnik, die bekannte Verzerrungen in KI-Modellen ausnutzt, um Ausgaben oder Ergebnisse zu manipulieren.
  ​
* Cedar: Amazons Richtliniensprache und -engine für fein granulare Berechtigungen, die bei der Umsetzung von ABAC für KI-Systeme verwendet wird.
  ​
* Gedankenkette: Eine Technik zur Verbesserung des Denkvermögens in Sprachmodellen durch Erzeugung von Zwischenschritten des Denkprozesses vor der Ausgabe einer endgültigen Antwort.
  ​
* Circuit Breaker: Mechanismen, die den Betrieb von KI-Systemen automatisch stoppen, wenn bestimmte Risikoschwellen überschritten werden.
  ​
* Datenleck: Unbeabsichtigte Offenlegung sensibler Informationen durch Ausgaben oder Verhalten von KI-Modellen.
  ​
* Datenvergiftung: Die absichtliche Korruption von Trainingsdaten, um die Modellintegrität zu gefährden, häufig um Hintertüren einzubauen oder die Leistung zu verschlechtern.
  ​
* Differenzielle Privatsphäre – Differenzielle Privatsphäre ist ein mathematisch rigoroses Rahmenwerk zur Veröffentlichung statistischer Informationen über Datensätze, während die Privatsphäre einzelner Datenpersonen geschützt wird. Es ermöglicht einem Dateninhaber, aggregierte Muster der Gruppe zu teilen und gleichzeitig die Informationen, die über spezifische Individuen preisgegeben werden, zu begrenzen.
  ​
* Einbettungen: Dichte Vektor-Darstellungen von Daten (Text, Bilder usw.), die die semantische Bedeutung in einem hochdimensionalen Raum erfassen.
  ​
* Erklärbarkeit – Erklärbarkeit in der KI ist die Fähigkeit eines KI-Systems, menschenverständliche Gründe für seine Entscheidungen und Vorhersagen zu liefern und Einblicke in seine internen Arbeitsweisen zu geben.
  ​
* Erklärbare KI (XAI): KI-Systeme, die entwickelt wurden, um durch verschiedene Techniken und Frameworks menschlich verständliche Erklärungen für ihre Entscheidungen und Verhaltensweisen zu liefern.
  ​
* Federiertes Lernen: Ein Ansatz im maschinellen Lernen, bei dem Modelle über mehrere dezentralisierte Geräte hinweg mit lokalen Datensätzen trainiert werden, ohne dass die Daten selbst ausgetauscht werden.
  ​
* Guardrails: Einschränkungen, die implementiert werden, um zu verhindern, dass KI-Systeme schädliche, voreingenommene oder anderweitig unerwünschte Ausgaben erzeugen.
  ​
* Halluzination – Eine KI-Halluzination bezieht sich auf ein Phänomen, bei dem ein KI-Modell falsche oder irreführende Informationen erzeugt, die nicht auf seinen Trainingsdaten oder der faktischen Realität basieren.
  ​
* Human-in-the-Loop (HITL): Systeme, die darauf ausgelegt sind, menschliche Aufsicht, Überprüfung oder Eingriffe an entscheidenden Entscheidungspunkten zu erfordern.
  ​
* Infrastructure as Code (IaC): Verwaltung und Bereitstellung von Infrastruktur durch Code statt manueller Prozesse, was Sicherheitsscans und konsistente Bereitstellungen ermöglicht.
  ​
* Jailbreak: Techniken, die verwendet werden, um Sicherheitsvorkehrungen in KI-Systemen, insbesondere in großen Sprachmodellen, zu umgehen und verbotene Inhalte zu erzeugen.
  ​
* Least Privilege: Das Sicherheitsprinzip, nur die minimal notwendigen Zugriffsrechte für Benutzer und Prozesse zu gewähren.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): Eine Technik zur Erklärung der Vorhersagen eines beliebigen maschinellen Lernklassifikators durch lokale Approximation mit einem interpretierbaren Modell.
  ​
* Mitgliedschaftsinferenzangriff: Ein Angriff, der darauf abzielt festzustellen, ob ein bestimmter Datenpunkt zum Trainieren eines maschinellen Lernmodells verwendet wurde.
  ​
* MITRE ATLAS: Bedrohungslandschaft durch Angreifer für künstliche Intelligenzsysteme; eine Wissensdatenbank zu gegnerischen Taktiken und Techniken gegen KI-Systeme.
  ​
* Modellkarte – Eine Modellkarte ist ein Dokument, das standardisierte Informationen über die Leistung, Einschränkungen, beabsichtigte Verwendungszwecke und ethische Überlegungen eines KI-Modells bereitstellt, um Transparenz und verantwortungsbewusste KI-Entwicklung zu fördern.
  ​
* Modell-Extraktion: Ein Angriff, bei dem ein Angreifer ein Zielmodell wiederholt abfragt, um ohne Genehmigung eine funktional ähnliche Kopie zu erstellen.
  ​
* Modellinversion: Ein Angriff, der versucht, Trainingsdaten durch die Analyse von Modellausgaben zu rekonstruieren.
  ​
* Modell-Lebenszyklus-Management – Das AI Modell-Lebenszyklus-Management ist der Prozess der Überwachung aller Phasen der Existenz eines AI Modells, einschließlich Design, Entwicklung, Bereitstellung, Überwachung, Wartung und schlussendliche Außerdienststellung, um sicherzustellen, dass es effektiv bleibt und den Zielen entspricht.
  ​
* Modellvergiftung: Einführung von Schwachstellen oder Hintertüren direkt in ein Modell während des Trainingsprozesses.
  ​
* Modell-Diebstahl/-Entwendung: Das Extrahieren einer Kopie oder Annäherung eines proprietären Modells durch wiederholte Abfragen.
  ​
* Multi-Agenten-System: Ein System, das aus mehreren interagierenden KI-Agenten besteht, von denen jeder potenziell unterschiedliche Fähigkeiten und Ziele hat.
  ​
* OPA (Open Policy Agent): Eine Open-Source-Richtlinien-Engine, die eine einheitliche Durchsetzung von Richtlinien über den gesamten Stack hinweg ermöglicht.
  ​
* Datenschutzwahrendes Maschinelles Lernen (Privacy-Preserving Machine Learning, PPML): Techniken und Methoden zum Trainieren und Bereitstellen von ML-Modellen unter Wahrung der Privatsphäre der Trainingsdaten.
  ​
* Prompt Injection: Ein Angriff, bei dem bösartige Anweisungen in Eingaben eingebettet werden, um das beabsichtigte Verhalten eines Modells zu überschreiben.
  ​
* RAG (Retrieval-Augmented Generation): Eine Technik, die große Sprachmodelle verbessert, indem relevante Informationen aus externen Wissensquellen abgerufen werden, bevor eine Antwort generiert wird.
  ​
* Red-Teaming: Die Praxis, KI-Systeme aktiv zu testen, indem simulierte Angriffe durchgeführt werden, um Schwachstellen zu identifizieren.
  ​
* SBOM (Software Bill of Materials): Eine formale Aufzeichnung, die die Details und Lieferkettenbeziehungen verschiedener Komponenten enthält, die beim Erstellen von Software oder KI-Modellen verwendet werden.
  ​
* SHAP (SHapley Additive exPlanations): Ein spieltheoretischer Ansatz zur Erklärung der Ausgabe eines beliebigen Machine-Learning-Modells durch Berechnung des Beitrags jeder Eigenschaft zur Vorhersage.
  ​
* Supply-Chain-Angriff: Kompromittierung eines Systems durch Angriff auf weniger sichere Elemente in seiner Lieferkette, wie z. B. Drittanbieter-Bibliotheken, Datensätze oder vortrainierte Modelle.
  ​
* Transferlernen: Eine Technik, bei der ein für eine Aufgabe entwickeltes Modell als Ausgangspunkt für ein Modell zu einer zweiten Aufgabe wiederverwendet wird.
  ​
* Vektor-Datenbank: Eine spezialisierte Datenbank, die zur Speicherung hochdimensionaler Vektoren (Embeddings) entwickelt wurde und effiziente Ähnlichkeitssuchen ermöglicht.
  ​
* Schwachstellen-Scan: Automatisierte Werkzeuge, die bekannte Sicherheitsschwachstellen in Softwarekomponenten, einschließlich KI-Frameworks und Abhängigkeiten, identifizieren.
  ​
* Wasserzeichen: Techniken zum Einbetten von nicht wahrnehmbaren Markierungen in KI-generierte Inhalte, um deren Herkunft zu verfolgen oder die KI-Generierung zu erkennen.
  ​
* Zero-Day-Schwachstelle: Eine zuvor unbekannte Schwachstelle, die Angreifer ausnutzen können, bevor Entwickler einen Patch erstellen und bereitstellen.

