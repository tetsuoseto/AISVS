# Anhang A: Glossar

>This umfassendes Glossar liefert Definitionen der wichtigsten Begriffe aus KI, ML und Sicherheit, die im AISVS verwendet werden, um Klarheit und gemeinsames Verständnis sicherzustellen.

* Adversariales Beispiel: Eine Eingabe, die absichtlich so gestaltet wurde, dass ein KI-Modell einen Fehler macht, oft durch das Hinzufügen subtiler Perturbationen, die für Menschen unmerklich sind.
  ​
* Adversarielle Robustheit – Adversarielle Robustheit in der KI bezieht sich auf die Fähigkeit eines Modells, seine Leistung aufrechtzuerhalten und sich dagegen zu wehren, von absichtlich gestalteten, böswilligen Eingaben getäuscht oder manipuliert zu werden, die darauf abzielen, Fehler zu verursachen.
  ​
* Agent – KI-Agenten sind Softwaresysteme, die KI verwenden, um Ziele zu verfolgen und Aufgaben im Namen der Nutzer zu erledigen. Sie zeigen Denkprozesse, Planungsfähigkeiten und Gedächtnis und verfügen über ein Maß an Autonomie, um Entscheidungen zu treffen, zu lernen und sich anzupassen.
  ​
* Agentische KI: KI-Systeme, die mit einem gewissen Autonomiegrad arbeiten können, um Ziele zu erreichen, wobei sie oft Entscheidungen treffen und Maßnahmen ergreifen, ohne direkte menschliche Intervention.
  ​
* Attributbasierte Zugriffskontrolle (ABAC): Ein Zugriffskontrollparadigma, bei dem Autorisierungsentscheidungen auf Attributen des Benutzers, der Ressource, der Aktion und der Umgebung basieren und zur Abfragezeit bewertet werden.
  ​
* Backdoor-Angriff: Eine Art von Datenvergiftungsangriff, bei dem das Modell darauf trainiert wird, auf bestimmte Auslöser hin auf eine bestimmte Weise zu reagieren, während es sich ansonsten normal verhält.
  ​
* Voreingenommenheit: systematische Fehler in den Ausgaben von KI-Modellen, die zu unfairen oder diskriminierenden Ergebnissen für bestimmte Gruppen oder in bestimmten Kontexten führen können.
  ​
* Bias-Ausnutzung: Eine Angriffstechnik, die bekannte Voreingenommenheiten in KI-Modellen ausnutzt, um Ausgaben oder Ergebnisse zu manipulieren.
  ​
* Cedar: Amazons Richtliniensprache und Engine für feingranulare Berechtigungen, die bei der Implementierung von ABAC für KI-Systeme verwendet wird.
  ​
* Gedankenkette: Eine Technik zur Verbesserung des Denkprozesses von Sprachmodellen, indem vor der Erzeugung einer endgültigen Antwort Zwischenschritte des Denkens erzeugt werden.
  ​
* Abschaltmechanismen: Mechanismen, die den Betrieb von KI-Systemen automatisch stoppen, wenn bestimmte Risikoschwellenwerte überschritten werden.
  ​
* Datenleck: Unbeabsichtigte Offenlegung sensibler Informationen durch Ausgaben oder das Verhalten des KI-Modells.
  ​
* Datenvergiftung: Die absichtliche Verfälschung von Trainingsdaten, um die Integrität des Modells zu beeinträchtigen, oft um Hintertüren einzubauen oder die Leistung zu verschlechtern.
  ​
* Differentielle Privatsphäre – Differentielle Privatsphäre ist ein mathematisch rigoroser Rahmen für die Veröffentlichung statistischer Informationen über Datensätze, während die Privatsphäre einzelner Datensubjekte geschützt wird. Es ermöglicht einem Datenhalter, aggregierte Muster der Gruppe zu teilen, während gleichzeitig Informationen, die über bestimmte Individuen preisgegeben werden, begrenzt werden.
  ​
* Einbettungen: dichte Vektorrepräsentationen von Daten (Text, Bilder usw.), die semantische Bedeutung in einem hochdimensionalen Raum erfassen.
  ​
* Erklärbarkeit – Die Erklärbarkeit von KI ist die Fähigkeit eines KI-Systems, menschlich nachvollziehbare Begründungen für seine Entscheidungen und Vorhersagen zu liefern und Einblicke in seine inneren Funktionsweisen zu geben.
  ​
* Erklärbare KI (XAI): KI-Systeme, die darauf ausgelegt sind, durch verschiedene Techniken und Rahmenwerke menschlich verständliche Erklärungen für ihre Entscheidungen und ihr Verhalten bereitzustellen.
  ​
* Föderiertes Lernen: Ein Ansatz des maschinellen Lernens, bei dem Modelle auf mehreren dezentralen Geräten trainiert werden, die lokale Datensätze halten, ohne die Daten selbst auszutauschen.
  ​
* Schutzmechanismen: Beschränkungen, die implementiert wurden, um zu verhindern, dass KI-Systeme schädliche, voreingenommene oder anderweitig unerwünschte Ausgaben erzeugen.
  ​
* Halluzination – Eine Halluzination der KI bezieht sich auf ein Phänomen, bei dem ein Modell der künstlichen Intelligenz inkorrekte oder irreführende Informationen erzeugt, die nicht auf seinen Trainingsdaten oder auf der fak­ti­schen Realität basieren.
  ​
* Mensch-in-the-Loop (HITL): Systeme, die darauf ausgelegt sind, menschliche Aufsicht, Verifikation oder Intervention an entscheidenden Entscheidungspunkten zu verlangen.
  ​
* Infrastruktur als Code (IaC): Verwaltung und Bereitstellung von Infrastruktur durch Code statt manueller Prozesse, wodurch Sicherheitsprüfungen ermöglicht werden und konsistente Bereitstellungen gewährleistet sind.
  ​
* Jailbreak: Techniken, die verwendet werden, um Sicherheitsbarrieren in KI-Systemen, insbesondere in großen Sprachmodellen, zu umgehen und verbotene Inhalte zu erzeugen.
  ​
* Prinzip der geringsten Privilegien: Das Sicherheitsprinzip, Benutzern und Prozessen nur die minimal notwendigen Zugriffsrechte zu gewähren.
  ​
* LIME (Lokale interpretierbare modellunabhängige Erklärungen): Eine Technik, um die Vorhersagen eines beliebigen Klassifikators des maschinellen Lernens zu erklären, indem er lokal durch ein interpretierbares Modell angenähert wird.
  ​
* Mitgliedschafts-Inferenz-Angriff: Ein Angriff, der darauf abzielt, festzustellen, ob ein bestimmter Datenpunkt zum Trainieren eines Modells des maschinellen Lernens verwendet wurde.
  ​
* MITRE ATLAS: Adversarial Threat Landscape for Artificial-Intelligence Systems; eine Wissensdatenbank über adversariale Taktiken und Techniken gegen KI-Systeme.
  ​
* Modellkarte – Eine Modellkarte ist ein Dokument, das standardisierte Informationen über die Leistung eines KI-Modells, Einschränkungen, beabsichtigte Nutzungen und ethische Überlegungen bereitstellt, um Transparenz und eine verantwortungsvolle KI-Entwicklung zu fördern.
  ​
* Modellextraktion: Ein Angriff, bei dem ein Angreifer wiederholt Anfragen an ein Zielmodell stellt, um eine funktionsähnliche Kopie ohne Genehmigung zu erstellen.
  ​
* Modellinversionsangriff: Ein Angriff, der versucht, Trainingsdaten durch Analyse der Modellausgaben zu rekonstruieren.
  ​
* Modell-Lebenszyklus-Management – KI-Modell-Lebenszyklus-Management ist der Prozess der Überwachung aller Phasen des Lebenszyklus eines KI-Modells, einschließlich Entwurf, Entwicklung, Bereitstellung, Überwachung, Wartung und Ausrangierung, um sicherzustellen, dass es wirksam bleibt und den Zielen entspricht.
  ​
* Modellvergiftung: Das direkte Einführen von Schwachstellen oder Hintertüren in ein Modell während des Trainingsprozesses.
  ​
* Modellraub/Diebstahl: Extrahieren einer Kopie oder Annäherung eines proprietären Modells durch wiederholte Abfragen.
  ​
* Multi-Agent-System: Ein System, das aus mehreren interagierenden KI-Agenten besteht, von denen jeder potenziell unterschiedliche Fähigkeiten und Ziele hat.
  ​
* OPA (Open Policy Agent): Eine Open-Source-Richtlinien-Engine, die eine einheitliche Richtliniendurchsetzung über den Stack hinweg ermöglicht.
  ​
* Datenschutzfreundliches Maschinelles Lernen (PPML): Techniken und Methoden zum Trainieren und Bereitstellen von ML-Modellen, während die Privatsphäre der Trainingsdaten geschützt wird.
  ​
* Prompt-Injektion: Ein Angriff, bei dem bösartige Anweisungen in Eingaben eingebettet werden, um das beabsichtigte Verhalten des Modells zu überschreiben.
  ​
* RAG (Retrieval-Augmented Generation): Eine Technik, die große Sprachmodelle dadurch verbessert, dass sie vor der Generierung einer Antwort relevante Informationen aus externen Wissensquellen abruf t.
  ​
* Red-Teaming: Die Praxis, KI-Systeme aktiv zu testen, indem man adversariale Angriffe simuliert, um Schwachstellen zu identifizieren.
  ​
* SBOM (Software-Stückliste): Eine formelle Aufzeichnung, die Details und Lieferkettenbeziehungen verschiedener Komponenten enthält, die bei der Erstellung von Software oder KI-Modellen verwendet werden.
  ​
* SHAP (SHapley Additive exPlanations): Ein spieltheoretischer Ansatz zur Erklärung der Ausgabe eines beliebigen maschinellen Lernmodells, indem der Beitrag jedes Merkmals zur Vorhersage berechnet wird.
  ​
* Lieferkettenangriff: Ein System kompromittieren, indem man weniger sichere Elemente in seiner Lieferkette ins Visier nimmt, zum Beispiel Drittanbieter-Bibliotheken, Datensätze oder vortrainierte Modelle.
  ​
* Transferlernen: Eine Technik, bei der ein für eine Aufgabe entwickeltes Modell als Ausgangspunkt für ein Modell einer zweiten Aufgabe wiederverwendet wird.
  ​
* Vektordatenbank: Eine spezialisierte Datenbank, die entwickelt wurde, um hochdimensionale Vektoren (Einbettungen) zu speichern und effiziente Ähnlichkeitsabfragen durchzuführen.
  ​
* Schwachstellen-Scan: Automatisierte Werkzeuge, die bekannte Sicherheitslücken in Softwarekomponenten identifizieren, einschließlich KI-Frameworks und Abhängigkeiten.
  ​
* Wasserzeichen: Techniken zum Einbetten unsichtbarer Markierungen in KI-generierte Inhalte, um deren Herkunft nachzuverfolgen oder die KI-Generierung zu erkennen.
  ​
* Zero-Day-Schwachstelle: Eine bisher unbekannte Schwachstelle, die Angreifer ausnutzen können, bevor Entwickler einen Patch erstellen und bereitstellen.

