# Anhang A: Glossar

>Dieses umfassende Glossar bietet Definitionen wichtiger Begriffe aus den Bereichen KI, ML und Sicherheit, die im gesamten AISVS verwendet werden, um Klarheit und ein gemeinsames Verständnis zu gewährleisten.

* Adversariales Beispiel: Eine absichtlich erzeugte Eingabe, die ein KI-Modell dazu veranlassen soll, einen Fehler zu machen, häufig durch das Hinzufügen subtiler Störungen, die für Menschen nicht wahrnehmbar sind.
  ​
* Gegnerische Robustheit – Gegnerische Robustheit in der KI bezieht sich auf die Fähigkeit eines Modells, seine Leistung aufrechtzuerhalten und nicht durch absichtlich gestaltete, bösartige Eingaben, die Fehler verursachen sollen, getäuscht oder manipuliert zu werden.
  ​
* Agent – KI-Agenten sind Softwaresysteme, die KI einsetzen, um Ziele zu verfolgen und Aufgaben im Auftrag von Benutzern zu erledigen. Sie zeigen Fähigkeiten wie Schlussfolgerungen, Planung und Gedächtnis und besitzen ein gewisses Maß an Autonomie, um Entscheidungen zu treffen, zu lernen und sich anzupassen.
  ​
* Agentische KI: KI-Systeme, die mit einem gewissen Grad an Autonomie operieren können, um Ziele zu erreichen, wobei sie oft Entscheidungen treffen und Maßnahmen ergreifen, ohne direkte menschliche Eingriffe.
  ​
* Attributbasierte Zugriffskontrolle (ABAC): Ein Zugriffskontrollparadigma, bei dem Autorisierungsentscheidungen auf Attributen des Benutzers, der Ressource, der Aktion und der Umgebung basieren und zur Abfragezeit ausgewertet werden.
  ​
* Backdoor-Angriff: Eine Art von Datenvergiftungsangriff, bei dem das Modell darauf trainiert wird, auf bestimmte Auslöser auf eine spezifische Weise zu reagieren, während es sich ansonsten normal verhält.
  ​
* Bias: Systematische Fehler in den Ausgaben von KI-Modellen, die zu unfairen oder diskriminierenden Ergebnissen für bestimmte Gruppen oder in spezifischen Kontexten führen können.
  ​
* Bias-Ausnutzung: Eine Angriffstechnik, die bekannte Verzerrungen in KI-Modellen ausnutzt, um Ausgaben oder Ergebnisse zu manipulieren.
  ​
* Cedar: Amazons Richtliniensprache und Engine für fein abgestufte Berechtigungen, die bei der Implementierung von ABAC für KI-Systeme verwendet wird.
  ​
* Chain of Thought: Eine Technik zur Verbesserung des Denkvermögens in Sprachmodellen durch Generieren von Zwischenschritten des Denkprozesses vor der Ausgabe einer endgültigen Antwort.
  ​
* Circuit Breakers: Mechanismen, die den Betrieb von KI-Systemen automatisch stoppen, wenn bestimmte Risikoschwellen überschritten werden.
  ​
* Datenleckage: Unbeabsichtigte Offenlegung sensibler Informationen durch Ausgaben oder Verhalten von KI-Modellen.
  ​
* Datenvergiftung: Die absichtliche Korruption von Trainingsdaten zur Beeinträchtigung der Modellintegrität, häufig um Hintertüren zu installieren oder die Leistung zu verschlechtern.
  ​
* Differenzielle Privatsphäre – Differenzielle Privatsphäre ist ein mathematisch rigoroser Rahmen zur Veröffentlichung statistischer Informationen über Datensätze, während gleichzeitig die Privatsphäre einzelner Betroffener geschützt wird. Sie ermöglicht einem Dateninhaber, aggregierte Muster der Gruppe zu teilen und dabei die Informationen, die über einzelne Personen preisgegeben werden, zu begrenzen.
  ​
* Einbettungen: Dichte Vektorrepräsentationen von Daten (Text, Bilder usw.), die semantische Bedeutung in einem hochdimensionalen Raum erfassen.
  ​
* Erklärbarkeit – Erklärbarkeit in der KI ist die Fähigkeit eines KI-Systems, menschenverständliche Gründe für seine Entscheidungen und Vorhersagen zu liefern und Einblicke in seine internen Abläufe zu geben.
  ​
* Erklärbare KI (XAI): KI-Systeme, die darauf ausgelegt sind, durch verschiedene Techniken und Rahmenwerke menschenverständliche Erklärungen für ihre Entscheidungen und Verhaltensweisen zu liefern.
  ​
* Föderiertes Lernen: Ein maschinelles Lernverfahren, bei dem Modelle über mehrere dezentralisierte Geräte hinweg trainiert werden, die lokale Datensätze besitzen, ohne dass die Daten selbst ausgetauscht werden.
  ​
* Schutzmaßnahmen: Einschränkungen, die implementiert werden, um zu verhindern, dass KI-Systeme schädliche, voreingenommene oder anderweitig unerwünschte Ergebnisse erzeugen.
  ​
* Halluzination – Eine AI-Halluzination bezeichnet ein Phänomen, bei dem ein KI-Modell falsche oder irreführende Informationen erzeugt, die nicht auf seinen Trainingsdaten oder der tatsächlichen Realität basieren.
  ​
* Human-in-the-Loop (HITL): Systeme, die so konzipiert sind, dass sie menschliche Aufsicht, Überprüfung oder Eingriffe an entscheidenden Entscheidungspunkten erfordern.
  ​
* Infrastructure as Code (IaC): Verwaltung und Bereitstellung von Infrastruktur durch Code statt manueller Prozesse, wodurch Sicherheitsscans und konsistente Bereitstellungen ermöglicht werden.
  ​
* Jailbreak: Techniken, die verwendet werden, um Sicherheitsvorkehrungen in KI-Systemen, insbesondere bei großen Sprachmodellen, zu umgehen, um verbotene Inhalte zu erzeugen.
  ​
* Minimalprinzip: Das Sicherheitsprinzip, bei dem Benutzern und Prozessen nur die unbedingt erforderlichen Zugriffsrechte gewährt werden.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): Eine Technik zur Erklärung der Vorhersagen beliebiger maschineller Lernklassifikatoren, indem diese lokal mit einem interpretierbaren Modell angenähert werden.
  ​
* Membership Inference Attack: Ein Angriff, der darauf abzielt festzustellen, ob ein bestimmter Datenpunkt zum Training eines Machine-Learning-Modells verwendet wurde.
  ​
* MITRE ATLAS: Adversarische Bedrohungslandschaft für Künstliche-Intelligenz-Systeme; eine Wissensdatenbank zu adversarischen Taktiken und Techniken gegen KI-Systeme.
  ​
* Modellkarte – Eine Modellkarte ist ein Dokument, das standardisierte Informationen über die Leistung, Einschränkungen, beabsichtigte Verwendungen und ethische Überlegungen eines KI-Modells bereitstellt, um Transparenz und verantwortungsbewusste KI-Entwicklung zu fördern.
  ​
* Modell-Extraktion: Ein Angriff, bei dem ein Angreifer ein Zielmodell wiederholt abfragt, um ohne Genehmigung eine funktional ähnliche Kopie zu erstellen.
  ​
* Modellinversion: Ein Angriff, der versucht, Trainingsdaten durch Analyse von Modellausgaben zu rekonstruieren.
  ​
* Modelllebenszyklusverwaltung – Die Verwaltung des Lebenszyklus eines KI-Modells ist der Prozess der Überwachung aller Phasen der Existenz eines KI-Modells, einschließlich dessen Entwurf, Entwicklung, Einsatz, Überwachung, Wartung und schließlich Stilllegung, um sicherzustellen, dass es wirksam bleibt und den Zielen entspricht.
  ​
* Modellvergiftung: Direkte Einführung von Schwachstellen oder Hintertüren in ein Modell während des Trainingsprozesses.
  ​
* Modell-Diebstahl/-Kopie: Das Extrahieren einer Kopie oder Annäherung eines proprietären Modells durch wiederholte Abfragen.
  ​
* Multi-Agenten-System: Ein System, das aus mehreren interagierenden KI-Agenten besteht, von denen jeder potenziell unterschiedliche Fähigkeiten und Ziele hat.
  ​
* OPA (Open Policy Agent): Eine Open-Source-Policy-Engine, die eine einheitliche Durchsetzung von Richtlinien über den gesamten Stack hinweg ermöglicht.
  ​
* Privacy-Preserving Machine Learning (PPML): Techniken und Methoden, um ML-Modelle zu trainieren und bereitzustellen, während die Privatsphäre der Trainingsdaten geschützt wird.
  ​
* Prompt Injection: Ein Angriff, bei dem bösartige Anweisungen in Eingaben eingebettet werden, um das beabsichtigte Verhalten eines Modells zu überschreiben.
  ​
* RAG (Retrieval-Augmented Generation): Eine Technik, die große Sprachmodelle verbessert, indem sie relevante Informationen aus externen Wissensquellen abruft, bevor sie eine Antwort generiert.
  ​
* Red-Teaming: Die Praxis, KI-Systeme aktiv durch die Simulation feindlicher Angriffe zu testen, um Schwachstellen zu identifizieren.
  ​
* SBOM (Software Bill of Materials): Ein formaler Nachweis, der die Details und Lieferkettenbeziehungen verschiedener Komponenten enthält, die beim Erstellen von Software oder KI-Modellen verwendet werden.
  ​
* SHAP (SHapley Additive exPlanations): Ein spieltheoretischer Ansatz zur Erklärung der Ausgabe eines beliebigen maschinellen Lernmodells durch Berechnung des Beitrags jeder einzelnen Eigenschaft zur Vorhersage.
  ​
* Lieferkettenangriff: Kompromittierung eines Systems durch das gezielte Anvisieren weniger sicherer Elemente in seiner Lieferkette, wie beispielsweise Drittanbieter-Bibliotheken, Datensätze oder vortrainierte Modelle.
  ​
* Transferlernen: Eine Technik, bei der ein für eine Aufgabe entwickeltes Modell als Ausgangspunkt für ein Modell einer zweiten Aufgabe wiederverwendet wird.
  ​
* Vektor-Datenbank: Eine spezialisierte Datenbank, die darauf ausgelegt ist, hochdimensionale Vektoren (Einbettungen) zu speichern und effiziente Ähnlichkeitssuchen durchzuführen.
  ​
* Schwachstellen-Scanning: Automatisierte Tools, die bekannte Sicherheitslücken in Softwarekomponenten, einschließlich KI-Frameworks und Abhängigkeiten, identifizieren.
  ​
* Wasserzeichen: Techniken zur Einbettung nicht wahrnehmbarer Marker in KI-generierte Inhalte, um deren Ursprung zu verfolgen oder die KI-Erzeugung zu erkennen.
  ​
* Zero-Day-Schwachstelle: Eine zuvor unbekannte Schwachstelle, die Angreifer ausnutzen können, bevor Entwickler einen Patch erstellen und bereitstellen.

