# Anhang A: Glossar

Dieses umfassende Glossar liefert Definitionen der wichtigsten Begriffe aus den Bereichen KI, ML und Sicherheit, die im gesamten AISVS verwendet werden, um Klarheit und ein gemeinsames Verständnis zu gewährleisten.

* Gegnerisches Beispiel: Eine absichtlich gestaltete Eingabe, die ein KI-Modell zu einem Fehler veranlassen soll, oft durch Hinzufügen subtiler, für Menschen nicht wahrnehmbarer Störungen.
  ​
* Adversarielle Robustheit – Adversarielle Robustheit in der KI bezieht sich auf die Fähigkeit eines Modells, seine Leistung aufrechtzuerhalten und sich gegen absichtlich konstruierte, bösartige Eingaben zu wehren, die darauf ausgelegt sind, Fehler zu verursachen oder das Modell zu täuschen.
  ​
* Agent – KI-Agenten sind Softwaresysteme, die KI nutzen, um Ziele zu verfolgen und Aufgaben im Auftrag von Nutzern zu erledigen. Sie zeigen Fähigkeiten wie Schlussfolgern, Planen und Gedächtnis und verfügen über ein gewisses Maß an Autonomie, um Entscheidungen zu treffen, zu lernen und sich anzupassen.
  ​
* Agentische KI: KI-Systeme, die mit einem gewissen Grad an Autonomie operieren können, um Ziele zu erreichen, und dabei oft Entscheidungen treffen und Maßnahmen ergreifen, ohne direkte menschliche Intervention.
  ​
* Attributbasierte Zugriffskontrolle (ABAC): Ein Zugriffssteuerungsparadigma, bei dem Autorisierungsentscheidungen auf Attributen des Benutzers, der Ressource, der Aktion und der Umgebung basieren, die zur Abfragezeit bewertet werden.
  ​
* Backdoor-Angriff: Eine Art von Datenvergiftungs-Angriff, bei dem das Modell darauf trainiert wird, auf bestimmte Auslöser in einer spezifischen Weise zu reagieren, während es sich sonst normal verhält.
  ​
* Verzerrung: Systematische Fehler in den Ausgaben von KI-Modellen, die zu unfairen oder diskriminierenden Ergebnissen für bestimmte Gruppen oder in spezifischen Kontexten führen können.
  ​
* Bias-Ausnutzung: Eine Angriffstechnik, die bekannte Verzerrungen in KI-Modellen ausnutzt, um Ausgaben oder Ergebnisse zu manipulieren.
  ​
* Cedar: Amazons Richtliniensprache und -engine für fein granulare Zugriffserlaubnisse, die bei der Implementierung von ABAC für KI-Systeme verwendet wird.
  ​
* Chain of Thought: Eine Technik zur Verbesserung des Denkvermögens in Sprachmodellen, indem Zwischenschritte des Denkprozesses erzeugt werden, bevor eine endgültige Antwort gegeben wird.
  ​
* Schutzschalter: Mechanismen, die den Betrieb von KI-Systemen automatisch stoppen, wenn bestimmte Risikoschwellen überschritten werden.
  ​
* Datenleck: Unbeabsichtigte Offenlegung sensibler Informationen durch AI-Modellausgaben oder -verhalten.
  ​
* Datenvergiftung: Die absichtliche Manipulation von Trainingsdaten zur Beeinträchtigung der Modellintegrität, oft um Hintertüren einzubauen oder die Leistung zu verschlechtern.
  ​
* Differenzielle Privatsphäre – Differenzielle Privatsphäre ist ein mathematisch fundierter Rahmen zur Veröffentlichung statistischer Informationen über Datensätze, wobei der Datenschutz einzelner Datenpersonen geschützt wird. Sie ermöglicht es einem Dateninhaber, aggregierte Muster der Gruppe zu teilen, während Informationen, die über einzelne Personen preisgegeben werden, eingeschränkt werden.
  ​
* Embeddings: Dichte Vektor-Darstellungen von Daten (Text, Bilder usw.), die semantische Bedeutungen in einem hochdimensionalen Raum erfassen.
  ​
* Erklärbarkeit – Erklärbarkeit in der KI ist die Fähigkeit eines KI-Systems, für seine Entscheidungen und Vorhersagen für Menschen verständliche Gründe anzugeben und Einblicke in seine internen Abläufe zu bieten.
  ​
* Erklärbare KI (XAI): KI-Systeme, die darauf ausgelegt sind, durch verschiedene Techniken und Rahmenwerke menschlich verständliche Erklärungen für ihre Entscheidungen und Verhaltensweisen zu liefern.
  ​
* Federated Learning: Ein maschinelles Lernverfahren, bei dem Modelle über mehrere dezentralisierte Geräte hinweg trainiert werden, die lokale Datensätze enthalten, ohne dass die Daten selbst ausgetauscht werden.
  ​
* Guardrails: Einschränkungen, die implementiert werden, um zu verhindern, dass KI-Systeme schädliche, voreingenommene oder anderweitig unerwünschte Ausgaben erzeugen.
  ​
* Halluzination – Eine AI-Halluzination bezieht sich auf ein Phänomen, bei dem ein KI-Modell falsche oder irreführende Informationen erzeugt, die nicht auf seinen Trainingsdaten oder der sachlichen Realität basieren.
  ​
* Human-in-the-Loop (HITL): Systeme, die so konzipiert sind, dass sie menschliche Aufsicht, Verifikation oder Eingriffe an entscheidenden Entscheidungspunkten erfordern.
  ​
* Infrastructure as Code (IaC): Verwaltung und Bereitstellung von Infrastruktur durch Code anstelle manueller Prozesse, was Sicherheitsscans und konsistente Bereitstellungen ermöglicht.
  ​
* Jailbreak: Techniken, die verwendet werden, um Sicherheitsvorkehrungen in KI-Systemen, insbesondere in großen Sprachmodellen, zu umgehen und verbotene Inhalte zu erzeugen.
  ​
* Minimalprinzip: Das Sicherheitsprinzip, nur die unbedingt erforderlichen Zugriffsrechte für Benutzer und Prozesse zu gewähren.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): Eine Technik zur Erklärung der Vorhersagen beliebiger maschineller Lernklassifikatoren, indem sie lokal durch ein interpretierbares Modell angenähert werden.
  ​
* Mitgliedschafts-Rückschluss-Angriff: Ein Angriff, der darauf abzielt zu ermitteln, ob ein bestimmter Datenpunkt zum Training eines Machine-Learning-Modells verwendet wurde.
  ​
* MITRE ATLAS: Adversarielle Bedrohungslandschaft für Künstliche-Intelligenz-Systeme; eine Wissensdatenbank für adversarielle Taktiken und Techniken gegen KI-Systeme.
  ​
* Modellkarte – Eine Modellkarte ist ein Dokument, das standardisierte Informationen über die Leistung, Einschränkungen, beabsichtigte Verwendungen und ethische Überlegungen eines KI-Modells bereitstellt, um Transparenz und verantwortungsbewusste KI-Entwicklung zu fördern.
  ​
* Modell-Extraktion: Ein Angriff, bei dem ein Angreifer ein Zielmodell wiederholt abfragt, um ohne Autorisierung eine funktional ähnliche Kopie zu erstellen.
  ​
* Modellinversion: Ein Angriff, der versucht, Trainingsdaten durch Analyse der Modellausgaben zu rekonstruieren.
  ​
* Modell-Lebenszyklus-Management – Das AI Modell-Lebenszyklus-Management ist der Prozess der Überwachung aller Phasen der Existenz eines KI-Modells, einschließlich dessen Entwurf, Entwicklung, Einsatz, Überwachung, Wartung und schließlich Ausmusterung, um sicherzustellen, dass es effektiv bleibt und mit den Zielen übereinstimmt.
  ​
* Modellvergiftung: Das Einführen von Schwachstellen oder Hintertüren direkt in ein Modell während des Trainingsprozesses.
  ​
* Modellraub/-diebstahl: Extrahieren einer Kopie oder Annäherung eines proprietären Modells durch wiederholte Abfragen.
  ​
* Multi-Agenten-System: Ein System, das aus mehreren interagierenden KI-Agenten besteht, von denen jeder potenziell unterschiedliche Fähigkeiten und Ziele hat.
  ​
* OPA (Open Policy Agent): Eine Open-Source-Policy-Engine, die eine einheitliche Durchsetzung von Richtlinien über den gesamten Stack hinweg ermöglicht.
  ​
* Datenschutzbewahrendes Maschinelles Lernen (PPML): Techniken und Methoden zum Trainieren und Bereitstellen von ML-Modellen unter Wahrung der Privatsphäre der Trainingsdaten.
  ​
* Prompt Injection: Ein Angriff, bei dem bösartige Anweisungen in Eingaben eingebettet werden, um das beabsichtigte Verhalten eines Modells zu überschreiben.
  ​
* RAG (Retrieval-Augmented Generation): Eine Technik, die große Sprachmodelle verbessert, indem sie vor der Generierung einer Antwort relevante Informationen aus externen Wissensquellen abruft.
  ​
* Red-Teaming: Die Praxis, KI-Systeme aktiv zu testen, indem simulierte Angriffe durch Gegner durchgeführt werden, um Schwachstellen zu identifizieren.
  ​
* SBOM (Software Bill of Materials): Ein formeller Nachweis, der die Details und Lieferkettenbeziehungen verschiedener Komponenten enthält, die beim Erstellen von Software oder KI-Modellen verwendet werden.
  ​
* SHAP (SHapley Additive exPlanations): Ein spieltheoretischer Ansatz zur Erklärung der Ausgabe eines beliebigen Machine-Learning-Modells durch Berechnung des Beitrags jeder Funktion zur Vorhersage.
  ​
* Lieferkettenangriff: Kompromittierung eines Systems durch das Anvisieren weniger sicherer Elemente in seiner Lieferkette, wie Drittanbieter-Bibliotheken, Datensätze oder vortrainierte Modelle.
  ​
* Transfer Learning: Eine Technik, bei der ein für eine Aufgabe entwickeltes Modell als Ausgangspunkt für ein Modell für eine zweite Aufgabe wiederverwendet wird.
  ​
* Vektor-Datenbank: Eine spezialisierte Datenbank, die entwickelt wurde, um hochdimensionale Vektoren (Embeddings) zu speichern und effiziente Ähnlichkeitssuchen durchzuführen.
  ​
* Schwachstellen-Scan: Automatisierte Werkzeuge, die bekannte Sicherheitslücken in Softwarekomponenten, einschließlich KI-Frameworks und Abhängigkeiten, identifizieren.
  ​
* Wasserzeichen: Techniken zum Einbetten von kaum wahrnehmbaren Markierungen in KI-generierte Inhalte, um deren Ursprung zu verfolgen oder KI-Generierung zu erkennen.
  ​
* Zero-Day-Schwachstelle: Eine zuvor unbekannte Schwachstelle, die Angreifer ausnutzen können, bevor Entwickler einen Patch erstellen und bereitstellen.

