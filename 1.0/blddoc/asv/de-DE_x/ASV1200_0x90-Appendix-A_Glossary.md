# Anhang A: Glossar

>Dieses umfassende Glossar bietet Definitionen wichtiger Begriffe aus den Bereichen KI, ML und Sicherheit, die im gesamten AISVS verwendet werden, um Klarheit und ein gemeinsames Verständnis zu gewährleisten.

* Adversariales Beispiel: Eine absichtlich gestaltete Eingabe, die ein KI-Modell dazu bringt, einen Fehler zu machen, oft durch Hinzufügen subtiler Störungen, die für Menschen nicht wahrnehmbar sind.
  ​
* Adversarielle Robustheit – Adversarielle Robustheit in der KI bezieht sich auf die Fähigkeit eines Modells, seine Leistung aufrechtzuerhalten und sich gegen absichtlich gestaltete, bösartige Eingaben zu wehren, die dazu dienen, Fehler zu verursachen oder es zu täuschen.
  ​
* Agent – KI-Agenten sind Softwaresysteme, die KI verwenden, um Ziele zu verfolgen und Aufgaben im Auftrag von Benutzern zu erfüllen. Sie zeigen Fähigkeiten im Bereich des Denkens, der Planung und des Gedächtnisses und besitzen ein gewisses Maß an Autonomie, um Entscheidungen zu treffen, zu lernen und sich anzupassen.
  ​
* Agentische KI: KI-Systeme, die mit einem gewissen Grad an Autonomie agieren können, um Ziele zu erreichen, häufig Entscheidungen treffen und Maßnahmen ergreifen, ohne direkte menschliche Intervention.
  ​
* Attributbasierte Zugriffskontrolle (ABAC): Ein Zugriffssteuerungsparadigma, bei dem Autorisierungsentscheidungen auf Attributen des Benutzers, der Ressource, der Aktion und der Umgebung basieren, die zur Abfragezeit bewertet werden.
  ​
* Backdoor-Angriff: Eine Art von Datenvergiftungsangriff, bei dem das Modell darauf trainiert wird, auf bestimmte Auslöser in einer bestimmten Weise zu reagieren, während es sich sonst normal verhält.
  ​
* Bias: Systematische Fehler in den Ausgaben von KI-Modellen, die zu unfairen oder diskriminierenden Ergebnissen für bestimmte Gruppen oder in speziellen Kontexten führen können.
  ​
* Bias-Ausnutzung: Eine Angriffsstrategie, die bekannte Verzerrungen in KI-Modellen ausnutzt, um Ausgaben oder Ergebnisse zu manipulieren.
  ​
* Cedar: Amazons Richtliniensprache und Engine für fein abgestufte Berechtigungen, die bei der Implementierung von ABAC für KI-Systeme verwendet wird.
  ​
* Chain of Thought: Eine Technik zur Verbesserung des Denkprozesses in Sprachmodellen durch Generierung von Zwischenschritten des Denkens vor der Ausgabe einer endgültigen Antwort.
  ​
* Circuit Breaker: Mechanismen, die den Betrieb von KI-Systemen automatisch stoppen, wenn bestimmte Risikoschwellen überschritten werden.
  ​
* Datenleck: Unbeabsichtigte Offenlegung sensibler Informationen durch Ausgaben oder Verhalten von KI-Modellen.
  ​
* Datenvergiftung: Die absichtliche Korruption von Trainingsdaten, um die Integrität des Modells zu beeinträchtigen, häufig um Hintertüren zu installieren oder die Leistung zu verschlechtern.
  ​
* Differential Privacy – Differential Privacy ist ein mathematisch rigoroser Rahmen zur Veröffentlichung statistischer Informationen über Datensätze, wobei der Datenschutz einzelner Datenpersonen geschützt wird. Es ermöglicht einem Datenhalter, aggregierte Muster der Gruppe zu teilen und gleichzeitig die Informationsweitergabe über spezifische Individuen zu begrenzen.
  ​
* Einbettungen: Dichte Vektor-Darstellungen von Daten (Text, Bilder usw.), die semantische Bedeutung in einem hochdimensionalen Raum erfassen.
  ​
* Erklärbarkeit – Erklärbarkeit in der KI ist die Fähigkeit eines KI-Systems, menschlich verständliche Gründe für seine Entscheidungen und Vorhersagen zu liefern und Einblicke in seine internen Abläufe zu geben.
  ​
* Erklärbare KI (XAI): KI-Systeme, die entwickelt wurden, um durch verschiedene Techniken und Rahmenwerke menschenverständliche Erklärungen für ihre Entscheidungen und Verhaltensweisen zu liefern.
  ​
* Federated Learning: Ein maschinelles Lernverfahren, bei dem Modelle über mehrere dezentralisierte Geräte hinweg trainiert werden, die lokale Datensätze besitzen, ohne dass die Daten selbst ausgetauscht werden.
  ​
* Leitplanken: Einschränkungen, die implementiert werden, um zu verhindern, dass KI-Systeme schädliche, voreingenommene oder anderweitig unerwünschte Ausgaben erzeugen.
  ​
* Halluzination – Eine AI-Halluzination bezieht sich auf ein Phänomen, bei dem ein AI-Modell falsche oder irreführende Informationen generiert, die nicht auf seinen Trainingsdaten oder der tatsächlichen Realität basieren.
  ​
* Human-in-the-Loop (HITL): Systeme, die so konzipiert sind, dass sie menschliche Aufsicht, Verifizierung oder Eingriffe an entscheidenden Entscheidungspunkten erfordern.
  ​
* Infrastructure as Code (IaC): Verwaltung und Bereitstellung von Infrastruktur durch Code anstelle manueller Prozesse, wodurch Sicherheitsscans und konsistente Bereitstellungen ermöglicht werden.
  ​
* Jailbreak: Techniken, die verwendet werden, um Sicherheitsmaßnahmen in KI-Systemen, insbesondere in großen Sprachmodellen, zu umgehen, um verbotenes Material zu erzeugen.
  ​
* Minimalberechtigung: Das Sicherheitsprinzip, nur die unbedingt notwendigen Zugriffsrechte für Benutzer und Prozesse zu gewähren.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): Eine Technik zur Erklärung der Vorhersagen eines beliebigen Machine-Learning-Klassifikators, indem dieser lokal mit einem interpretierbaren Modell approximiert wird.
  ​
* Membership Inference Attack: Ein Angriff, der darauf abzielt festzustellen, ob ein bestimmter Datenpunkt zum Training eines maschinellen Lernmodells verwendet wurde.
  ​
* MITRE ATLAS: Adversarielle Bedrohungslandschaft für künstliche Intelligenzsysteme; eine Wissensdatenbank adversarieller Taktiken und Techniken gegen KI-Systeme.
  ​
* Modellkarte – Eine Modellkarte ist ein Dokument, das standardisierte Informationen über die Leistung, Einschränkungen, beabsichtigte Anwendungen und ethische Überlegungen eines KI-Modells bereitstellt, um Transparenz und verantwortungsbewusste KI-Entwicklung zu fördern.
  ​
* Modell-Extraktion: Ein Angriff, bei dem ein Angreifer ein Zielmodell wiederholt abfragt, um ohne Autorisierung eine funktional ähnliche Kopie zu erstellen.
  ​
* Modellinversion: Ein Angriff, der versucht, Trainingsdaten durch Analyse von Modellausgaben zu rekonstruieren.
  ​
* Modelllebenszyklusverwaltung – Die Verwaltung des Lebenszyklus von KI-Modellen ist der Prozess der Überwachung aller Phasen der Existenz eines KI-Modells, einschließlich dessen Gestaltung, Entwicklung, Implementierung, Überwachung, Wartung und eventualer Außerbetriebnahme, um sicherzustellen, dass es effektiv bleibt und mit den Zielen übereinstimmt.
  ​
* Modellvergiftung: Einführung von Schwachstellen oder Hintertüren direkt in ein Modell während des Trainingsprozesses.
  ​
* Modellraub/Diebstahl: Das Extrahieren einer Kopie oder Annäherung eines proprietären Modells durch wiederholte Anfragen.
  ​
* Multi-Agenten-System: Ein System, das aus mehreren interagierenden KI-Agenten besteht, von denen jeder potenziell unterschiedliche Fähigkeiten und Ziele hat.
  ​
* OPA (Open Policy Agent): Eine Open-Source-Richtlinien-Engine, die eine einheitliche Durchsetzung von Richtlinien über den gesamten Stack ermöglicht.
  ​
* Datenschutzwahrendes maschinelles Lernen (PPML): Techniken und Methoden zum Trainieren und Bereitstellen von ML-Modellen unter Wahrung der Privatsphäre der Trainingsdaten.
  ​
* Prompt Injection: Ein Angriff, bei dem bösartige Anweisungen in Eingaben eingebettet werden, um das beabsichtigte Verhalten eines Modells zu überschreiben.
  ​
* RAG (Retrieval-Augmented Generation): Eine Technik, die große Sprachmodelle verbessert, indem sie vor der Generierung einer Antwort relevante Informationen aus externen Wissensquellen abruft.
  ​
* Red-Teaming: Die Praxis, KI-Systeme aktiv zu testen, indem adversariale Angriffe simuliert werden, um Schwachstellen zu identifizieren.
  ​
* SBOM (Software Bill of Materials): Eine formelle Aufzeichnung, die die Details und Lieferkettenbeziehungen verschiedener Komponenten enthält, die beim Erstellen von Software oder KI-Modellen verwendet werden.
  ​
* SHAP (SHapley Additive Erklärungen): Ein spieltheoretischer Ansatz zur Erklärung der Ausgabe eines beliebigen Machine-Learning-Modells durch Berechnung des Beitrags jeder Eigenschaft zur Vorhersage.
  ​
* Lieferkettenangriff: Kompromittierung eines Systems durch gezielten Angriff auf weniger sichere Elemente in seiner Lieferkette, wie Drittanbieter-Bibliotheken, Datensätze oder vortrainierte Modelle.
  ​
* Transfer Learning: Eine Technik, bei der ein für eine Aufgabe entwickeltes Modell als Ausgangspunkt für ein Modell einer zweiten Aufgabe wiederverwendet wird.
  ​
* Vektordatenbank: Eine spezialisierte Datenbank, die dafür entwickelt wurde, hochdimensionale Vektoren (Embeddings) zu speichern und effiziente Ähnlichkeitssuchen durchzuführen.
  ​
* Schwachstellenscanning: Automatisierte Werkzeuge, die bekannte Sicherheitslücken in Softwarekomponenten, einschließlich KI-Frameworks und Abhängigkeiten, identifizieren.
  ​
* Wasserzeichen: Techniken zum Einbetten von für das menschliche Auge nicht wahrnehmbaren Markierungen in KI-generierte Inhalte, um deren Ursprung zu verfolgen oder KI-Generierung zu erkennen.
  ​
* Zero-Day-Schwachstelle: Eine zuvor unbekannte Schwachstelle, die Angreifer ausnutzen können, bevor Entwickler einen Fix erstellen und bereitstellen.

