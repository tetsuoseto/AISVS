# Vorsatzblatt

## Über den Standard

Der Artificial Intelligence Security Verification Standard (AISVS) ist ein gemeinschaftlich entwickelter Katalog von Sicherheitsanforderungen, den Data Scientists, MLOps-Ingenieure, Softwarearchitekten, Entwickler, Tester, Sicherheitsexperten, Tool-Anbieter, Regulierungsbehörden und Anwender nutzen können, um vertrauenswürdige KI-gestützte Systeme und Anwendungen zu entwerfen, zu bauen, zu testen und zu verifizieren. Er bietet eine gemeinsame Sprache zur Spezifikation von Sicherheitskontrollen über den gesamten KI-Lebenszyklus hinweg – von der Datenerhebung und Modellentwicklung bis hin zur Implementierung und kontinuierlichen Überwachung – damit Organisationen die Resilienz, den Datenschutz und die Sicherheit ihrer KI-Lösungen messen und verbessern können.

## Urheberrecht und Lizenz

Version 0.1 (Erster öffentlicher Entwurf - Arbeit in Vorbereitung), 2025  

![license](../images/license.png)

Copyright © 2025 Das AISVS-Projekt.  

Veröffentlicht unter dem[Creative Commons Attribution‑ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).  
Bei jeglicher Wiederverwendung oder Verbreitung müssen Sie die Lizenzbedingungen dieses Werks klar an andere weitergeben.

## Projektleiter

|            |                         |
| ---------- | ----------------------- |
| Jim Manico | Aras „Russ“ Memisyazici |

## Mitwirkende und Prüfer

|                                    |                             |
| ---------------------------------- | --------------------------- |
| https://github.com/ottosulin       | https://github.com/mbhatt1  |
| https://github.com/vineethsai      | https://github.com/cciprofm |
| https://github.com/deepakrpandey12 |                             |

---

AISVS ist ein brandneuer Standard, der speziell entwickelt wurde, um die einzigartigen Sicherheitsherausforderungen von künstlichen Intelligenzsystemen zu adressieren. Während er sich an allgemeineren Sicherheitsbest Practices orientiert, wurde jede Anforderung in AISVS von Grund auf entwickelt, um die Bedrohungslandschaft der KI widerzuspiegeln und Organisationen dabei zu helfen, sicherere und widerstandsfähigere KI-Lösungen zu entwickeln.

