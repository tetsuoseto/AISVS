# Apéndice A: Glosario

>This glosario exhaustivo proporciona definiciones de términos clave de IA, ML y seguridad utilizados a lo largo del AISVS para garantizar claridad y comprensión común.

* Ejemplo adversarial: Una entrada deliberadamente diseñada para hacer que un modelo de IA cometa un error, a menudo mediante la adición de perturbaciones sutiles imperceptibles para los humanos.
  ​
* Robustez adversarial – La robustez en IA se refiere a la capacidad de un modelo para mantener su rendimiento y resistir ser engañado o manipulado por entradas intencionadamente elaboradas y maliciosas diseñadas para provocar errores.
  ​
* Agente – Los agentes de IA son sistemas de software que utilizan IA para perseguir objetivos y completar tareas en nombre de los usuarios. Muestran razonamiento, planificación y memoria, y tienen un nivel de autonomía para tomar decisiones, aprender y adaptarse.
  ​
* IA con agencia: sistemas de IA que pueden operar con cierto grado de autonomía para lograr objetivos, a menudo tomando decisiones y llevando a cabo acciones sin intervención humana directa.
  ​
* Control de Acceso Basado en Atributos (ABAC): Un paradigma de control de acceso en el que las decisiones de autorización se basan en atributos del usuario, del recurso, de la acción y del entorno, evaluadas en el momento de la consulta.
  ​
* Ataque de puerta trasera: un tipo de ataque de envenenamiento de datos en el que el modelo se entrena para responder de forma específica a ciertos desencadenantes, mientras que se comporta normalmente en otros casos.
  ​
* Sesgo: Errores sistemáticos en las salidas de modelos de IA que pueden conducir a resultados injustos o discriminatorios para ciertos grupos o en contextos específicos.
  ​
* Explotación de sesgos: Una técnica de ataque que aprovecha sesgos conocidos en modelos de IA para manipular salidas o resultados.
  ​
* Cedar: el lenguaje de políticas y el motor de Amazon para permisos de granularidad fina utilizados en la implementación de ABAC para sistemas de IA.
  ​
* Cadena de razonamiento: una técnica para mejorar el razonamiento en modelos de lenguaje mediante la generación de pasos de razonamiento intermedios antes de producir una respuesta final.
  ​
* Disyuntores: Mecanismos que detienen automáticamente las operaciones del sistema de IA cuando se superan umbrales de riesgo específicos.
  ​
* Filtración de datos: exposición no intencionada de información sensible a través de las salidas o el comportamiento de un modelo de IA.
  ​
* Envenenamiento de datos: La corrupción deliberada de los datos de entrenamiento para comprometer la integridad del modelo, a menudo para instalar puertas traseras o degradar el rendimiento.
  ​
* Privacidad diferencial – La privacidad diferencial es un marco matemáticamente riguroso para divulgar información estadística sobre conjuntos de datos, al tiempo que protege la privacidad de los sujetos de datos individuales. Permite al titular de datos compartir patrones agregados del grupo, mientras limita la información que se filtre sobre individuos específicos.
  ​
* Embeddings: Representaciones vectoriales densas de datos (texto, imágenes, etc.) que capturan el significado semántico en un espacio de alta dimensionalidad.
  ​
* Explicabilidad – La explicabilidad en IA es la capacidad de un sistema de IA para proporcionar razones comprensibles para sus decisiones y predicciones, ofreciendo vislumbres de su funcionamiento interno.
  ​
* IA Explicable (XAI): Sistemas de IA diseñados para proporcionar explicaciones comprensibles para los humanos de sus decisiones y comportamientos a través de diversas técnicas y marcos.
  ​
* Aprendizaje Federado: Un enfoque de aprendizaje automático en el que los modelos se entrenan en múltiples dispositivos descentralizados que albergan muestras de datos locales, sin intercambiar los datos en sí.
  ​
* Barreras: Restricciones implementadas para evitar que los sistemas de IA produzcan salidas dañinas, sesgadas o, por lo demás, indeseables.
  ​
* Alucinación – Una alucinación de IA se refiere a un fenómeno en el que un modelo de IA genera información incorrecta o engañosa que no se basa en sus datos de entrenamiento ni en la realidad fáctica.
  ​
* Intervención humana en el bucle (HITL): Sistemas diseñados para exigir supervisión humana, verificación o intervención humana en puntos de decisión críticos.
  ​
* Infraestructura como código (IaC): Gestión y aprovisionamiento de la infraestructura mediante código en lugar de procesos manuales, lo que permite el escaneo de seguridad y despliegues consistentes.
  ​
* Jailbreak: Técnicas utilizadas para eludir las salvaguardas de seguridad en sistemas de IA, especialmente en los grandes modelos de lenguaje, para generar contenido prohibido.
  ​
* Principio de mínimo privilegio: el principio de seguridad que consiste en otorgar solo los derechos de acceso mínimos necesarios para los usuarios y los procesos.
  ​
* LIME (Local Interpretable Model-agnostic Explanations): una técnica para explicar las predicciones de cualquier clasificador de aprendizaje automático, aproximándolo localmente con un modelo interpretable.
  ​
* Ataque de inferencia de membresía: Un ataque cuyo objetivo es determinar si un punto de datos específico se utilizó para entrenar un modelo de aprendizaje automático.
  ​
* MITRE ATLAS: Panorama de amenazas adversarias para sistemas de inteligencia artificial; una base de conocimientos sobre tácticas y técnicas adversarias contra sistemas de IA.
  ​
* Tarjeta de modelo – Una tarjeta de modelo es un documento que proporciona información estandarizada sobre el rendimiento de un modelo de IA, sus limitaciones, usos previstos y consideraciones éticas para promover la transparencia y el desarrollo responsable de la IA.
  ​
* Extracción de modelo: Un ataque en el que un adversario consulta repetidamente un modelo objetivo para crear una copia funcionalmente similar sin autorización.
  ​
* Inversión de modelos: Un ataque que intenta reconstruir los datos de entrenamiento analizando las salidas del modelo.
  ​
* Gestión del ciclo de vida del modelo – La gestión del ciclo de vida de la IA es el proceso de supervisar todas las etapas de la existencia de un modelo de IA, incluido su diseño, desarrollo, despliegue, monitoreo, mantenimiento y eventual retiro, para garantizar que siga siendo eficaz y esté alineado con los objetivos.
  ​
* Envenenamiento de modelos: Introducir vulnerabilidades o puertas traseras directamente en un modelo durante el proceso de entrenamiento.
  ​
* Robo/Extracción de modelos: Extraer una copia o una aproximación de un modelo propietario mediante consultas repetidas.
  ​
* Sistema multiagente: Un sistema compuesto por múltiples agentes de IA que interactúan entre sí, cada uno con capacidades y objetivos potencialmente diferentes.
  ​
* OPA (Open Policy Agent): Un motor de políticas de código abierto que permite la aplicación unificada de políticas en toda la pila.
  ​
* Aprendizaje automático con privacidad (PPML): Técnicas y métodos para entrenar y desplegar modelos de ML mientras se protege la privacidad de los datos de entrenamiento.
  ​
* Inyección de prompt: un ataque en el que instrucciones maliciosas se insertan en las entradas para anular el comportamiento previsto del modelo.
  ​
* RAG (Generación Aumentada por Recuperación): Una técnica que mejora los modelos de lenguaje a gran escala al recuperar información relevante de fuentes de conocimiento externas antes de generar una respuesta.
  ​
* Red-Teaming: la práctica de probar activamente sistemas de IA simulando ataques adversarios para identificar vulnerabilidades.
  ​
* SBOM (Lista de Materiales de Software): Un registro formal que contiene los detalles y las relaciones de la cadena de suministro de varios componentes utilizados para construir software o modelos de IA.
  ​
* SHAP (SHapley Additive exPlanations): Un enfoque teórico de juego para explicar la salida de cualquier modelo de aprendizaje automático al calcular la contribución de cada característica a la predicción.
  ​
* Ataque a la cadena de suministro: Comprometer un sistema apuntando a elementos menos seguros de su cadena de suministro, como bibliotecas de terceros, conjuntos de datos o modelos preentrenados.
  ​
* Aprendizaje por transferencia: Una técnica en la que un modelo desarrollado para una tarea se reutiliza como punto de partida para un modelo en una segunda tarea.
  ​
* Base de datos de vectores: una base de datos especializada diseñada para almacenar vectores de alta dimensionalidad (embeddings) y realizar búsquedas de similitud eficientes.
  ​
* Escaneo de vulnerabilidades: herramientas automatizadas que identifican vulnerabilidades de seguridad conocidas en componentes de software, incluyendo marcos de IA y dependencias.
  ​
* Marcado de agua digital: Técnicas para incrustar marcadores imperceptibles en contenido generado por IA para rastrear su origen o detectar la generación por IA.
  ​
* Vulnerabilidad de día cero: una vulnerabilidad previamente desconocida que los atacantes pueden explotar antes de que los desarrolladores creen e implementen un parche.

